<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Modelización de la frecuencia | Loss Data Analytics</title>
  <meta name="description" content="Chapter 2 Modelización de la frecuencia | Loss Data Analytics is an interactive, online, freely available text. - The online version will contain many interactive objects (quizzes, computer demonstrations, interactive graphs, video, and the like) to promote deeper learning. - A subset of the book will be available in pdf format for low-cost printing. - The online text will be available in multiple languages to promote access to a worldwide audience." />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Modelización de la frecuencia | Loss Data Analytics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Chapter 2 Modelización de la frecuencia | Loss Data Analytics is an interactive, online, freely available text. - The online version will contain many interactive objects (quizzes, computer demonstrations, interactive graphs, video, and the like) to promote deeper learning. - A subset of the book will be available in pdf format for low-cost printing. - The online text will be available in multiple languages to promote access to a worldwide audience." />
  <meta name="github-repo" content="https://github.com/openacttexts/Loss-Data-Analytics" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Modelización de la frecuencia | Loss Data Analytics" />
  
  <meta name="twitter:description" content="Chapter 2 Modelización de la frecuencia | Loss Data Analytics is an interactive, online, freely available text. - The online version will contain many interactive objects (quizzes, computer demonstrations, interactive graphs, video, and the like) to promote deeper learning. - A subset of the book will be available in pdf format for low-cost printing. - The online text will be available in multiple languages to promote access to a worldwide audience." />
  

<meta name="author" content="An open text authored by the Actuarial Community" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="C-Intro.html"/>
<link rel="next" href="C-Severity.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<!-- Mathjax -->
<script type='text/x-mathjax-config'>
		MathJax.Hub.Config({
			extensions: ['tex2jax.js'],
			jax: ['input/TeX', 'output/HTML-CSS'],
			tex2jax: {
				inlineMath: [ ['$','$'], ['\\(','\\)'] ],
				displayMath: [ ['$$','$$'], ['\\[','\\]'] ],
				processEscapes: true
			},
			'HTML-CSS': { availableFonts: ['TeX'] }
		});
</script>

<!-- The following code is for the quizzes -->
<script src="https://surveyjs.azureedge.net/1.0.50/survey.jquery.js"></script>
<link href="https://surveyjs.azureedge.net/1.0.50/survey.css" type="text/css" rel="stylesheet"/>
<script src="https://cdnjs.cloudflare.com/ajax/libs/showdown/1.6.4/showdown.min.js"></script>  

<script>
function markdownConverterEWF() {  
//Create showdown markdown converter
var converter = new showdown.Converter();
converter.setOption('ghCompatibleHeaderId', true);
survey
    .onTextMarkdown
    .add(function (survey, options) {
        //convert the mardown text to html
        var str = converter.makeHtml(options.text);
        //remove root paragraphs <p></p>
        str = str.substring(3);
        str = str.substring(0, str.length - 4);
        //set html
        options.html = str;
         MathJax.Hub.Queue(['Typeset',MathJax.Hub, 'options']);
    });  
};
// Quiz Header info
const jsonHeader = { 
    showProgressBar: "bottom",
    showTimerPanel: "none",
    maxTimeToFinishPage: 10000,
    maxTimeToFinish: 25000,
    firstPageIsStarted: true,
    startSurveyText: "Start Quiz" //,
//    title: "Does This Make Sense?"
}
// One and Two question quizzes
function jsonSummary1EWF(json) {  
let jsonEnd1 = { 
completedHtml: 
json["pages"][1]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][1]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][1]["questions"][0]["correctAnswer"]
};  
return jsonEnd1;
};


function jsonSummary2EWF(json) {  
let jsonEnd2 = { 
completedHtml: 
json["pages"][1]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][1]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][1]["questions"][0]["correctAnswer"]
+"<br>"+
json["pages"][2]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][2]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][2]["questions"][0]["correctAnswer"]
};  
return jsonEnd2;
};

// Three, four, and five question quizzes
function jsonSummary3EWF(json) {  
let jsonEnd3 = { 
completedHtml: 
json["pages"][1]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][1]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][1]["questions"][0]["correctAnswer"]
+"<br>"+
json["pages"][2]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][2]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][2]["questions"][0]["correctAnswer"]
+"<br>"+
json["pages"][3]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][3]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][3]["questions"][0]["correctAnswer"]
};  
return jsonEnd3;
};

function jsonSummary4EWF(json) {  
jsonEnd4 = jsonSummary3EWF(json);
jsonEnd4.completedHtml = jsonEnd4.completedHtml +  
"<br>"+
json["pages"][4]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][4]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][4]["questions"][0]["correctAnswer"]
;  
return jsonEnd4;
};

function jsonSummary5EWF(json) {  
jsonEnd5 = jsonSummary4EWF(json);
jsonEnd5.completedHtml = jsonEnd5.completedHtml +  
"<br>"+
json["pages"][5]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][5]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][5]["questions"][0]["correctAnswer"]
;  
return jsonEnd5;
};

function jsonSummary6EWF(json) {  
jsonEnd6 = jsonSummary5EWF(json);
jsonEnd6.completedHtml = jsonEnd6.completedHtml +  
"<br>"+
json["pages"][6]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][6]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][6]["questions"][0]["correctAnswer"]
;  
return jsonEnd6;
};

function jsonSummary7EWF(json) {  
jsonEnd7 = jsonSummary6EWF(json);
jsonEnd7.completedHtml = jsonEnd7.completedHtml +  
"<br>"+
json["pages"][7]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][7]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][7]["questions"][0]["correctAnswer"]
;  
return jsonEnd7;
};

function jsonSummary8EWF(json) {  
jsonEnd8 = jsonSummary7EWF(json);
jsonEnd8.completedHtml = jsonEnd8.completedHtml +  
"<br>"+
json["pages"][8]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][8]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][8]["questions"][0]["correctAnswer"]
;  
return jsonEnd8;
};

function jsonSummary9EWF(json) {  
jsonEnd9 = jsonSummary8EWF(json);
jsonEnd9.completedHtml = jsonEnd9.completedHtml +  
"<br>"+
json["pages"][9]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][9]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][9]["questions"][0]["correctAnswer"]
;  
return jsonEnd9;
};


function jsonSummary10EWF(json) {  
jsonEnd10 = jsonSummary9EWF(json);
jsonEnd10.completedHtml = jsonEnd10.completedHtml +  
"<br>"+
json["pages"][10]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][10]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][10]["questions"][0]["correctAnswer"]
;  
return jsonEnd10;
};


function jsonSummary11EWF(json) {  
jsonEnd11 = jsonSummary10EWF(json);
jsonEnd11.completedHtml = jsonEnd11.completedHtml +  
"<br>"+
json["pages"][11]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][11]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][11]["questions"][0]["correctAnswer"]
;  
return jsonEnd11;
};



</script>  
<!-- This completes the code for the quizzes -->


<!-- Various toggle functions used throughout --> 
<script language="javascript">
function toggle(id1,id2) {
	var ele = document.getElementById(id1); var text = document.getElementById(id2);
	if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Mostrar Solución";}
		else {ele.style.display = "block"; text.innerHTML = "Ocultar Solución";}}
function togglecode(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Mostrar Código R";}
      else {ele.style.display = "block"; text.innerHTML = "Ocultar Código R";}}
function toggleEX(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Mostrar Ejemplo";}
      else {ele.style.display = "block"; text.innerHTML = "Ocultar Ejemplo";}}
function toggleTheory(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Mostrar Teoría";}
      else {ele.style.display = "block"; text.innerHTML = "Ocultar Teoría";}}
function toggleQuiz(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Mostrar Solución de Prueba";}
      else {ele.style.display = "block"; text.innerHTML = "Ocultar Solución de Prueba";}}      
</script>

<!-- A few functions for revealing definitions -->
<script language="javascript">
<!--   $( function() {
    $("#tabs").tabs();
  } ); -->

$(document).ready(function(){
    $('[data-toggle="tooltip"]').tooltip();
});

$(document).ready(function(){
    $('[data-toggle="popover"]').popover(); 
});
</script>

<script language="javascript">
function openTab(evt, tabName) {
    var i, tabcontent, tablinks;
    tabcontent = document.getElementsByClassName("tabcontent");
    for (i = 0; i < tabcontent.length; i++) {
        tabcontent[i].style.display = "none";
    }
    tablinks = document.getElementsByClassName("tablinks");
    for (i = 0; i < tablinks.length; i++) {
        tablinks[i].className = tablinks[i].className.replace(" active", "");
    }
    document.getElementById(tabName).style.display = "block";
    evt.currentTarget.className += " active";
}

// Get the element with id="defaultOpen" and click on it
document.getElementById("defaultOpen").click();
</script>


<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-125587869-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-125587869-1');
</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Loss Data Analytics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#contributors"><i class="fa fa-check"></i>Contributors</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#reviewers"><i class="fa fa-check"></i>Reviewers</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#for-our-readers"><i class="fa fa-check"></i>For our Readers</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="C-Intro.html"><a href="C-Intro.html"><i class="fa fa-check"></i><b>1</b> Introduction to Loss Data Analytics</a><ul>
<li class="chapter" data-level="1.1" data-path="C-Intro.html"><a href="C-Intro.html#S:Intro"><i class="fa fa-check"></i><b>1.1</b> Relevance of Analytics to Insurance Activities</a><ul>
<li class="chapter" data-level="1.1.1" data-path="C-Intro.html"><a href="C-Intro.html#nature-and-relevance-of-insurance"><i class="fa fa-check"></i><b>1.1.1</b> Nature and Relevance of Insurance</a></li>
<li class="chapter" data-level="1.1.2" data-path="C-Intro.html"><a href="C-Intro.html#what-is-analytics"><i class="fa fa-check"></i><b>1.1.2</b> What is Analytics?</a></li>
<li class="chapter" data-level="1.1.3" data-path="C-Intro.html"><a href="C-Intro.html#S:InsProcesses"><i class="fa fa-check"></i><b>1.1.3</b> Insurance Processes</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="C-Intro.html"><a href="C-Intro.html#S:PredModApps"><i class="fa fa-check"></i><b>1.2</b> Insurance Company Operations</a><ul>
<li class="chapter" data-level="1.2.1" data-path="C-Intro.html"><a href="C-Intro.html#initiating-insurance"><i class="fa fa-check"></i><b>1.2.1</b> Initiating Insurance</a></li>
<li class="chapter" data-level="1.2.2" data-path="C-Intro.html"><a href="C-Intro.html#renewing-insurance"><i class="fa fa-check"></i><b>1.2.2</b> Renewing Insurance</a></li>
<li class="chapter" data-level="1.2.3" data-path="C-Intro.html"><a href="C-Intro.html#claims-and-product-management"><i class="fa fa-check"></i><b>1.2.3</b> Claims and Product Management</a></li>
<li class="chapter" data-level="1.2.4" data-path="C-Intro.html"><a href="C-Intro.html#S:Reserving"><i class="fa fa-check"></i><b>1.2.4</b> Loss Reserving</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="C-Intro.html"><a href="C-Intro.html#S:LGPIF"><i class="fa fa-check"></i><b>1.3</b> Case Study: Wisconsin Property Fund</a><ul>
<li class="chapter" data-level="1.3.1" data-path="C-Intro.html"><a href="C-Intro.html#S:OutComes"><i class="fa fa-check"></i><b>1.3.1</b> Fund Claims Variables: Frequency and Severity</a></li>
<li class="chapter" data-level="1.3.2" data-path="C-Intro.html"><a href="C-Intro.html#S:FundVariables"><i class="fa fa-check"></i><b>1.3.2</b> Fund Rating Variables</a></li>
<li class="chapter" data-level="1.3.3" data-path="C-Intro.html"><a href="C-Intro.html#fund-operations"><i class="fa fa-check"></i><b>1.3.3</b> Fund Operations</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="C-Intro.html"><a href="C-Intro.html#Intro-further-reading-and-resources"><i class="fa fa-check"></i><b>1.4</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html"><i class="fa fa-check"></i><b>2</b> Modelización de la frecuencia</a><ul>
<li class="chapter" data-level="2.1" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:frequency-distributions"><i class="fa fa-check"></i><b>2.1</b> Distribuciones de frecuencias</a><ul>
<li class="chapter" data-level="2.1.1" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:how-frequency-augments-severity-information"><i class="fa fa-check"></i><b>2.1.1</b> Cómo la frecuencia incrementa la información sobre la cuantía</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:basic-frequency-distributions"><i class="fa fa-check"></i><b>2.2</b> Distribuciones de frecuencias elementales</a><ul>
<li class="chapter" data-level="2.2.1" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:foundations"><i class="fa fa-check"></i><b>2.2.1</b> Fundamentos</a></li>
<li class="chapter" data-level="2.2.2" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:generating-functions"><i class="fa fa-check"></i><b>2.2.2</b> Funciones Generadoras de Momentos y de Probabilidad</a></li>
<li class="chapter" data-level="2.2.3" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:important-frequency-distributions"><i class="fa fa-check"></i><b>2.2.3</b> Distribuciones de Frecuencias Importantes</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:the-a-b-0-class"><i class="fa fa-check"></i><b>2.3</b> La clase (a, b, 0)</a></li>
<li class="chapter" data-level="2.4" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:estimating-frequency-distributions"><i class="fa fa-check"></i><b>2.4</b> Estimación de las distribuciones de frecuencias</a><ul>
<li class="chapter" data-level="2.4.1" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:parameter-estimation"><i class="fa fa-check"></i><b>2.4.1</b> Estimación de los parámetros</a></li>
<li class="chapter" data-level="2.4.2" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:frequency-distributions-mle"><i class="fa fa-check"></i><b>2.4.2</b> MLE de las distribuciones de frecuencias</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:other-frequency-distributions"><i class="fa fa-check"></i><b>2.5</b> Otras Distribuciones de Frecuencias</a><ul>
<li class="chapter" data-level="2.5.1" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:zero-truncation-or-modification"><i class="fa fa-check"></i><b>2.5.1</b> Modificación o Truncamiento en Cero</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:mixture-distributions"><i class="fa fa-check"></i><b>2.6</b> Distribuciones Mixtas</a></li>
<li class="chapter" data-level="2.7" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:goodness-of-fit"><i class="fa fa-check"></i><b>2.7</b> Bondad del Ajuste</a></li>
<li class="chapter" data-level="2.8" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:exercises"><i class="fa fa-check"></i><b>2.8</b> Ejercicios</a></li>
<li class="chapter" data-level="2.9" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#Freq-further-reading-and-resources"><i class="fa fa-check"></i><b>2.9</b> Recursos adicionales y autores</a><ul>
<li class="chapter" data-level="2.9.1" data-path="C-Frequency-Modeling.html"><a href="C-Frequency-Modeling.html#S:rcode"><i class="fa fa-check"></i><b>2.9.1</b> TS 2.A. Código R para Gráficos</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="C-Severity.html"><a href="C-Severity.html"><i class="fa fa-check"></i><b>3</b> Modelización de la severidad de las pérdidas</a><ul>
<li class="chapter" data-level="3.1" data-path="C-Severity.html"><a href="C-Severity.html#S:BasicQuantities"><i class="fa fa-check"></i><b>3.1</b> Cantidades distribucionales básicas</a><ul>
<li class="chapter" data-level="3.1.1" data-path="C-Severity.html"><a href="C-Severity.html#S:Chap3Moments"><i class="fa fa-check"></i><b>3.1.1</b> Momentos</a></li>
<li class="chapter" data-level="3.1.2" data-path="C-Severity.html"><a href="C-Severity.html#cuantiles"><i class="fa fa-check"></i><b>3.1.2</b> Cuantiles</a></li>
<li class="chapter" data-level="3.1.3" data-path="C-Severity.html"><a href="C-Severity.html#función-generatriz-de-momentos"><i class="fa fa-check"></i><b>3.1.3</b> Función generatriz de momentos</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="C-Severity.html"><a href="C-Severity.html#S:ContinuousDistn"><i class="fa fa-check"></i><b>3.2</b> Distribuciones continuas para modelizar la severidad de las pérdidas</a><ul>
<li class="chapter" data-level="3.2.1" data-path="C-Severity.html"><a href="C-Severity.html#S:Loss:Gamma"><i class="fa fa-check"></i><b>3.2.1</b> Distribución gamma</a></li>
<li class="chapter" data-level="3.2.2" data-path="C-Severity.html"><a href="C-Severity.html#distribución-pareto"><i class="fa fa-check"></i><b>3.2.2</b> Distribución Pareto</a></li>
<li class="chapter" data-level="3.2.3" data-path="C-Severity.html"><a href="C-Severity.html#S:LS:Weibull"><i class="fa fa-check"></i><b>3.2.3</b> Distribución Weibull</a></li>
<li class="chapter" data-level="3.2.4" data-path="C-Severity.html"><a href="C-Severity.html#distribución-beta-generalizada-de-segundo-tipo"><i class="fa fa-check"></i><b>3.2.4</b> Distribución Beta Generalizada de segundo tipo</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="C-Severity.html"><a href="C-Severity.html#MethodsCreation"><i class="fa fa-check"></i><b>3.3</b> Métodos para crear distribuciones nuevas</a><ul>
<li class="chapter" data-level="3.3.1" data-path="C-Severity.html"><a href="C-Severity.html#funciones-de-variables-aleatorias-y-sus-distribuciones"><i class="fa fa-check"></i><b>3.3.1</b> Funciones de variables aleatorias y sus distribuciones</a></li>
<li class="chapter" data-level="3.3.2" data-path="C-Severity.html"><a href="C-Severity.html#multiplicación-por-una-constante"><i class="fa fa-check"></i><b>3.3.2</b> Multiplicación por una constante</a></li>
<li class="chapter" data-level="3.3.3" data-path="C-Severity.html"><a href="C-Severity.html#elevación-a-una-potencia"><i class="fa fa-check"></i><b>3.3.3</b> Elevación a una potencia</a></li>
<li class="chapter" data-level="3.3.4" data-path="C-Severity.html"><a href="C-Severity.html#exponenciación"><i class="fa fa-check"></i><b>3.3.4</b> Exponenciación</a></li>
<li class="chapter" data-level="3.3.5" data-path="C-Severity.html"><a href="C-Severity.html#mixturas-finitas"><i class="fa fa-check"></i><b>3.3.5</b> Mixturas finitas</a></li>
<li class="chapter" data-level="3.3.6" data-path="C-Severity.html"><a href="C-Severity.html#mixturas-continuas"><i class="fa fa-check"></i><b>3.3.6</b> Mixturas continuas</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="C-Severity.html"><a href="C-Severity.html#S:CoverageModifications"><i class="fa fa-check"></i><b>3.4</b> Modificaciones de cobertura</a><ul>
<li class="chapter" data-level="3.4.1" data-path="C-Severity.html"><a href="C-Severity.html#S:PolicyDeduct"><i class="fa fa-check"></i><b>3.4.1</b> Franquicias</a></li>
<li class="chapter" data-level="3.4.2" data-path="C-Severity.html"><a href="C-Severity.html#S:PolicyLimits"><i class="fa fa-check"></i><b>3.4.2</b> Límites de la póliza</a></li>
<li class="chapter" data-level="3.4.3" data-path="C-Severity.html"><a href="C-Severity.html#coseguro-e-inflación"><i class="fa fa-check"></i><b>3.4.3</b> Coseguro e inflación</a></li>
<li class="chapter" data-level="3.4.4" data-path="C-Severity.html"><a href="C-Severity.html#S:Chap3Reinsurance"><i class="fa fa-check"></i><b>3.4.4</b> Reaseguro</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="C-Severity.html"><a href="C-Severity.html#S:MaxLikeEstimation"><i class="fa fa-check"></i><b>3.5</b> Estimación por máxima verosimilitud</a><ul>
<li class="chapter" data-level="3.5.1" data-path="C-Severity.html"><a href="C-Severity.html#estimadores-de-máxima-verosimilitud-para-datos-completos"><i class="fa fa-check"></i><b>3.5.1</b> Estimadores de máxima verosimilitud para datos completos</a></li>
<li class="chapter" data-level="3.5.2" data-path="C-Severity.html"><a href="C-Severity.html#S:Loss:MLEModified"><i class="fa fa-check"></i><b>3.5.2</b> Estimadores por máxima verosimilitud usando datos modificados</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="C-Severity.html"><a href="C-Severity.html#LM-further-reading-and-resources"><i class="fa fa-check"></i><b>3.6</b> Recursos y contribuciones adicionales</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html"><i class="fa fa-check"></i><b>4</b> Selección del modelo y estimación</a><ul>
<li class="chapter" data-level="4.1" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#S:MS:NonParInf"><i class="fa fa-check"></i><b>4.1</b> Inferencia No Paramétrica</a><ul>
<li class="chapter" data-level="4.1.1" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#estimación-no-paramétrica"><i class="fa fa-check"></i><b>4.1.1</b> Estimación No Paramétrica</a></li>
<li class="chapter" data-level="4.1.2" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#S:MS:ToolsModelSelection"><i class="fa fa-check"></i><b>4.1.2</b> Herramientas para la Selección de Modelos y Diagnósticos</a></li>
<li class="chapter" data-level="4.1.3" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#valores-iniciales"><i class="fa fa-check"></i><b>4.1.3</b> Valores Iniciales</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#S:MS:ModelSelection"><i class="fa fa-check"></i><b>4.2</b> Selección del Modelo</a><ul>
<li class="chapter" data-level="4.2.1" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#selección-del-modelo-iterativa"><i class="fa fa-check"></i><b>4.2.1</b> Selección del Modelo Iterativa</a></li>
<li class="chapter" data-level="4.2.2" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#selección-de-modelo-basada-en-un-conjunto-de-datos-de-entrenamiento"><i class="fa fa-check"></i><b>4.2.2</b> Selección de Modelo basada en un Conjunto de Datos de Entrenamiento</a></li>
<li class="chapter" data-level="4.2.3" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#selección-de-modelo-basada-en-un-conjunto-de-datos-de-prueba"><i class="fa fa-check"></i><b>4.2.3</b> Selección de Modelo basada en un Conjunto de Datos de Prueba</a></li>
<li class="chapter" data-level="4.2.4" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#selección-del-modelo-basada-en-validación-cruzada"><i class="fa fa-check"></i><b>4.2.4</b> Selección del Modelo basada en Validación Cruzada</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#S:MS:ModifiedData"><i class="fa fa-check"></i><b>4.3</b> Estimación utilizando Datos Modificados</a><ul>
<li class="chapter" data-level="4.3.1" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#estimación-paramétrica-usando-datos-modificados"><i class="fa fa-check"></i><b>4.3.1</b> Estimación Paramétrica usando Datos Modificados</a></li>
<li class="chapter" data-level="4.3.2" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#estimación-no-paramétrica-utilizando-datos-modificados"><i class="fa fa-check"></i><b>4.3.2</b> Estimación no Paramétrica Utilizando Datos Modificados</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#S:MS:BayesInference"><i class="fa fa-check"></i><b>4.4</b> Inferencia Bayesiana</a><ul>
<li class="chapter" data-level="4.4.1" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#S:IntroBayes"><i class="fa fa-check"></i><b>4.4.1</b> Introducción a la Inferencia Bayesiana</a></li>
<li class="chapter" data-level="4.4.2" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#modelo-bayesiano"><i class="fa fa-check"></i><b>4.4.2</b> Modelo Bayesiano</a></li>
<li class="chapter" data-level="4.4.3" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#inferencia-bayesiana"><i class="fa fa-check"></i><b>4.4.3</b> Inferencia Bayesiana</a></li>
<li class="chapter" data-level="4.4.4" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#S:ConjugateDistributions"><i class="fa fa-check"></i><b>4.4.4</b> Distribuciones Conjugadas</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="C-ModelSelection.html"><a href="C-ModelSelection.html#MS:further-reading-and-resources"><i class="fa fa-check"></i><b>4.5</b> Más Recursos y Colaboradores</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html"><i class="fa fa-check"></i><b>5</b> Modelos de pérdidas agregadas</a><ul>
<li class="chapter" data-level="5.1" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#introducción"><i class="fa fa-check"></i><b>5.1</b> Introducción</a></li>
<li class="chapter" data-level="5.2" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#modelo-de-riesgo-individual"><i class="fa fa-check"></i><b>5.2</b> Modelo de riesgo individual</a></li>
<li class="chapter" data-level="5.3" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#modelo-de-riesgo-colectivo"><i class="fa fa-check"></i><b>5.3</b> Modelo de riesgo colectivo</a><ul>
<li class="chapter" data-level="5.3.1" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#momentos-y-distribución"><i class="fa fa-check"></i><b>5.3.1</b> Momentos y distribución</a></li>
<li class="chapter" data-level="5.3.2" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#seguro-stop-loss"><i class="fa fa-check"></i><b>5.3.2</b> Seguro Stop-loss</a></li>
<li class="chapter" data-level="5.3.3" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#resultados-analíticos"><i class="fa fa-check"></i><b>5.3.3</b> Resultados analíticos</a></li>
<li class="chapter" data-level="5.3.4" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#distribución-tweedie"><i class="fa fa-check"></i><b>5.3.4</b> Distribución Tweedie</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#cálculo-de-la-distribución-de-pérdidas-agregadas"><i class="fa fa-check"></i><b>5.4</b> Cálculo de la distribución de pérdidas agregadas</a><ul>
<li class="chapter" data-level="5.4.1" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#método-recursivo"><i class="fa fa-check"></i><b>5.4.1</b> Método recursivo</a></li>
<li class="chapter" data-level="5.4.2" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#simulación"><i class="fa fa-check"></i><b>5.4.2</b> Simulación</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#efectos-de-la-modificación-de-las-coberturas"><i class="fa fa-check"></i><b>5.5</b> Efectos de la modificación de las coberturas</a><ul>
<li class="chapter" data-level="5.5.1" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#impacto-de-la-exposición-en-la-frecuencia"><i class="fa fa-check"></i><b>5.5.1</b> Impacto de la exposición en la frecuencia</a></li>
<li class="chapter" data-level="5.5.2" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#S:MS:DedImpactClmFreq"><i class="fa fa-check"></i><b>5.5.2</b> Impacto de los deducibles en la frecuencia de siniestros</a></li>
<li class="chapter" data-level="5.5.3" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#impacto-de-las-modificaciones-de-la-póliza-en-la-siniestralidad-agregada"><i class="fa fa-check"></i><b>5.5.3</b> Impacto de las modificaciones de la póliza en la siniestralidad agregada</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="C-AggLossModels.html"><a href="C-AggLossModels.html#AL-further-reading-and-resources"><i class="fa fa-check"></i><b>5.6</b> Otros recursos y colaboradores</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="C-Simulation.html"><a href="C-Simulation.html"><i class="fa fa-check"></i><b>6</b> Simulation and Resampling</a><ul>
<li class="chapter" data-level="6.1" data-path="C-Simulation.html"><a href="C-Simulation.html#S:SimulationFundamentals"><i class="fa fa-check"></i><b>6.1</b> Simulation Fundamentals</a><ul>
<li class="chapter" data-level="6.1.1" data-path="C-Simulation.html"><a href="C-Simulation.html#generating-independent-uniform-observations"><i class="fa fa-check"></i><b>6.1.1</b> Generating Independent Uniform Observations</a></li>
<li class="chapter" data-level="6.1.2" data-path="C-Simulation.html"><a href="C-Simulation.html#S:InverseTransform"><i class="fa fa-check"></i><b>6.1.2</b> Inverse Transform Method</a></li>
<li class="chapter" data-level="6.1.3" data-path="C-Simulation.html"><a href="C-Simulation.html#simulation-precision"><i class="fa fa-check"></i><b>6.1.3</b> Simulation Precision</a></li>
<li class="chapter" data-level="6.1.4" data-path="C-Simulation.html"><a href="C-Simulation.html#S:SimulationStatInference"><i class="fa fa-check"></i><b>6.1.4</b> Simulation and Statistical Inference</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="C-Simulation.html"><a href="C-Simulation.html#S:Bootstrap"><i class="fa fa-check"></i><b>6.2</b> Bootstrapping and Resampling</a><ul>
<li class="chapter" data-level="6.2.1" data-path="C-Simulation.html"><a href="C-Simulation.html#bootstrap-foundations"><i class="fa fa-check"></i><b>6.2.1</b> Bootstrap Foundations</a></li>
<li class="chapter" data-level="6.2.2" data-path="C-Simulation.html"><a href="C-Simulation.html#bootstrap-precision-bias-standard-deviation-and-mse"><i class="fa fa-check"></i><b>6.2.2</b> Bootstrap Precision: Bias, Standard Deviation, and MSE</a></li>
<li class="chapter" data-level="6.2.3" data-path="C-Simulation.html"><a href="C-Simulation.html#confidence-intervals"><i class="fa fa-check"></i><b>6.2.3</b> Confidence Intervals</a></li>
<li class="chapter" data-level="6.2.4" data-path="C-Simulation.html"><a href="C-Simulation.html#S:ParametricBootStrap"><i class="fa fa-check"></i><b>6.2.4</b> Parametric Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="C-Simulation.html"><a href="C-Simulation.html#S:CrossValidation"><i class="fa fa-check"></i><b>6.3</b> Cross-Validation</a><ul>
<li class="chapter" data-level="6.3.1" data-path="C-Simulation.html"><a href="C-Simulation.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>6.3.1</b> k-Fold Cross-Validation</a></li>
<li class="chapter" data-level="6.3.2" data-path="C-Simulation.html"><a href="C-Simulation.html#leave-one-out-cross-validation"><i class="fa fa-check"></i><b>6.3.2</b> Leave-One-Out Cross-Validation</a></li>
<li class="chapter" data-level="6.3.3" data-path="C-Simulation.html"><a href="C-Simulation.html#cross-validation-and-bootstrap"><i class="fa fa-check"></i><b>6.3.3</b> Cross-Validation and Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="C-Simulation.html"><a href="C-Simulation.html#S:ImportanceSampling"><i class="fa fa-check"></i><b>6.4</b> Importance Sampling</a></li>
<li class="chapter" data-level="6.5" data-path="C-Simulation.html"><a href="C-Simulation.html#S:MCMC"><i class="fa fa-check"></i><b>6.5</b> Monte Carlo Markov Chain (MCMC)</a><ul>
<li class="chapter" data-level="6.5.1" data-path="C-Simulation.html"><a href="C-Simulation.html#hastings-metropolis"><i class="fa fa-check"></i><b>6.5.1</b> Hastings Metropolis</a></li>
<li class="chapter" data-level="6.5.2" data-path="C-Simulation.html"><a href="C-Simulation.html#gibbs-sampler"><i class="fa fa-check"></i><b>6.5.2</b> Gibbs sampler</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="C-Simulation.html"><a href="C-Simulation.html#Simulation:further-reading-and-resources"><i class="fa fa-check"></i><b>6.6</b> Further Resources and Contributors</a><ul>
<li class="chapter" data-level="6.6.1" data-path="C-Simulation.html"><a href="C-Simulation.html#ts-6.a.-bootstrap-applications-in-predictive-modeling"><i class="fa fa-check"></i><b>6.6.1</b> TS 6.A. Bootstrap Applications in Predictive Modeling</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html"><i class="fa fa-check"></i><b>7</b> Premium Foundations</a><ul>
<li class="chapter" data-level="7.1" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#S:IntroductionRatemaking"><i class="fa fa-check"></i><b>7.1</b> Introduction to Ratemaking</a></li>
<li class="chapter" data-level="7.2" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#S:AggRateMaking"><i class="fa fa-check"></i><b>7.2</b> Aggregate Ratemaking Methods</a><ul>
<li class="chapter" data-level="7.2.1" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#S:PurePremium"><i class="fa fa-check"></i><b>7.2.1</b> Pure Premium Method</a></li>
<li class="chapter" data-level="7.2.2" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#S:LossRatio"><i class="fa fa-check"></i><b>7.2.2</b> Loss Ratio Method</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#S:PricingPrinciples"><i class="fa fa-check"></i><b>7.3</b> Pricing Principles</a><ul>
<li class="chapter" data-level="7.3.1" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#premium-principles"><i class="fa fa-check"></i><b>7.3.1</b> Premium Principles</a></li>
<li class="chapter" data-level="7.3.2" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#properties-of-premium-principles"><i class="fa fa-check"></i><b>7.3.2</b> Properties of Premium Principles</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#S:HeterogeneousRisks"><i class="fa fa-check"></i><b>7.4</b> Heterogeneous Risks</a><ul>
<li class="chapter" data-level="7.4.1" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#S:ExposureToRisk"><i class="fa fa-check"></i><b>7.4.1</b> Exposure to Risk</a></li>
<li class="chapter" data-level="7.4.2" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#S:RatingFactors"><i class="fa fa-check"></i><b>7.4.2</b> Rating Factors</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#S:TrendDevelopment"><i class="fa fa-check"></i><b>7.5</b> Development and Trending</a><ul>
<li class="chapter" data-level="7.5.1" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#exposures-and-premiums"><i class="fa fa-check"></i><b>7.5.1</b> Exposures and Premiums</a></li>
<li class="chapter" data-level="7.5.2" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#losses-claims-and-payments"><i class="fa fa-check"></i><b>7.5.2</b> Losses, Claims, and Payments</a></li>
<li class="chapter" data-level="7.5.3" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#S:CompareMethods"><i class="fa fa-check"></i><b>7.5.3</b> Comparing Pure Premium and Loss Ratio Methods</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#S:GiniStatistic"><i class="fa fa-check"></i><b>7.6</b> Selecting a Premium</a><ul>
<li class="chapter" data-level="7.6.1" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#classic-lorenz-curve"><i class="fa fa-check"></i><b>7.6.1</b> Classic Lorenz Curve</a></li>
<li class="chapter" data-level="7.6.2" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#performance-curve-and-a-gini-statistic"><i class="fa fa-check"></i><b>7.6.2</b> Performance Curve and a Gini Statistic</a></li>
<li class="chapter" data-level="7.6.3" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#out-of-sample-validation"><i class="fa fa-check"></i><b>7.6.3</b> Out-of-Sample Validation</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#further-resources-and-contributors"><i class="fa fa-check"></i><b>7.7</b> Further Resources and Contributors</a><ul>
<li class="chapter" data-level="" data-path="C-PremiumFoundations.html"><a href="C-PremiumFoundations.html#ts-7.a.-rate-regulation"><i class="fa fa-check"></i>TS 7.A. Rate Regulation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="C-RiskClass.html"><a href="C-RiskClass.html"><i class="fa fa-check"></i><b>8</b> Clasificación de riesgos</a><ul>
<li class="chapter" data-level="8.1" data-path="C-RiskClass.html"><a href="C-RiskClass.html#S:RC:Introduction"><i class="fa fa-check"></i><b>8.1</b> Introducción</a></li>
<li class="chapter" data-level="8.2" data-path="C-RiskClass.html"><a href="C-RiskClass.html#S:RC:PoissonRegression"><i class="fa fa-check"></i><b>8.2</b> Modelo de Regresión de Poisson</a><ul>
<li class="chapter" data-level="8.2.1" data-path="C-RiskClass.html"><a href="C-RiskClass.html#S:RC:Need.Poi.reg"><i class="fa fa-check"></i><b>8.2.1</b> Necesidad de la Regresión de Poisson</a></li>
<li class="chapter" data-level="8.2.2" data-path="C-RiskClass.html"><a href="C-RiskClass.html#regresión-de-poisson"><i class="fa fa-check"></i><b>8.2.2</b> Regresión de Poisson</a></li>
<li class="chapter" data-level="8.2.3" data-path="C-RiskClass.html"><a href="C-RiskClass.html#incorporación-de-la-exposición"><i class="fa fa-check"></i><b>8.2.3</b> Incorporación de la exposición</a></li>
<li class="chapter" data-level="8.2.4" data-path="C-RiskClass.html"><a href="C-RiskClass.html#ejercicios-3"><i class="fa fa-check"></i><b>8.2.4</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="C-RiskClass.html"><a href="C-RiskClass.html#S:CatVarMultiTarriff"><i class="fa fa-check"></i><b>8.3</b> Variables Categóricas y Tarifa Multiplicativa</a><ul>
<li class="chapter" data-level="8.3.1" data-path="C-RiskClass.html"><a href="C-RiskClass.html#factores-de-tarificación-y-tarifa"><i class="fa fa-check"></i><b>8.3.1</b> Factores de Tarificación y Tarifa</a></li>
<li class="chapter" data-level="8.3.2" data-path="C-RiskClass.html"><a href="C-RiskClass.html#modelo-de-tarifa-multiplicativa"><i class="fa fa-check"></i><b>8.3.2</b> Modelo de Tarifa Multiplicativa</a></li>
<li class="chapter" data-level="8.3.3" data-path="C-RiskClass.html"><a href="C-RiskClass.html#regresión-de-poisson-para-la-tarifa-multiplicativa"><i class="fa fa-check"></i><b>8.3.3</b> Regresión de Poisson para la Tarifa Multiplicativa</a></li>
<li class="chapter" data-level="8.3.4" data-path="C-RiskClass.html"><a href="C-RiskClass.html#ejemplos-numéricos"><i class="fa fa-check"></i><b>8.3.4</b> Ejemplos numéricos</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="C-RiskClass.html"><a href="C-RiskClass.html#RC:further-reading-and-resources"><i class="fa fa-check"></i><b>8.4</b> Más recursos y colaboradores</a><ul>
<li class="chapter" data-level="" data-path="C-RiskClass.html"><a href="C-RiskClass.html#ts-8.a-estimación-de-modelos-de-regresión-de-poisson"><i class="fa fa-check"></i>TS 8.A – Estimación de Modelos de Regresión de Poisson</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="C-Credibility.html"><a href="C-Credibility.html"><i class="fa fa-check"></i><b>9</b> Experience Rating Using Credibility Theory</a><ul>
<li class="chapter" data-level="9.1" data-path="C-Credibility.html"><a href="C-Credibility.html#introduction-to-applications-of-credibility-theory"><i class="fa fa-check"></i><b>9.1</b> Introduction to Applications of Credibility Theory</a></li>
<li class="chapter" data-level="9.2" data-path="C-Credibility.html"><a href="C-Credibility.html#limited-fluctuation-credibility"><i class="fa fa-check"></i><b>9.2</b> Limited Fluctuation Credibility</a><ul>
<li class="chapter" data-level="9.2.1" data-path="C-Credibility.html"><a href="C-Credibility.html#S:frequency"><i class="fa fa-check"></i><b>9.2.1</b> Full Credibility for Claim Frequency</a></li>
<li class="chapter" data-level="9.2.2" data-path="C-Credibility.html"><a href="C-Credibility.html#full-credibility-for-aggregate-losses-and-pure-premium"><i class="fa fa-check"></i><b>9.2.2</b> Full Credibility for Aggregate Losses and Pure Premium</a></li>
<li class="chapter" data-level="9.2.3" data-path="C-Credibility.html"><a href="C-Credibility.html#full-credibility-for-severity"><i class="fa fa-check"></i><b>9.2.3</b> Full Credibility for Severity</a></li>
<li class="chapter" data-level="9.2.4" data-path="C-Credibility.html"><a href="C-Credibility.html#partial-credibility"><i class="fa fa-check"></i><b>9.2.4</b> Partial Credibility</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="C-Credibility.html"><a href="C-Credibility.html#bühlmann-credibility"><i class="fa fa-check"></i><b>9.3</b> Bühlmann Credibility</a><ul>
<li class="chapter" data-level="9.3.1" data-path="C-Credibility.html"><a href="C-Credibility.html#S:EPV-VHM-Z"><i class="fa fa-check"></i><b>9.3.1</b> Credibility Z, <em>EPV</em>, and <em>VHM</em></a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="C-Credibility.html"><a href="C-Credibility.html#bühlmann-straub-credibility"><i class="fa fa-check"></i><b>9.4</b> Bühlmann-Straub Credibility</a></li>
<li class="chapter" data-level="9.5" data-path="C-Credibility.html"><a href="C-Credibility.html#bayesian-inference-and-bühlmann-credibility"><i class="fa fa-check"></i><b>9.5</b> Bayesian Inference and Bühlmann Credibility</a><ul>
<li class="chapter" data-level="9.5.1" data-path="C-Credibility.html"><a href="C-Credibility.html#gamma-poisson-model"><i class="fa fa-check"></i><b>9.5.1</b> Gamma-Poisson Model</a></li>
<li class="chapter" data-level="9.5.2" data-path="C-Credibility.html"><a href="C-Credibility.html#beta-binomial-model"><i class="fa fa-check"></i><b>9.5.2</b> Beta-Binomial Model</a></li>
<li class="chapter" data-level="9.5.3" data-path="C-Credibility.html"><a href="C-Credibility.html#exact-credibility"><i class="fa fa-check"></i><b>9.5.3</b> Exact Credibility</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="C-Credibility.html"><a href="C-Credibility.html#estimating-credibility-parameters"><i class="fa fa-check"></i><b>9.6</b> Estimating Credibility Parameters</a><ul>
<li class="chapter" data-level="9.6.1" data-path="C-Credibility.html"><a href="C-Credibility.html#full-credibility-standard-for-limited-fluctuation-credibility"><i class="fa fa-check"></i><b>9.6.1</b> Full Credibility Standard for Limited Fluctuation Credibility</a></li>
<li class="chapter" data-level="9.6.2" data-path="C-Credibility.html"><a href="C-Credibility.html#nonparametric-estimation-for-bühlmann-and-bühlmann-straub-models"><i class="fa fa-check"></i><b>9.6.2</b> Nonparametric Estimation for Bühlmann and Bühlmann-Straub Models</a></li>
<li class="chapter" data-level="9.6.3" data-path="C-Credibility.html"><a href="C-Credibility.html#semiparametric-estimation-for-bühlmann-and-bühlmann-straub-models"><i class="fa fa-check"></i><b>9.6.3</b> Semiparametric Estimation for Bühlmann and Bühlmann-Straub Models</a></li>
<li class="chapter" data-level="9.6.4" data-path="C-Credibility.html"><a href="C-Credibility.html#balancing-credibility-estimators"><i class="fa fa-check"></i><b>9.6.4</b> Balancing Credibility Estimators</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="C-Credibility.html"><a href="C-Credibility.html#Cred-further-reading-and-resources"><i class="fa fa-check"></i><b>9.7</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="C-PortMgt.html"><a href="C-PortMgt.html"><i class="fa fa-check"></i><b>10</b> Gestión de Carteras de Seguros incluyendo Reaseguro</a><ul>
<li class="chapter" data-level="10.1" data-path="C-PortMgt.html"><a href="C-PortMgt.html#introducción-a-las-carteras-de-seguros"><i class="fa fa-check"></i><b>10.1</b> Introducción a las Carteras de Seguros</a></li>
<li class="chapter" data-level="10.2" data-path="C-PortMgt.html"><a href="C-PortMgt.html#S:Tails"><i class="fa fa-check"></i><b>10.2</b> Colas de las Distribuciones</a><ul>
<li class="chapter" data-level="10.2.1" data-path="C-PortMgt.html"><a href="C-PortMgt.html#clasificación-basada-en-los-momentos"><i class="fa fa-check"></i><b>10.2.1</b> Clasificación Basada en los Momentos</a></li>
<li class="chapter" data-level="10.2.2" data-path="C-PortMgt.html"><a href="C-PortMgt.html#comparación-basada-en-el-comportamiento-de-colas-con-límites"><i class="fa fa-check"></i><b>10.2.2</b> Comparación basada en el comportamiento de colas con límites</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="C-PortMgt.html"><a href="C-PortMgt.html#S:RiskMeasure"><i class="fa fa-check"></i><b>10.3</b> Medidas de Riesgo</a><ul>
<li class="chapter" data-level="10.3.1" data-path="C-PortMgt.html"><a href="C-PortMgt.html#medidas-de-riesgo-coherentes"><i class="fa fa-check"></i><b>10.3.1</b> Medidas de Riesgo Coherentes</a></li>
<li class="chapter" data-level="10.3.2" data-path="C-PortMgt.html"><a href="C-PortMgt.html#valor-en-riesgo"><i class="fa fa-check"></i><b>10.3.2</b> Valor en Riesgo</a></li>
<li class="chapter" data-level="10.3.3" data-path="C-PortMgt.html"><a href="C-PortMgt.html#cola-del-valor-en-riesgo"><i class="fa fa-check"></i><b>10.3.3</b> Cola del Valor en Riesgo</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="C-PortMgt.html"><a href="C-PortMgt.html#S:Reaseguro"><i class="fa fa-check"></i><b>10.4</b> Reaseguro</a><ul>
<li class="chapter" data-level="10.4.1" data-path="C-PortMgt.html"><a href="C-PortMgt.html#S:ProportionalRe"><i class="fa fa-check"></i><b>10.4.1</b> Reaseguro Proporcional</a></li>
<li class="chapter" data-level="10.4.2" data-path="C-PortMgt.html"><a href="C-PortMgt.html#S:NonProportionalRe"><i class="fa fa-check"></i><b>10.4.2</b> Reaseguro No-Proporcional</a></li>
<li class="chapter" data-level="10.4.3" data-path="C-PortMgt.html"><a href="C-PortMgt.html#S:AdditionalRe"><i class="fa fa-check"></i><b>10.4.3</b> Acuerdos de Reaseguro Adicionales</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="C-PortMgt.html"><a href="C-PortMgt.html#recursos-y-colaboradores-adicionales"><i class="fa fa-check"></i><b>10.5</b> Recursos y Colaboradores adicionales</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="C-LossReserves.html"><a href="C-LossReserves.html"><i class="fa fa-check"></i><b>11</b> Loss Reserving</a><ul>
<li class="chapter" data-level="11.1" data-path="C-LossReserves.html"><a href="C-LossReserves.html#S:motivation"><i class="fa fa-check"></i><b>11.1</b> Motivation</a><ul>
<li class="chapter" data-level="11.1.1" data-path="C-LossReserves.html"><a href="C-LossReserves.html#S:claim-types"><i class="fa fa-check"></i><b>11.1.1</b> Closed, IBNR, and RBNS Claims</a></li>
<li class="chapter" data-level="11.1.2" data-path="C-LossReserves.html"><a href="C-LossReserves.html#why-reserving"><i class="fa fa-check"></i><b>11.1.2</b> Why Reserving?</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="C-LossReserves.html"><a href="C-LossReserves.html#S:Data"><i class="fa fa-check"></i><b>11.2</b> Loss Reserve Data</a><ul>
<li class="chapter" data-level="11.2.1" data-path="C-LossReserves.html"><a href="C-LossReserves.html#from-micro-to-macro"><i class="fa fa-check"></i><b>11.2.1</b> From Micro to Macro</a></li>
<li class="chapter" data-level="11.2.2" data-path="C-LossReserves.html"><a href="C-LossReserves.html#run-off-triangles"><i class="fa fa-check"></i><b>11.2.2</b> Run-off Triangles</a></li>
<li class="chapter" data-level="11.2.3" data-path="C-LossReserves.html"><a href="C-LossReserves.html#loss-reserve-notation"><i class="fa fa-check"></i><b>11.2.3</b> Loss Reserve Notation</a></li>
<li class="chapter" data-level="11.2.4" data-path="C-LossReserves.html"><a href="C-LossReserves.html#S:Rcode"><i class="fa fa-check"></i><b>11.2.4</b> R Code to Summarize Loss Reserve Data</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="C-LossReserves.html"><a href="C-LossReserves.html#S:Chain-ladder"><i class="fa fa-check"></i><b>11.3</b> The Chain-Ladder</a><ul>
<li class="chapter" data-level="11.3.1" data-path="C-LossReserves.html"><a href="C-LossReserves.html#S:DeterministicCL"><i class="fa fa-check"></i><b>11.3.1</b> The Deterministic Chain-Ladder</a></li>
<li class="chapter" data-level="11.3.2" data-path="C-LossReserves.html"><a href="C-LossReserves.html#macks-distribution-free-chain-ladder-model"><i class="fa fa-check"></i><b>11.3.2</b> Mack’s Distribution-Free Chain-Ladder Model</a></li>
<li class="chapter" data-level="11.3.3" data-path="C-LossReserves.html"><a href="C-LossReserves.html#r-code-for-chain-ladder-predictions"><i class="fa fa-check"></i><b>11.3.3</b> R code for Chain-Ladder Predictions</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="C-LossReserves.html"><a href="C-LossReserves.html#S:GLMs"><i class="fa fa-check"></i><b>11.4</b> GLMs and Bootstrap for Loss Reserves</a><ul>
<li class="chapter" data-level="11.4.1" data-path="C-LossReserves.html"><a href="C-LossReserves.html#model-specification"><i class="fa fa-check"></i><b>11.4.1</b> Model Specification</a></li>
<li class="chapter" data-level="11.4.2" data-path="C-LossReserves.html"><a href="C-LossReserves.html#model-estimation-and-prediction"><i class="fa fa-check"></i><b>11.4.2</b> Model Estimation and Prediction</a></li>
<li class="chapter" data-level="11.4.3" data-path="C-LossReserves.html"><a href="C-LossReserves.html#bootstrap"><i class="fa fa-check"></i><b>11.4.3</b> Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="C-LossReserves.html"><a href="C-LossReserves.html#LossRe:further-reading-and-resources"><i class="fa fa-check"></i><b>11.5</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html"><i class="fa fa-check"></i><b>12</b> Experience Rating using Bonus-Malus</a><ul>
<li class="chapter" data-level="12.1" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#S:ERBM:Intro"><i class="fa fa-check"></i><b>12.1</b> Introduction</a></li>
<li class="chapter" data-level="12.2" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#S:ERBM:NCD"><i class="fa fa-check"></i><b>12.2</b> NCD System in Several Countries</a><ul>
<li class="chapter" data-level="12.2.1" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#ncd-system-in-malaysia"><i class="fa fa-check"></i><b>12.2.1</b> NCD System in Malaysia</a></li>
<li class="chapter" data-level="12.2.2" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#ncd-system-in-other-countries"><i class="fa fa-check"></i><b>12.2.2</b> NCD System in Other Countries</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#S:ERBM:BMS"><i class="fa fa-check"></i><b>12.3</b> BMS and Markov Chain Model</a><ul>
<li class="chapter" data-level="12.3.1" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#transition-probability"><i class="fa fa-check"></i><b>12.3.1</b> Transition Probability</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#S:ERBM:StatDist"><i class="fa fa-check"></i><b>12.4</b> BMS and Stationary Distribution</a><ul>
<li class="chapter" data-level="12.4.1" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#stationary-distribution"><i class="fa fa-check"></i><b>12.4.1</b> Stationary Distribution</a></li>
<li class="chapter" data-level="12.4.2" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#r-program-for-stationary-distribution"><i class="fa fa-check"></i><b>12.4.2</b> R Program for Stationary Distribution</a></li>
<li class="chapter" data-level="12.4.3" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#premium-evolution"><i class="fa fa-check"></i><b>12.4.3</b> Premium Evolution</a></li>
<li class="chapter" data-level="12.4.4" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#r-program-for-premium-evolution"><i class="fa fa-check"></i><b>12.4.4</b> R Program for Premium Evolution</a></li>
<li class="chapter" data-level="12.4.5" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#convergence-rate"><i class="fa fa-check"></i><b>12.4.5</b> Convergence Rate</a></li>
<li class="chapter" data-level="12.4.6" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#r-program-for-convergence-rate"><i class="fa fa-check"></i><b>12.4.6</b> R Program for Convergence Rate</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#S:PremRtg"><i class="fa fa-check"></i><b>12.5</b> BMS and Premium Rating</a><ul>
<li class="chapter" data-level="12.5.1" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#premium-rating"><i class="fa fa-check"></i><b>12.5.1</b> Premium Rating</a></li>
<li class="chapter" data-level="12.5.2" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#frequency-model-poisson-and-negative-binomial-regressions"><i class="fa fa-check"></i><b>12.5.2</b> Frequency Model – Poisson and Negative Binomial Regressions</a></li>
<li class="chapter" data-level="12.5.3" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#premium-rating-with-bonus-malus-data"><i class="fa fa-check"></i><b>12.5.3</b> Premium Rating with Bonus-Malus Data</a></li>
<li class="chapter" data-level="12.5.4" data-path="C-BonusMalus.html"><a href="C-BonusMalus.html#premium-rating-without-bonus-malus-data"><i class="fa fa-check"></i><b>12.5.4</b> Premium Rating without Bonus-Malus Data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="C-DataSystems.html"><a href="C-DataSystems.html"><i class="fa fa-check"></i><b>13</b> Data and Systems</a><ul>
<li class="chapter" data-level="13.1" data-path="C-DataSystems.html"><a href="C-DataSystems.html#data"><i class="fa fa-check"></i><b>13.1</b> Data</a><ul>
<li class="chapter" data-level="13.1.1" data-path="C-DataSystems.html"><a href="C-DataSystems.html#data-types-and-sources"><i class="fa fa-check"></i><b>13.1.1</b> Data Types and Sources</a></li>
<li class="chapter" data-level="13.1.2" data-path="C-DataSystems.html"><a href="C-DataSystems.html#data-structures-and-storage"><i class="fa fa-check"></i><b>13.1.2</b> Data Structures and Storage</a></li>
<li class="chapter" data-level="13.1.3" data-path="C-DataSystems.html"><a href="C-DataSystems.html#data-quality"><i class="fa fa-check"></i><b>13.1.3</b> Data Quality</a></li>
<li class="chapter" data-level="13.1.4" data-path="C-DataSystems.html"><a href="C-DataSystems.html#data-cleaning"><i class="fa fa-check"></i><b>13.1.4</b> Data Cleaning</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="C-DataSystems.html"><a href="C-DataSystems.html#data-analysis-preliminaries"><i class="fa fa-check"></i><b>13.2</b> Data Analysis Preliminaries</a><ul>
<li class="chapter" data-level="13.2.1" data-path="C-DataSystems.html"><a href="C-DataSystems.html#S:process"><i class="fa fa-check"></i><b>13.2.1</b> Data Analysis Process</a></li>
<li class="chapter" data-level="13.2.2" data-path="C-DataSystems.html"><a href="C-DataSystems.html#exploratory-versus-confirmatory"><i class="fa fa-check"></i><b>13.2.2</b> Exploratory versus Confirmatory</a></li>
<li class="chapter" data-level="13.2.3" data-path="C-DataSystems.html"><a href="C-DataSystems.html#supervised-versus-unsupervised"><i class="fa fa-check"></i><b>13.2.3</b> Supervised versus Unsupervised</a></li>
<li class="chapter" data-level="13.2.4" data-path="C-DataSystems.html"><a href="C-DataSystems.html#parametric-versus-nonparametric"><i class="fa fa-check"></i><b>13.2.4</b> Parametric versus Nonparametric</a></li>
<li class="chapter" data-level="13.2.5" data-path="C-DataSystems.html"><a href="C-DataSystems.html#S:expred"><i class="fa fa-check"></i><b>13.2.5</b> Explanation versus Prediction</a></li>
<li class="chapter" data-level="13.2.6" data-path="C-DataSystems.html"><a href="C-DataSystems.html#data-modeling-versus-algorithmic-modeling"><i class="fa fa-check"></i><b>13.2.6</b> Data Modeling versus Algorithmic Modeling</a></li>
<li class="chapter" data-level="13.2.7" data-path="C-DataSystems.html"><a href="C-DataSystems.html#big-data-analysis"><i class="fa fa-check"></i><b>13.2.7</b> Big Data Analysis</a></li>
<li class="chapter" data-level="13.2.8" data-path="C-DataSystems.html"><a href="C-DataSystems.html#reproducible-analysis"><i class="fa fa-check"></i><b>13.2.8</b> Reproducible Analysis</a></li>
<li class="chapter" data-level="13.2.9" data-path="C-DataSystems.html"><a href="C-DataSystems.html#ethical-issues"><i class="fa fa-check"></i><b>13.2.9</b> Ethical Issues</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="C-DataSystems.html"><a href="C-DataSystems.html#data-analysis-techniques"><i class="fa fa-check"></i><b>13.3</b> Data Analysis Techniques</a><ul>
<li class="chapter" data-level="13.3.1" data-path="C-DataSystems.html"><a href="C-DataSystems.html#exploratory-techniques"><i class="fa fa-check"></i><b>13.3.1</b> Exploratory Techniques</a></li>
<li class="chapter" data-level="13.3.2" data-path="C-DataSystems.html"><a href="C-DataSystems.html#confirmatory-techniques"><i class="fa fa-check"></i><b>13.3.2</b> Confirmatory Techniques</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="C-DataSystems.html"><a href="C-DataSystems.html#some-r-functions"><i class="fa fa-check"></i><b>13.4</b> Some R Functions</a></li>
<li class="chapter" data-level="13.5" data-path="C-DataSystems.html"><a href="C-DataSystems.html#summary"><i class="fa fa-check"></i><b>13.5</b> Summary</a></li>
<li class="chapter" data-level="13.6" data-path="C-DataSystems.html"><a href="C-DataSystems.html#DS:further-reading-and-resources"><i class="fa fa-check"></i><b>13.6</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html"><i class="fa fa-check"></i><b>14</b> Dependence Modeling</a><ul>
<li class="chapter" data-level="14.1" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#S:VarTypes"><i class="fa fa-check"></i><b>14.1</b> Variable Types</a><ul>
<li class="chapter" data-level="14.1.1" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#S:QuaVar"><i class="fa fa-check"></i><b>14.1.1</b> Qualitative Variables</a></li>
<li class="chapter" data-level="14.1.2" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#S:QuanVar"><i class="fa fa-check"></i><b>14.1.2</b> Quantitative Variables</a></li>
<li class="chapter" data-level="14.1.3" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#multivariate-variables"><i class="fa fa-check"></i><b>14.1.3</b> Multivariate Variables</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#S:Measures"><i class="fa fa-check"></i><b>14.2</b> Classic Measures of Scalar Associations</a><ul>
<li class="chapter" data-level="14.2.1" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#association-measures-for-quantitative-variables"><i class="fa fa-check"></i><b>14.2.1</b> Association Measures for Quantitative Variables</a></li>
<li class="chapter" data-level="14.2.2" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#rank-based-measures"><i class="fa fa-check"></i><b>14.2.2</b> Rank Based Measures</a></li>
<li class="chapter" data-level="14.2.3" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#nominal-variables"><i class="fa fa-check"></i><b>14.2.3</b> Nominal Variables</a></li>
<li class="chapter" data-level="14.2.4" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#ordinal-variables"><i class="fa fa-check"></i><b>14.2.4</b> Ordinal Variables</a></li>
<li class="chapter" data-level="14.2.5" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#interval-variables"><i class="fa fa-check"></i><b>14.2.5</b> Interval Variables</a></li>
<li class="chapter" data-level="14.2.6" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#discrete-and-continuous-variables"><i class="fa fa-check"></i><b>14.2.6</b> Discrete and Continuous Variables</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#S:Copula"><i class="fa fa-check"></i><b>14.3</b> Introduction to Copulas</a></li>
<li class="chapter" data-level="14.4" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#S:CopAppl"><i class="fa fa-check"></i><b>14.4</b> Application Using Copulas</a><ul>
<li class="chapter" data-level="14.4.1" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#data-description"><i class="fa fa-check"></i><b>14.4.1</b> Data Description</a></li>
<li class="chapter" data-level="14.4.2" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#marginal-models"><i class="fa fa-check"></i><b>14.4.2</b> Marginal Models</a></li>
<li class="chapter" data-level="14.4.3" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#probability-integral-transformation"><i class="fa fa-check"></i><b>14.4.3</b> Probability Integral Transformation</a></li>
<li class="chapter" data-level="14.4.4" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#joint-modeling-with-copula-function"><i class="fa fa-check"></i><b>14.4.4</b> Joint Modeling with Copula Function</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#S:CopTyp"><i class="fa fa-check"></i><b>14.5</b> Types of Copulas</a><ul>
<li class="chapter" data-level="14.5.1" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#elliptical-copulas"><i class="fa fa-check"></i><b>14.5.1</b> Elliptical Copulas</a></li>
<li class="chapter" data-level="14.5.2" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#archimedian-copulas"><i class="fa fa-check"></i><b>14.5.2</b> Archimedian Copulas</a></li>
<li class="chapter" data-level="14.5.3" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#properties-of-copulas"><i class="fa fa-check"></i><b>14.5.3</b> Properties of Copulas</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#S:CopImp"><i class="fa fa-check"></i><b>14.6</b> Why is Dependence Modeling Important?</a></li>
<li class="chapter" data-level="14.7" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#Dep:further-reading-and-resources"><i class="fa fa-check"></i><b>14.7</b> Further Resources and Contributors</a><ul>
<li class="chapter" data-level="" data-path="C-DependenceModel.html"><a href="C-DependenceModel.html#ts-14.a.-other-classic-measures-of-scalar-associations"><i class="fa fa-check"></i>TS 14.A. Other Classic Measures of Scalar Associations</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="C-AppA.html"><a href="C-AppA.html"><i class="fa fa-check"></i><b>15</b> Apéndice A: Revisión de la Inferencia Estadística</a><ul>
<li class="chapter" data-level="15.1" data-path="C-AppA.html"><a href="C-AppA.html#S:AppA:BASIC"><i class="fa fa-check"></i><b>15.1</b> Conceptos Básicos</a><ul>
<li class="chapter" data-level="15.1.1" data-path="C-AppA.html"><a href="C-AppA.html#muestreo-aleatorio"><i class="fa fa-check"></i><b>15.1.1</b> Muestreo Aleatorio</a></li>
<li class="chapter" data-level="15.1.2" data-path="C-AppA.html"><a href="C-AppA.html#distribución-muestral"><i class="fa fa-check"></i><b>15.1.2</b> Distribución Muestral</a></li>
<li class="chapter" data-level="15.1.3" data-path="C-AppA.html"><a href="C-AppA.html#teorema-central-del-límite"><i class="fa fa-check"></i><b>15.1.3</b> Teorema Central del Límite</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="C-AppA.html"><a href="C-AppA.html#S:AppA:PE"><i class="fa fa-check"></i><b>15.2</b> Estimación Puntual y Propiedades</a><ul>
<li class="chapter" data-level="15.2.1" data-path="C-AppA.html"><a href="C-AppA.html#método-de-estimación-de-momentos"><i class="fa fa-check"></i><b>15.2.1</b> Método de Estimación de Momentos</a></li>
<li class="chapter" data-level="15.2.2" data-path="C-AppA.html"><a href="C-AppA.html#S:AppA:MLE"><i class="fa fa-check"></i><b>15.2.2</b> Estimación por Máxima Verosimilitud</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="C-AppA.html"><a href="C-AppA.html#S:AppA:IE"><i class="fa fa-check"></i><b>15.3</b> Estimación de Intervalo</a><ul>
<li class="chapter" data-level="15.3.1" data-path="C-AppA.html"><a href="C-AppA.html#S:AppA:IE:ED"><i class="fa fa-check"></i><b>15.3.1</b> Distribución Exacta para la Media de Muestra Normal</a></li>
<li class="chapter" data-level="15.3.2" data-path="C-AppA.html"><a href="C-AppA.html#propiedades-de-muestra-grande-del-estimador-mle"><i class="fa fa-check"></i><b>15.3.2</b> Propiedades de Muestra Grande del Estimador MLE</a></li>
<li class="chapter" data-level="15.3.3" data-path="C-AppA.html"><a href="C-AppA.html#intervalo-de-confianza"><i class="fa fa-check"></i><b>15.3.3</b> Intervalo de confianza</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="C-AppA.html"><a href="C-AppA.html#S:AppA:HT"><i class="fa fa-check"></i><b>15.4</b> Pruebas de Hipótesis</a><ul>
<li class="chapter" data-level="15.4.1" data-path="C-AppA.html"><a href="C-AppA.html#conceptos-básicos"><i class="fa fa-check"></i><b>15.4.1</b> Conceptos Básicos</a></li>
<li class="chapter" data-level="15.4.2" data-path="C-AppA.html"><a href="C-AppA.html#contraste-t-student-basado-en-el-estimador-mle"><i class="fa fa-check"></i><b>15.4.2</b> Contraste <span class="math inline">\(t\)</span>-Student Basado en el Estimador MLE</a></li>
<li class="chapter" data-level="15.4.3" data-path="C-AppA.html"><a href="C-AppA.html#S:AppA:HT:LRT"><i class="fa fa-check"></i><b>15.4.3</b> Prueba de la Razón de Verosimilitud</a></li>
<li class="chapter" data-level="15.4.4" data-path="C-AppA.html"><a href="C-AppA.html#S:AppA:HT:IC"><i class="fa fa-check"></i><b>15.4.4</b> Criterios de Información</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="C-AppB.html"><a href="C-AppB.html"><i class="fa fa-check"></i><b>16</b> Apéndice B: Esperanzas Iteradas</a><ul>
<li class="chapter" data-level="16.1" data-path="C-AppB.html"><a href="C-AppB.html#S:AppB:CD"><i class="fa fa-check"></i><b>16.1</b> Distribución Condicionada y Esperanza Condicional</a><ul>
<li class="chapter" data-level="16.1.1" data-path="C-AppB.html"><a href="C-AppB.html#distribución-condicional"><i class="fa fa-check"></i><b>16.1.1</b> Distribución Condicional</a></li>
<li class="chapter" data-level="16.1.2" data-path="C-AppB.html"><a href="C-AppB.html#caso-continuo"><i class="fa fa-check"></i><b>16.1.2</b> Caso Continuo</a></li>
<li class="chapter" data-level="16.1.3" data-path="C-AppB.html"><a href="C-AppB.html#esperanza-condicional-y-varianza-condicional"><i class="fa fa-check"></i><b>16.1.3</b> Esperanza Condicional y Varianza Condicional</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="C-AppB.html"><a href="C-AppB.html#S:AppB:IE"><i class="fa fa-check"></i><b>16.2</b> Esperanzas Iteradas y Varianza Total</a><ul>
<li class="chapter" data-level="16.2.1" data-path="C-AppB.html"><a href="C-AppB.html#ley-de-las-esperanzas-iteradas"><i class="fa fa-check"></i><b>16.2.1</b> Ley de las Esperanzas Iteradas</a></li>
<li class="chapter" data-level="16.2.2" data-path="C-AppB.html"><a href="C-AppB.html#ley-de-la-varianza-total"><i class="fa fa-check"></i><b>16.2.2</b> Ley de la Varianza Total</a></li>
<li class="chapter" data-level="16.2.3" data-path="C-AppB.html"><a href="C-AppB.html#aplicación"><i class="fa fa-check"></i><b>16.2.3</b> Aplicación</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="C-AppB.html"><a href="C-AppB.html#S:AppConjugateDistributions"><i class="fa fa-check"></i><b>16.3</b> Distribuciones Conjugadas</a><ul>
<li class="chapter" data-level="16.3.1" data-path="C-AppB.html"><a href="C-AppB.html#familia-exponencial-lineal"><i class="fa fa-check"></i><b>16.3.1</b> Familia Exponencial Lineal</a></li>
<li class="chapter" data-level="16.3.2" data-path="C-AppB.html"><a href="C-AppB.html#distribuciones-conjugadas"><i class="fa fa-check"></i><b>16.3.2</b> Distribuciones Conjugadas</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="C-AppC.html"><a href="C-AppC.html"><i class="fa fa-check"></i><b>17</b> Apéndice C: Teoría de la Máxima Verosimilitud</a><ul>
<li class="chapter" data-level="17.1" data-path="C-AppC.html"><a href="C-AppC.html#S:AppC:LF"><i class="fa fa-check"></i><b>17.1</b> Función de Verosimilitud</a><ul>
<li class="chapter" data-level="17.1.1" data-path="C-AppC.html"><a href="C-AppC.html#la-función-de-verosimilitud-y-de-log-verosimilitud"><i class="fa fa-check"></i><b>17.1.1</b> La Función de Verosimilitud y de Log-verosimilitud</a></li>
<li class="chapter" data-level="17.1.2" data-path="C-AppC.html"><a href="C-AppC.html#propiedades-de-las-funciones-de-verosimilitud"><i class="fa fa-check"></i><b>17.1.2</b> Propiedades de las Funciones de Verosimilitud</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="C-AppC.html"><a href="C-AppC.html#S:AppC:MLE"><i class="fa fa-check"></i><b>17.2</b> Estimadores de Máxima Verosimilitud</a><ul>
<li class="chapter" data-level="17.2.1" data-path="C-AppC.html"><a href="C-AppC.html#definición-y-derivación-del-estimador-mle"><i class="fa fa-check"></i><b>17.2.1</b> Definición y Derivación del Estimador MLE</a></li>
<li class="chapter" data-level="17.2.2" data-path="C-AppC.html"><a href="C-AppC.html#propiedades-asintóticas-del-estimador-mle"><i class="fa fa-check"></i><b>17.2.2</b> Propiedades Asintóticas del Estimador MLE</a></li>
<li class="chapter" data-level="17.2.3" data-path="C-AppC.html"><a href="C-AppC.html#uso-de-la-estimación-de-máxima-verosimilitud"><i class="fa fa-check"></i><b>17.2.3</b> Uso de la Estimación de Máxima Verosimilitud</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="C-AppC.html"><a href="C-AppC.html#S:AppC:SI"><i class="fa fa-check"></i><b>17.3</b> Inferencia Estadística Basada en la Estimación de Báxima Verosimilitud</a><ul>
<li class="chapter" data-level="17.3.1" data-path="C-AppC.html"><a href="C-AppC.html#contraste-de-hipótesis"><i class="fa fa-check"></i><b>17.3.1</b> Contraste de Hipótesis</a></li>
<li class="chapter" data-level="17.3.2" data-path="C-AppC.html"><a href="C-AppC.html#S:AppC:MLEModelVal"><i class="fa fa-check"></i><b>17.3.2</b> Validación del Modelo y MLE</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="C-SummaryDistributions.html"><a href="C-SummaryDistributions.html"><i class="fa fa-check"></i><b>18</b> Appendix D: Summary of Distributions</a><ul>
<li class="chapter" data-level="18.1" data-path="C-SummaryDistributions.html"><a href="C-SummaryDistributions.html#discrete-distributions"><i class="fa fa-check"></i><b>18.1</b> Discrete Distributions</a><ul>
<li class="chapter" data-level="18.1.1" data-path="C-SummaryDistributions.html"><a href="C-SummaryDistributions.html#the-ab0-class"><i class="fa fa-check"></i><b>18.1.1</b> The (a,b,0) Class</a></li>
<li class="chapter" data-level="18.1.2" data-path="C-SummaryDistributions.html"><a href="C-SummaryDistributions.html#the-ab1-class"><i class="fa fa-check"></i><b>18.1.2</b> The (a,b,1) Class</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="C-SummaryDistributions.html"><a href="C-SummaryDistributions.html#continuous-distributions"><i class="fa fa-check"></i><b>18.2</b> Continuous Distributions</a><ul>
<li class="chapter" data-level="18.2.1" data-path="C-SummaryDistributions.html"><a href="C-SummaryDistributions.html#one-parameter-distributions"><i class="fa fa-check"></i><b>18.2.1</b> One Parameter Distributions</a></li>
<li class="chapter" data-level="18.2.2" data-path="C-SummaryDistributions.html"><a href="C-SummaryDistributions.html#two-parameter-distributions"><i class="fa fa-check"></i><b>18.2.2</b> Two Parameter Distributions</a></li>
<li class="chapter" data-level="18.2.3" data-path="C-SummaryDistributions.html"><a href="C-SummaryDistributions.html#three-parameter-distributions"><i class="fa fa-check"></i><b>18.2.3</b> Three Parameter Distributions</a></li>
<li class="chapter" data-level="18.2.4" data-path="C-SummaryDistributions.html"><a href="C-SummaryDistributions.html#four-parameter-distribution"><i class="fa fa-check"></i><b>18.2.4</b> Four Parameter Distribution</a></li>
<li class="chapter" data-level="18.2.5" data-path="C-SummaryDistributions.html"><a href="C-SummaryDistributions.html#other-distributions"><i class="fa fa-check"></i><b>18.2.5</b> Other Distributions</a></li>
<li class="chapter" data-level="18.2.6" data-path="C-SummaryDistributions.html"><a href="C-SummaryDistributions.html#distributions-with-finite-support"><i class="fa fa-check"></i><b>18.2.6</b> Distributions with Finite Support</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="C-SummaryDistributions.html"><a href="C-SummaryDistributions.html#limited-expected-values"><i class="fa fa-check"></i><b>18.3</b> Limited Expected Values</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/OpenActTexts/Loss-Data-Analytics" target="blank">Loss Data Analytics on GitHub</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Loss Data Analytics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="C:Frequency-Modeling" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Modelización de la frecuencia</h1>
<p><em>Resumen del capítulo.</em> Uno de los objetivos primordiales de las aseguradoras es estimar la magnitud de las pérdidas agregadas que debe soportar en virtud de sus contratos de seguro. Las pérdidas agregadas se ven afectadas tanto por la frecuencia de los eventos asegurados como por la cuantía del evento asegurado. Descomponer las pérdidas agregadas en estos dos componentes, cada uno de los cuales requiere una gran atención, es fundamental para el análisis y tarificación. En este capítulo se examinan las distribuciones de frecuencia, las medidas resumen y las técnicas de estimación de los parámetros.</p>
<p>En la Sección <a href="C-Frequency-Modeling.html#S:frequency-distributions">2.1</a>, presentamos la terminología y discutimos las razones por las que estudiamos la frecuencia y la cuantía por separado. Los fundamentos de las distribuciones y medidas de frecuencia se presentan en la Sección <a href="C-Frequency-Modeling.html#S:basic-frequency-distributions">2.2</a> junto con tres de las principales distribuciones: binomial, Poisson, y binomial negativa. Estas tres distribuciones son miembros de lo que se conoce como la clase de distribuciones (a,b,0), una característica distintiva e identificadora que permite un cálculo eficiente de las probabilidades, que se discute con más detalle en la Sección <a href="C-Frequency-Modeling.html#S:the-a-b-0-class">2.3</a>. Cuando se ajusta una distribución a un conjunto de datos, es necesario estimar los valores de los parámetros y en la Sección <a href="C-Frequency-Modeling.html#S:estimating-frequency-distributions">2.4</a>, se explica el procedimiento para la estimación por máxima verosimilitud. En el caso de los datos de seguros, la observación de 0, que indica la ocurrencia de cero de un evento concreto, es frecuente y puede merecer una atención adicional. Según el tipo de datos, y explicado con más detalle en la Sección <a href="C-Frequency-Modeling.html#S:other-frequency-distributions">2.5</a>, puede ser imposible tener un cero del suceso estudiado, o la probabilidad de cero puede ser de tal magnitud que el ajuste directo llevaría a estimaciones inadecuadas. Las técnicas de truncamiento o modificación del cero permiten un mejor ajuste de la distribución. Cabe señalar que nuestra cartera de seguros podría estar compuesta por diferentes subgrupos, cada uno con su propio conjunto de características individuales, la Sección <a href="C-Frequency-Modeling.html#S:mixture-distributions">2.6</a> introduce las distribuciones mixtas y la metodología para permitir esta heterogeneidad dentro de una cartera. La Sección <a href="C-Frequency-Modeling.html#S:goodness-of-fit">2.7</a> describe la bondad de ajuste que mide la razonabilidad de las estimaciones de los parámetros. Los ejercicios se presentan en la Sección <a href="C-Frequency-Modeling.html#S:exercises">2.8</a> and la Sección <a href="C-Frequency-Modeling.html#S:rcode">2.9.1</a> concluye el capítulo con el código <code>R</code> para los gráficos mostrados en la Sección <a href="C-Frequency-Modeling.html#S:estimating-frequency-distributions">2.4</a>.</p>
<div id="S:frequency-distributions" class="section level2">
<h2><span class="header-section-number">2.1</span> Distribuciones de frecuencias</h2>
<hr />
<p>En esta sección se aprende a analizar la importancia de la modelización de las frecuencias en términos</p>
<ul>
<li>contractuales,</li>
<li>de comportamiento,</li>
<li>bases de datos y</li>
<li>razones administrativas/regulatorias.</li>
</ul>
<hr />
<div id="S:how-frequency-augments-severity-information" class="section level3">
<h3><span class="header-section-number">2.1.1</span> Cómo la frecuencia incrementa la información sobre la cuantía</h3>
<div id="S:basic-terminology" class="section level4">
<h4><span class="header-section-number">2.1.1.1</span> Terminología básica</h4>
<p>En este capítulo, <strong>pérdida</strong>, también llamada daño económico, denota la cantidad sufrida por el asegurado. Denominamos <strong>siniestro</strong> para indicar la indemnización cuando ocurre un evento asegurado, por lo tanto, la cantidad que paga el asegurador. Aunque algunos textos utilizan <strong>pérdida</strong> y <strong>siniestro</strong> indistintamente, hacemos aquí una distinción para remarcar cómo las disposiciones contractuales de los seguros, tales como las deducciones y los límites, afectan a la cuantía de la reclamación derivada de una pérdida. La frecuencia representa la frecuencia con la que ocurre un evento asegurado, normalmente dentro de un contrato de póliza. Aquí nos centramos en las variables aleatorias de recuento que representan el número de siniestros, es decir, la frecuencia con la que se produce un evento. La cuantía indica la cantidad, o tamaño, de cada pago de un evento asegurado. En futuros capítulos, se examina el modelo agregado, que combina modelos de frecuencia con modelos de cuantía o severidad.</p>
</div>
<div id="S:the-importance-of-frequency" class="section level4">
<h4><span class="header-section-number">2.1.1.2</span> La importancia de la Frecuencia</h4>
<p>Recordemos en el Capítulo <a href="C-Intro.html#C:Intro">1</a> que fijar el precio de un producto de seguro puede ser un problema complejo. En la fabricación, el coste de un producto es (relativamente) conocido. En otras áreas de servicios financieros, se dispone de precios de mercado. En los seguros, podemos generalizar la fijación de precios de la siguiente manera: empezar con un coste esperado. Añadir “márgenes” para tener en cuenta el riesgo del producto, los gastos incurridos en el mantenimiento del producto y una asignación de beneficios/superávit para el asegurador.</p>
<p>Ese coste esperado para el seguro puede definirse como el número esperado de siniestros por la cantidad esperada por siniestro, es decir, la <em>frecuencia por cuantía</em> esperada. Centrarse en el recuento de siniestros permite al asegurador considerar aquellos factores que afectan directamente a la ocurrencia de una pérdida, generando así potencialmente un siniestro. El proceso de la frecuencia puede entonces modelizarse.</p>
</div>
<div id="S:why-examine-frequency-information" class="section level4">
<h4><span class="header-section-number">2.1.1.3</span> Por qué Examinar la Información de la Frecuencia</h4>
<p>Las aseguradoras y otros interesados, incluidas las organizaciones gubernamentales, tienen diferentes motivos para generar y mantener bases de datos de frecuencias.</p>
<ul>
<li><p><strong>Contractual.</strong> En los contratos de seguro, es común que se enumeren e invoquen deducibles y límites de póliza particulares para cada ocurrencia de un evento asegurado. En consecuencia, los datos de recuento de siniestros generados indicarían el número de siniestros que cumplen esos criterios, ofreciendo una medida única de la frecuencia de los mismos. Por otra parte, los modelos de pérdidas totales aseguradas tendrían que contabilizar los deducibles y los límites de la póliza para cada evento asegurado.</p></li>
<li><p><strong>Conducta.</strong> Al considerar los factores que influyen en la frecuencia de las pérdidas, se debería tener en cuenta el comportamiento de riesgo y de su reducción por parte de los individuos y las empresas. Las variables explicativas (de tarificación) pueden tener diferentes efectos en los modelos de la frecuencia de un evento en contraste con el tamaño del mismo.</p>
<ul>
<li><p>En los seguros de salud, la decisión de utilizar los servicios sanitarios por parte de los individuos, y de reducir al mínimo su utilización mediante medidas de prevención y salud, está relacionada principalmente con sus características personales. El coste por usuario viene determinado por esas características personales, el estado de salud del asegurado, los posibles tratamientos y las decisiones tomadas por el proveedor de atención médica (como el médico) y el paciente. Si bien hay una superposición de esos factores y la forma en que afectan a los costes totales de la atención sanitaria, nos podemos centrar por separado en los factores que afectan la frecuencia de las visitas a los servicios sanitarios y la cuantía de los costes de la atención médica.</p></li>
<li><p>En productos de seguros a particulares, el historial de siniestralidad es un importante factor de suscripción. Es común utilizar un indicador de si el asegurado tuvo o no un siniestro dentro de un determinado período de tiempo antes del contrato. Además, el número de siniestros incurridos por el asegurado en períodos anteriores tiene capacidad predictiva.</p></li>
<li><p>En el seguro del hogar, al modelizar la frecuencia de pérdidas potenciales, el asegurador podría considerar las medidas de prevención de pérdidas que el propietario ha adoptado, como los sistemas de alarma o de seguridad visibles. Por otra parte, al modelizar la cuantía de las pérdidas, el asegurador examinaría los factores que afectan a los costes de reparación y sustitución.</p></li>
</ul></li>
<li><p><strong>Bases de datos</strong>. Las aseguradoras pueden mantener bases de datos separadas que ayudan a desarrollar modelos separados de frecuencia y cuantía. Por ejemplo, un archivo de titulares de pólizas se genera cuando se suscribe una póliza. En este archivo se registra mucha información de suscripción sobre el asegurado o los asegurados, como la edad, el sexo y la información previa sobre siniestralidad; información sobre la póliza como la cobertura, los deducibles y las limitaciones, así como la existencia de reclamaciones de seguro. Un archivo separado, conocido como el archivo de “siniestros”, registra los detalles de la reclamación contra el asegurador, incluyendo la cantidad. (También puede haber un archivo de “pagos” que registra el proceso de los pagos, aunque no nos ocuparemos de eso aquí). Este proceso de recogida de información podría luego extenderse a la modelización por parte de los asegurados como procesos separados de la frecuencia y la cuantía.</p></li>
<li><p><strong>Regulación y Administración.</strong> El seguro es una industria altamente regulada y supervisada, dada su importancia en la provisión de seguridad financiera a los individuos y empresas que se enfrentan a los riesgos. Como parte de sus obligaciones los reguladores exigen habitualmente que se informe sobre el número de reclamaciones y las cantidades. Esto puede deberse a que puede haber definiciones alternativas de “cuantía”, por ejemplo, lo pagado frente a lo incurrido, y hay menos posibilidades de error al informar del número de reclamaciones. Esta vigilancia continua ayuda a garantizar la estabilidad financiera de estas compañías de seguros.</p></li>
</ul>
</div>
</div>
</div>
<div id="S:basic-frequency-distributions" class="section level2">
<h2><span class="header-section-number">2.2</span> Distribuciones de frecuencias elementales</h2>
<hr />
<p>En esta sección, aprenderás a:</p>
<ul>
<li>Determinar los valores que resumen una distribución como la función de distribución y de supervivencia, así como los momentos como la media y la varianza</li>
<li>Definir y calcular las funciones generadoras de momentos y de probabilidades</li>
<li>Describir y comprender las relaciones entre tres importantes distribuciones: binomial, Poisson y binomial negativa</li>
</ul>
<hr />
<p>En esta sección, presentaremos las distribuciones que se utilizan frecuentemente en la práctica actuarial para modelizar los datos de recuento. La variable aleatoria del número de siniestros se denota con <span class="math inline">\(N\)</span>; por su propia naturaleza toma sólo valores enteros no negativos. Por lo tanto, las distribuciones que se muestran a continuación son todas distribuciones discretas con soporte el conjunto de números enteros no negativos <span class="math inline">\(\{0, 1, \ldots \}\)</span>.</p>
<div id="S:foundations" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Fundamentos</h3>
<p>Dado que <span class="math inline">\(N\)</span> es una variable aleatoria discreta que toma valores en <span class="math inline">\(\{0, 1, \ldots \}\)</span>, la descripción completa más natural de su distribución es a través de la especificación de las probabilidades con las que asume cada uno de los valores enteros no negativos. Esto nos lleva al concepto de la función de masa de probabilidad (pmf, según sus siglas en inglés) de <span class="math inline">\(N\)</span>, denotada como <span class="math inline">\(p_N(\cdot)\)</span> y definida de la siguiente manera:</p>
<p><span class="math display">\[\begin{equation}
p_N(k)=\Pr(N=k), \quad \hbox{for } k=0,1,\ldots
\end{equation}\]</span></p>
<p>Cabe señalar que hay descripciones completas, o caracterizaciones, alternativas de la distribución de <span class="math inline">\(N\)</span>; por ejemplo, una de ellas es la función de distribución de <span class="math inline">\(N\)</span> denotada por <span class="math inline">\(F_N(\cdot)\)</span> y definida a continuación:</p>
<p><span class="math display">\[\begin{equation}
F_N(x):=\begin{cases}
\sum\limits_{k=0}^{\lfloor x \rfloor } \Pr(N=k), &amp;x\geq 0;\\
0, &amp; \hbox{en otro caso}.
\end{cases}
\end{equation}\]</span></p>
<p>En lo anterior, <span class="math inline">\(\lfloor \cdot \rfloor\)</span> indica la función entero; <span class="math inline">\(\lfloor x \rfloor\)</span> indica el mayor entero menor o igual a <span class="math inline">\(x\)</span>. Cabe señalar que la función de supervivencia de <span class="math inline">\(N\)</span>, denotada como <span class="math inline">\(S_N(\cdot)\)</span>, se define como la complementaria de <span class="math inline">\(F_N(\cdot)\)</span>, <em>i.e.</em> <span class="math inline">\(S_N(\cdot):=1-F_N(\cdot)\)</span>. Claramente, esta última es otra caracterización de la distribución de <span class="math inline">\(N\)</span>.</p>
<p>A menudo se está interesado en cuantificar un aspecto concreto de la distribución y no su descripción completa. Esto es particularmente útil cuando se comparan distribuciones. La <em>posición central</em> de la distribución es uno de esos aspectos, y hay muchas medidas diferentes que se utilizan frecuentemente para cuantificarlo. De éstas, la media es la más popular; la media de <span class="math inline">\(N\)</span>, denotada por <span class="math inline">\(\mu_N\)</span>,<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> se define como</p>
<p><span class="math display">\[\begin{equation}
\mu_N=\sum_{k=0}^\infty k~p_N(k).
\end{equation}\]</span></p>
<p>Cabe señalar que <span class="math inline">\(\mu_N\)</span> es el valor esperado de la variable aleatoria <span class="math inline">\(N\)</span>, <em>i.e.</em> <span class="math inline">\(\mu_N=\mathrm{E}[N]\)</span>. Esto nos lleva a una clase general de medidas, los momentos de la distribución; el momento <span class="math inline">\(r\)</span>-ésimo de <span class="math inline">\(N\)</span>, para <span class="math inline">\(r&gt; 0\)</span>, se define como <span class="math inline">\(\mathrm{E}{[N^r]}\)</span> y se denota <span class="math inline">\(\mu_N&#39;(r)\)</span>. Por lo tanto, para <span class="math inline">\(r&gt;0\)</span>, tenemos</p>
<p><span class="math display">\[\begin{equation}
\mu_N&#39;(r)= \mathrm{E}{[N^r]}= \sum_{k=0}^\infty k^r~p_N(k).
\end{equation}\]</span></p>
<p>Nótese que <span class="math inline">\(\mu_N&#39;(\cdot)\)</span> es una función no decreciente bien definida que toma los valores en <span class="math inline">\([0,\infty]\)</span>, como <span class="math inline">\(\Pr(N\in\{0, 1, \ldots \})=1\)</span>; también, nótese que <span class="math inline">\(\mu_N=\mu_N&#39;(1)\)</span>. A partir de aquí, cuando nos refiramos a un momento estará implícito que es finito a menos que se mencione lo contrario.</p>
<p>Otro aspecto fundamental de una distribución es su dispersión, y de las diversas medidas de dispersión estudiadas en la literatura, la desviación estándar es la más popular. Para definirla, primero definimos la varianza de <span class="math inline">\(N\)</span>, denotada por <span class="math inline">\(\mathrm{Var}[N]\)</span>, como <span class="math inline">\(\mathrm{Var}[N]:=\mathrm{E}{[(N-\mu_N)^2]}\)</span> cuando <span class="math inline">\(\mu_N\)</span> es finita. Por las propiedades básicas del valor esperado de una variable aleatoria, vemos que <span class="math inline">\(\mathrm{Var}[N]:=\mathrm{E}[N^2]-[\mathrm{E}(N)]^2\)</span>. La desviación estándar de <span class="math inline">\(N\)</span>, denotada por <span class="math inline">\(\sigma_N\)</span>, se define como la raíz cuadrada de <span class="math inline">\(\mathrm{Var}~N\)</span>.
Nótese que esta última queda bien definida como <span class="math inline">\(\mathrm{Var}[N]\)</span>, por su definición de promedio de la desviación respecto a la media al cuadrado, y es no negativa; <span class="math inline">\(\mathrm{Var}[N]\)</span> se denota como <span class="math inline">\(\sigma_N^2\)</span>. Obsérvese que estas dos medidas toman valores en <span class="math inline">\([0,\infty]\)</span>.</p>
</div>
<div id="S:generating-functions" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Funciones Generadoras de Momentos y de Probabilidad</h3>
<p>Ahora presentaremos dos funciones generadoras que son útiles cuando se trabaja con variables de recuento. Recordemos que para una variable aleatoria discreta, la la función generadora de momentos (mgf, según sus siglas en inglés) de <span class="math inline">\(N\)</span>, denotada como <span class="math inline">\(M_N(\cdot)\)</span>, se define como</p>
<p><span class="math display">\[
M_N(t) = \mathrm{E}~{[e^{tN}]} = \sum^{\infty}_{k=0} ~e^{tk}~ p_N(k), \quad t\in \mathbb{R}.
\]</span></p>
<p>Obsérvese que mientras <span class="math inline">\(M_N(\cdot)\)</span> está bien definida ya que es el valor esperado de una variable aleatoria no negativa (<span class="math inline">\(e^{tN}\)</span>), aunque puede tomar el valor <span class="math inline">\(\infty\)</span>. Nótese que para una variable aleatoria de recuento, <span class="math inline">\(M_N(\cdot)\)</span> tiene un valor finito en <span class="math inline">\((-\infty,0]\)</span> con <span class="math inline">\(M_N(0)=1\)</span>. El siguiente teorema, cuya demostración se encuentra en <span class="citation">(Billingsley <a href="#ref-billingsley" role="doc-biblioref">2008</a>)</span> (pages 285-6), justifica su nombre.</p>
<!-- \begin{theorem}\label{freq:thm1} -->

<div class="theorem">
<p><span id="thm:freq-thm1" class="theorem"><strong>Theorem 2.1  </strong></span>Consideremos que <span class="math inline">\(N\)</span> es una variable aleatoria de recuento tal que <span class="math inline">\(\mathrm{E}~{[e^{t^\ast N}]}\)</span> es finita para algún <span class="math inline">\(t^\ast&gt;0\)</span>. Se tiene lo siguiente:</p>
<p>Todos los momentos de <span class="math inline">\(N\)</span> son finitos, <em>i.e.</em>
<span class="math display">\[
\mathrm{E}{[N^r]}&lt;\infty, \quad r \gt 0.
\]</span>
La <em>mgf</em> se puede usar para <em>generar</em> sus momentos de la siguiente forma:
<span class="math display">\[
\left.\frac{{\rm d}^m}{{\rm d}t^m} M_N(t)\right\vert_{t=0}=\mathrm{E}{N^m}, \quad m\geq 1.
\]</span>
La <em>mgf</em> <span class="math inline">\(M_N(\cdot)\)</span> caracteriza la distribución; en otras palabras, especifica de manera única la distribución.</p>
</div>

<p>Otro motivo por el que la <em>mgf</em> es muy útil como herramienta es que, para dos variables aleatorias independientes <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span>, con sus mgfs que existen alrededor de <span class="math inline">\(0\)</span>, la <em>mgf</em> de <span class="math inline">\(X+Y\)</span> es el producto de sus respectivas mgfs.</p>
<p>Una función generadora relacionada con la <em>mgf</em> es la función generadora de probabilidad (pgf, según sus siglas en inglés), y es una herramienta útil para las variables aleatorias que toman valores en los números enteros no negativos. Para una variable aleatoria <span class="math inline">\(N\)</span>, por <span class="math inline">\(P_N(\cdot)\)</span> denotamos su <em>pgf</em> y se define como:<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></p>
<p><span class="math display">\[\begin{equation}
P_N(s):=\mathrm{E}~{[s^N]}, \quad s\geq 0.
\end{equation}\]</span></p>
<p>Es sencillo ver que si la <em>mgf</em> <span class="math inline">\(M_N(\cdot)\)</span> existe en <span class="math inline">\((-\infty,t^\ast)\)</span> entonces
<span class="math display">\[
P_N(s)=M_N(\log(s)), \quad s&lt;e^{t^\ast}.
\]</span>
Además, si la <em>pgf</em> existe en el intervalo <span class="math inline">\([0,s^\ast)\)</span> con <span class="math inline">\(s^\ast&gt;1\)</span>, entonces la <em>mgf</em> <span class="math inline">\(M_N(\cdot)\)</span> existe en <span class="math inline">\((-\infty,\log(s^\ast))\)</span>, y especifica la distribución de <span class="math inline">\(N\)</span> de forma única por el Teorema <a href="C-Frequency-Modeling.html#thm:freq-thm1">2.1</a>. El siguiente resultado para <em>pgf</em> es análogo al Teorema <a href="C-Frequency-Modeling.html#thm:freq-thm1">2.1</a>, y en concreto motiva su nombre.</p>
<!-- \begin{theorem}\label{pgfthm} -->

<div class="theorem">
<p><span id="thm:pgfthm" class="theorem"><strong>Theorem 2.2  </strong></span>
Suponer que <span class="math inline">\(N\)</span> es una variable aleatoria de tal manera que <span class="math inline">\(\mathrm{E}~{(s^{\ast})^N}\)</span> es finita para algún <span class="math inline">\(s^\ast&gt;1\)</span>. Se tiene lo siguiente:</p>
Todos los momentos de <span class="math inline">\(N\)</span> son finitos, <em>i.e.</em>
<span class="math display">\[
\mathrm{E}~{N^r}&lt;\infty, \quad r\geq 0.
\]</span>
La <em>pmf</em> de <span class="math inline">\(N\)</span> se puede derivar de la <em>pgf</em> de la siguiente forma:
<span class="math display">\[
p_N(m)=\begin{cases} 
P_N(0), &amp;m=0;\cr
&amp;\cr
\left(\frac{1}{m!}\right) \left.\frac{{\rm d}^m}{{\rm d}s^m} P_N(s)\right\vert_{s=0}\;, &amp;m\geq 1.\cr
\end{cases}
\]</span>
Los momentos factoriales de <span class="math inline">\(N\)</span> se pueden derivar de la siguiente manera:
<span class="math display">\[
\left.\frac{{\rm d}^m}{{\rm d}s^m} P_N(s)\right\vert_{s=1}=\mathrm{E}~{\prod\limits_{i=0}^{m-1} (N-i)}, \quad m\geq 1.
\]</span>
La <em>pgf</em> <span class="math inline">\(P_N(\cdot)\)</span> caracteriza la distribución; en otras palabras, especifica de manera única la distribución.
</div>

</div>
<div id="S:important-frequency-distributions" class="section level3">
<h3><span class="header-section-number">2.2.3</span> Distribuciones de Frecuencias Importantes</h3>
<p>En esta subsección estudiaremos tres importantes distribuciones de frecuencia utilizadas en estadística, que son las distribuciones binomial, Poisson y binomial negativa. En lo siguiente, un riesgo denota una unidad cubierta por el seguro. Un riesgo puede ser un individuo, un edificio, una empresa o algún otro aspecto para el que se proporciona cobertura de seguro. Para contextualizar, imaginemos un conjunto de datos de seguros que contenga el número de siniestros por riesgo o que esté estratificado de alguna otra manera. Las distribuciones mencionadas anteriormente resultan ser también las que más se utilizan en el ámbito asegurador por diferentes razones, algunas de las cuales se mencionan a continuación.</p>
<ul>
<li><p>Estas distribuciones pueden motivarse a partir de experimentos aleatorios que son buenas aproximaciones a los procesos de la vida real de los que surgen muchos datos de seguros. Por lo tanto, no es sorprendente que, en conjunto, ofrezcan un ajuste razonable a muchos conjuntos de datos de interés en seguros. La idoneidad de una distribución concreta para el conjunto de datos puede determinarse utilizando metodologías estadísticas estándar, como se discute más adelante en este capítulo.</p></li>
<li><p>Proporcionan una base suficientemente rica para generar otras distribuciones que se ajustan aún mejor o se adaptan bien a situaciones más reales de interés para nosotros.</p>
<ul>
<li><p>Las tres distribuciones son de un parámetro o dos parámetros. En el ajuste a los datos al parámetro se le asigna un valor concreto. El conjunto de estas distribuciones puede ampliarse hasta sus envolventes convexas tratando el/los parámetro(s) como una variable aleatoria (o vector) con su propia distribución de probabilidad, con este conjunto más amplio de distribuciones que ofrece una mayor flexibilidad. Un ejemplo sencillo que se aborda mejor con esta ampliación es una cartera de siniestros generada por asegurados pertenecientes a muchas clases de riesgo diferentes.</p></li>
<li><p>En los datos de seguros se puede observar un número desproporcionado de ceros, es decir, de cero siniestros por riesgo. Al ajustarse a los datos, la distribución de frecuencias en su especificación estándar a menudo no tiene en cuenta suficientemente este hecho. Sin embargo, la modificación natural de las tres distribuciones anteriores se adapta bien a este fenómeno para ofrecer un mejor ajuste.</p></li>
<li><p>En el seguro nos interesa el total de los siniestros pagados, cuya distribución resulta de la combinación de la distribución de frecuencia ajustada con una distribución de severidad. Estas tres distribuciones tienen propiedades que facilitan trabajar con la distribución de severidad agregada resultante.</p></li>
</ul></li>
</ul>
<!-- \phantom{Es útil poner aquí un ejemplo de distribución discreta con tres valores de soporte y derivar su mgf/pgf/momentos etc..} -->
<div id="S:binomial-distribution" class="section level4">
<h4><span class="header-section-number">2.2.3.1</span> Distribución Binomial</h4>
<p>Empezamos con la distribución binomial que se genera a partir de una secuencia finita de experimentos idénticos e independientes con resultados dicotómicos. El más clásico de estos experimentos es el del lanzamiento de una moneda (trucada o no trucada) con el resultado de cara o cruz. Así, si <span class="math inline">\(N\)</span> denota el número de caras en una secuencia de <span class="math inline">\(m\)</span> experimentos independientes del lanzamiento de una moneda idéntica cuya probabilidad de obtener cara es <span class="math inline">\(q\)</span>, entonces la distribución de <span class="math inline">\(N\)</span> se denomina distribución binomial con parámetros <span class="math inline">\((m,q)\)</span>, con <span class="math inline">\(m\)</span> un entero positivo y <span class="math inline">\(q\in[0,1]\)</span>. Nótese que cuando <span class="math inline">\(q=0\)</span> (resp., <span class="math inline">\(q=1\)</span>) entonces la distribución es degenerada con <span class="math inline">\(N=0\)</span> (resp., <span class="math inline">\(N=m\)</span>) con probabilidad <span class="math inline">\(1\)</span>. De forma clara, cuando <span class="math inline">\(q\in(0,1)\)</span> su suporte es igual a <span class="math inline">\(\{0,1,\ldots,m\}\)</span> con <em>pmf</em> dada por<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a></p>
<p><span class="math display">\[\begin{equation*}
p_k:= \binom{m}{k} q^k (1-q)^{m-k}, \quad k=0,\ldots,m.
\end{equation*}\]</span></p>
<p>donde <span class="math display">\[\binom{m}{k} = \frac{m!}{k!(m-k)!}\]</span></p>
<p>La razón de su nombre es que la <em>pmf</em> toma valores entre los valores que surgen de la expansión binomial <span class="math inline">\((q +(1-q))^m\)</span>. Esta característica nos permite obtener la siguiente expresión para la <em>pgf</em> de la distribución binomial:</p>
<p><span class="math display">\[
P_N(z):= \sum_{k=0}^m z^k \binom{m}{k} q^k (1-q)^{m-k} = \sum_{k=0}^m  \binom{m}{k} (zq)^k (1-q)^{m-k} = (qz+(1-q))^m = (1+q(z-1))^m.
\]</span></p>
<p>Nótese que la expresión anterior para la <em>pgf</em> nos confirma que la distribución binomial es la convolución m-ésima de la distribución de Bernoulli, que a su vez es una distribución binomial con <span class="math inline">\(m=1\)</span> y <em>pgf</em> <span class="math inline">\((1+q(z-1))\)</span>. Además, cabe señalar que la <em>mgf</em> de la distribución binomial viene dada por <span class="math inline">\((1+q(e^t-1))^m\)</span>.</p>
<p>Los momentos centrales de la distribución binomial se pueden encontrar de diferentes maneras. Para enfatizar la propiedad de que se trata de una convulsión <span class="math inline">\(m\)</span>-ésima de la distribución de Bernoulli, derivamos a continuación los momentos que se basan en esta propiedad. Empezamos observando que la distribución de Bernoulli con parámetro <span class="math inline">\(q\)</span> asigna la probabilidad de <span class="math inline">\(q\)</span> y <span class="math inline">\(1-q\)</span> a <span class="math inline">\(1\)</span> y <span class="math inline">\(0\)</span>, respectivamente. Así que su media es igual a <span class="math inline">\(q\)</span> (<span class="math inline">\(=0\times (1-q) + 1\times q\)</span>); nótese que su segundo momento ordinario es igual a su media como <span class="math inline">\(N^2=N\)</span> con probabilidad <span class="math inline">\(1\)</span>. Usando estas dos características vemos que la varianza es igual a <span class="math inline">\(q(1-q)\)</span>. Pasando a la distribución binomial con parámetros <span class="math inline">\(m\)</span> y <span class="math inline">\(q\)</span>, usando el hecho de que es la convolución <span class="math inline">\(m\)</span>-ésima de la distribución de Bernoulli, escribimos <span class="math inline">\(N\)</span> como la suma de <span class="math inline">\(N_1,\ldots,N_m\)</span>, donde <span class="math inline">\(N_i\)</span> son variables de Bernoulli <em>iid</em>. Ahora a partir de los momentos de la Bernoulli y la linealidad de la esperanza, vemos que</p>
<p><span class="math display">\[
\mathrm{E}[{N}]=\mathrm{E}[{\sum_{i=1}^m N_i}] = \sum_{i=1}^m ~\mathrm{E}[N_i] = mq.
\]</span>
Además, dado que la varianza de la suma de variables aleatorias independientes es la suma de sus varianzas, vemos que</p>
<p><span class="math display">\[
\mathrm{Var}[{N}]=\mathrm{Var}~\left({\sum_{i=1}^m N_i}\right)=\sum_{i=1}^m \mathrm{Var}[{N_i}] = mq(1-q).
\]</span></p>
<p>En los ejercicios se proponen formas alternativas de derivar los momentos anteriores. Es importante remarcar, especialmente desde el punto de vista de las aplicaciones, que la media es mayor que la varianza a menos que <span class="math inline">\(q=0\)</span>.</p>
</div>
<div id="S:poisson-distribution" class="section level4">
<h4><span class="header-section-number">2.2.3.2</span> Distribución de Poisson</h4>
<p>Después de la distribución binomial, la distribución de Poisson (llamada así por el polímata francés Simeon Denis Poisson) es probablemente la más conocida de las distribuciones discretas. En parte se debe a que surge de forma natural como la distribución de recuento de las ocurrencias aleatorias de un tipo de evento en un determinado período de tiempo, si la tasa de ocurrencia de los eventos es constante. También surge como el límite asintótico de la distribución binomial con <span class="math inline">\(m\rightarrow \infty\)</span> y <span class="math inline">\(mq\rightarrow \lambda\)</span>.</p>
<p>La distribución de Poisson se parametriza con un único parámetro normalmente denotado por <span class="math inline">\(\lambda\)</span> que toma valores en <span class="math inline">\((0,\infty)\)</span>. Su <em>pmf</em> viene dada por</p>
<p><span class="math display">\[
p_k = \frac{e^{-\lambda}\lambda^k}{k!}, k=0,1,\ldots
\]</span></p>
<p>Es fácil comprobar que la expresión anterior es una <em>pmf</em> ya que los términos son claramente no-negativos, y a partir de la expansión infinita de la serie de Taylor de <span class="math inline">\(e^\lambda\)</span> se obtiene que suman uno. De forma genérica, podemos derivar su <em>pgf</em>, <span class="math inline">\(P(\cdot)\)</span>, como sigue:</p>
<p><span class="math display">\[
P_N(z)= \sum_{k=0}^\infty p_k z^k = \sum_{k=0}^\infty  \frac{e^{-\lambda}\lambda^kz^k}{k!} = e^{-\lambda} e^{\lambda z}
= e^{\lambda(z-1)}, \forall z\in\mathbb{R}.
\]</span></p>
<p>De aquí, derivamos su <em>mgf</em> como sigue:</p>
<p><span class="math display">\[
M_N(t)=P_N(e^t)=e^{\lambda(e^t-1)}, t\in \mathbb{R}.
\]</span></p>
<p>Para obtener su media, observamos que para la distribución de Poisson</p>
<p><span class="math display">\[
kp_k=\begin{cases}
0,  &amp;k=0;\cr
\lambda~p_{k-1}, &amp;k\geq1.
\end{cases}
\]</span></p>
<p>se puede comprobar fácilmente. En particular, lo anterior implica que</p>
<p><span class="math display">\[
\mathrm{E}[{N}]=\sum_{k\geq 0} k~p_k =\lambda\sum_{k\geq 1} p_{k-1} = \lambda\sum_{j\geq 0} p_{j} =\lambda.
\]</span>
De hecho, de manera más general, utilizando una generalización de lo anterior o el Teorema <a href="C-Frequency-Modeling.html#thm:pgfthm">2.2</a>, vemos que
<span class="math display">\[
\mathrm{E}{\prod\limits_{i=0}^{m-1} (N-i)}=\left.\frac{{\rm d}^m}{{\rm d}s^m} P_N(s)\right\vert_{s=1}=\lambda^m, \quad m\geq 1.
\]</span>
En concreto, lo anterior implica que
<span class="math display">\[
\mathrm{Var}[{N}]=\mathrm{E}[{N^2}]-[\mathrm{E}({N})]^2 = \mathrm{E}~[N(N-1)]+\mathrm{E}[N]-(\mathrm{E}[{N]})^2=\lambda^2+\lambda-\lambda^2=\lambda.
\]</span>
Nótese que, curiosamente, para la distribución de Poisson <span class="math inline">\(\mathrm{Var}[N]=\mathrm{E}[N]\)</span>.</p>
</div>
<div id="S:negative-binomial-distribution" class="section level4">
<h4><span class="header-section-number">2.2.3.3</span> Distribución Binomial Negativa</h4>
<p>La tercera distribución importante de recuento es la distribución binomial negativa. Recordemos que la distribución binomial surge como la distribución del número de <em>éxitos</em> en la repetición independiente de <span class="math inline">\(m\)</span> veces de un experimento con resultados binarios o dicotómicos. Si por el contrario, consideramos el número de <em>éxitos</em> hasta que observamos el <span class="math inline">\(r\)</span>-ésimo <em>fallo</em> en repeticiones independientes de un experimento con resultados binarios, entonces su distribución es una distribución binomial negativa. Un caso particular, cuando <span class="math inline">\(r=1\)</span>, es la distribución geométrica. Sin embargo, cuando <span class="math inline">\(r\)</span> no es un número entero, el experimento aleatorio anterior no sería aplicable. A partir de aquí, permitiremos que el parámetro <span class="math inline">\(r\)</span> sea cualquier número real positivo, para luego motivar la distribución de manera más general. Para explicar su nombre, recordemos la serie binomial, <em>i.e.</em></p>
<p><span class="math display">\[
(1+x)^s= 1 + s x + \frac{s(s-1)}{2!}x^2 + \ldots..., \quad s\in\mathbb{R}; \vert x \vert&lt;1.
\]</span>
Si definimos <span class="math inline">\(\binom{s}{k}\)</span>, el coeficiente binomial generalizado, por
<span class="math display">\[
\binom{s}{k}=\frac{s(s-1)\cdots(s-k+1)}{k!},
\]</span>
tenemos, entonces, que
<span class="math display">\[
(1+x)^s= \sum_{k=0}^{\infty} \binom{s}{k} x^k, \quad s\in\mathbb{R}; \vert x \vert&lt;1.
\]</span>
Si fijamos <span class="math inline">\(s=-r\)</span>, entonces observamos que lo anterior genera
<span class="math display">\[
(1-x)^{-r}= 1 + r x + \frac{(r+1)r}{2!}x^2 + \ldots...= \sum_{k=0}^\infty \binom{r+k-1}{k} x^k, \quad r\in\mathbb{R}; \vert x \vert&lt;1.
\]</span>
Lo anterior implica que si definimos <span class="math inline">\(p_k\)</span> como
<span class="math display">\[
p_k = \binom{k+r-1}{k} \left(\frac{1}{1+\beta}\right)^r \left(\frac{\beta}{1+\beta}\right)^k, \quad k=0,1,\ldots
\]</span>
para <span class="math inline">\(r&gt;0\)</span> y <span class="math inline">\(\beta\geq0\)</span>, entonces queda definida una <em>pmf</em> válida. Esta distribución que se ha definido se denomina la distribución negativa binomial con parámetros <span class="math inline">\((r,\beta)\)</span> con <span class="math inline">\(r&gt;0\)</span> y <span class="math inline">\(\beta\geq 0\)</span>. Además, la serie binomial también implica que la <em>pgf</em> de la distribución venga dada por
<span class="math display">\[
\begin{aligned}
  P_N(z) &amp;= (1-\beta(z-1))^{-r}, \quad \vert z \vert \lt 1+\frac{1}{\beta}, \beta\geq0. 
\end{aligned}
\]</span>
Lo anterior implica que la <em>mgf</em> venga dada por
<span class="math display">\[
\begin{aligned}
  M_N(t) &amp;= (1-\beta(e^t-1))^{-r}, \quad t \lt \log\left(1+\frac{1}{\beta}\right), \beta\geq0. 
\end{aligned}
\]</span>
Derivamos sus momentos utilizando el Teorema <a href="C-Frequency-Modeling.html#thm:freq-thm1">2.1</a> como sigue:</p>
<p><span class="math display">\[\begin{eqnarray*}
\mathrm{E}[N]&amp;=&amp;M&#39;(0)= \left. r\beta e^t (1-\beta(e^t-1))^{-r-1}\right\vert_{t=0}=r\beta;\\
\mathrm{E}[N^2]&amp;=&amp;M&#39;&#39;(0)= \left.\left[ r\beta e^t (1-\beta(e^t-1))^{-r-1} + r(r+1)\beta^2 e^{2t} (1-\beta(e^t-1))^{-r-2}\right]\right\vert_{t=0}\\
&amp;=&amp;r\beta(1+\beta)+r^2\beta^2;\\
\hbox{y }\mathrm{Var}[N]&amp;=&amp;\mathrm{E}{[N^2]}-(\mathrm{E}[{N}])^2=r\beta(1+\beta)+r^2\beta^2-r^2\beta^2=r\beta(1+\beta)
\end{eqnarray*}\]</span></p>
<p>Observamos que cuando <span class="math inline">\(\beta&gt;0\)</span>, tenemos <span class="math inline">\(\mathrm{Var}[N] &gt;\mathrm{E}[N]\)</span>. En otras palabras, esta distribución es sobredispersa (en relación a la Poisson); de manera similar, cuando <span class="math inline">\(q&gt;0\)</span> la distribución binomial se dice que es infradispersa (en relación a la Poisson).</p>
<p>Finalmente, observamos que la distribución de Poisson también surge como límite de distribuciones binomiales negativas. Para establecer esto, fijamos <span class="math inline">\(\beta_r\)</span> de tal forma que cuando <span class="math inline">\(r\)</span> tienda a infinito <span class="math inline">\(r\beta_r\)</span> tiende a <span class="math inline">\(\lambda&gt;0\)</span>. Entonces, podemos ver que las mgfs de las distribuciones binomiales negativas con parámetros <span class="math inline">\((r,\beta_r)\)</span> satisfacen</p>
<p><span class="math display">\[
\lim_{r\rightarrow 0} (1-\beta_r(e^t-1))^{-r}=\exp\{\lambda(e^t-1)\},
\]</span>
con el lado derecho de la ecuación anterior siendo la <em>mgf</em> de la distribución de Poisson con parámetro <span class="math inline">\(\lambda\)</span>.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a></p>
</div>
</div>
</div>
<div id="S:the-a-b-0-class" class="section level2">
<h2><span class="header-section-number">2.3</span> La clase (a, b, 0)</h2>
<hr />
<p>En esta sección, se aprende a:</p>
<ul>
<li>Definir la clase (a,b,0) de distribuciones de frecuencia</li>
<li>Discutir la importancia de la relación recursiva que sustenta esta clase de distribuciones</li>
<li>Identificar las condiciones en las que esta clase general se reduce a cada una de las distribuciones binomial, Poisson y binomial negativa</li>
</ul>
<hr />
<p>En la sección anterior estudiamos tres distribuciones, en concreto, la binomial, la Poisson y la binomial negativa. En el caso de la Poisson, para obtener su media usamos el hecho que</p>
<p><span class="math display">\[
kp_k=\lambda p_{k-1}, \quad k\geq 1,
\]</span>
lo que se puede expresar de forma equivalente como
<span class="math display">\[
\frac{p_k}{p_{k-1}}=\frac{\lambda}{k}, \quad k\geq 1. 
\]</span></p>
<p>Curiosamente, se puede mostrar de forma similar que para la distribución binomial</p>
<p><span class="math display">\[
\frac{p_k}{p_{k-1}}=\frac{-q}{1-q}+\left(\frac{(m+1)q}{1-q}\right)\frac{1}{k}, \quad k=1,\ldots,m, 
\]</span>
y para la distribución binomial negativa
<span class="math display">\[
\frac{p_k}{p_{k-1}}=\frac{\beta}{1+\beta}+\left(\frac{(r-1)\beta}{1+\beta}\right)\frac{1}{k}, \quad k\geq 1. 
\]</span>
Las tres relaciones previas son de la forma
<span class="math display" id="eq:ab0">\[\begin{equation}
\frac{p_k}{p_{k-1}}=a+\frac{b}{k}, \quad k\geq 1; 
\tag{2.1}
\end{equation}\]</span></p>
<p>esto plantea la cuestión de si hay otras distribuciones que satisfagan esta relación de recurrencia aparentemente general. Nótese que la relación de la izquierda, el cociente entre dos probabilidades, es no negativa.</p>
<p>Para empezar, permitamos <span class="math inline">\(a&lt;0\)</span>. En este caso, como <span class="math inline">\(k\rightarrow \infty\)</span>, <span class="math inline">\((a+b/k)\rightarrow a&lt;0\)</span>. De esto se deduce que si <span class="math inline">\(a&lt;0\)</span> entonces <span class="math inline">\(b\)</span> debería satisfacer <span class="math inline">\(b=-ka\)</span>, para <span class="math inline">\(k\geq 1\)</span>. Cualquier par <span class="math inline">\((a,b)\)</span> puede escribirse como</p>
<p><span class="math display">\[
\left(\frac{-q}{1-q},\frac{(m+1)q}{1-q}\right), \quad q\in(0,1), m\geq 1;
\]</span>
nótese que en el caso <span class="math inline">\(a&lt;0\)</span> con <span class="math inline">\(a+b=0\)</span> produce la degenerada de una distribución de <span class="math inline">\(0\)</span> que es la distribución binomial con <span class="math inline">\(q=0\)</span> y un arbitrario <span class="math inline">\(m\geq 1\)</span>.</p>
<p>En el caso de <span class="math inline">\(a=0\)</span>, de nuevo por la no negatividad de la proporción <span class="math inline">\(p_k/p_{k-1}\)</span>, tenemos <span class="math inline">\(b\geq 0\)</span>. Si <span class="math inline">\(b=0\)</span> la distribución es degenerada en <span class="math inline">\(0\)</span>, que es una binomial con <span class="math inline">\(q=0\)</span> o una distribución de Poisson con <span class="math inline">\(\lambda=0\)</span> o una distribución binomial negativa con <span class="math inline">\(\beta=0\)</span>. Si <span class="math inline">\(b&gt;0\)</span>, entonces de forma clara esta distribución es una distribución de Poisson con una media (<em>i.e.</em> <span class="math inline">\(\lambda\)</span>) igual a <span class="math inline">\(b\)</span>, como se muestra al principio de esta sección.</p>
<p>En el caso de <span class="math inline">\(a&gt;0\)</span>, de nuevo por la no negatividad de la proporción <span class="math inline">\(p_k/p_{k-1}\)</span>, tenemos <span class="math inline">\(a+b/k\geq 0\)</span> para todo <span class="math inline">\(k\geq 1\)</span>. La más estricta de estas es la desigualdad <span class="math inline">\(a+b\geq 0\)</span>. Nótese que <span class="math inline">\(a+b=0\)</span> de nuevo resulta en una degenerada en <span class="math inline">\(0\)</span>; excluyendo este caso tenemos <span class="math inline">\(a+b&gt;0\)</span> o equivalentemente <span class="math inline">\(b=(r-1)a\)</span> con <span class="math inline">\(r&gt;0\)</span>. Después de un poco de álgebra, fácilmente se obtiene la siguiente expresión para <span class="math inline">\(p_k\)</span>:</p>
<p><span class="math display">\[
p_k = \binom{k+r-1}{k} p_0 a^k, \quad k=1,2,\ldots. 
\]</span>
La serie anterior converge a <span class="math inline">\(a&lt;1\)</span> cuando <span class="math inline">\(r&gt;0\)</span>, con la suma dada por <span class="math inline">\(p_0*((1-a)^{(-r)}-1)\)</span>. Por lo tanto, al igualar este último a <span class="math inline">\(1-p_0\)</span> obtenemos <span class="math inline">\(p_0=(1-a)^{(r)}\)</span>. Así, en este caso el par <span class="math inline">\((a,b)\)</span> es de la forma <span class="math inline">\((a,(r-1)a)\)</span>, para <span class="math inline">\(r&gt;0\)</span> y <span class="math inline">\(0&lt;a&lt;1\)</span>; ya que una parametrización equivalente es <span class="math inline">\((\beta/(1+\beta),(r-1)\beta/(1+\beta))\)</span>, para <span class="math inline">\(r&gt;0\)</span> y <span class="math inline">\(\beta&gt;0\)</span>, vemos de lo anterior que estas distribuciones son distribuciones binomiales negativas.</p>
<p>A partir del desarrollo anterior vemos que no sólo la recurrencia <a href="C-Frequency-Modeling.html#eq:ab0">(2.1)</a> une estas tres distribuciones, sino que también las caracteriza. Por esta razón, estas tres distribuciones se denominan de forma genérica en la literatura actuarial como clase (a,b,0) de distribuciones, con <span class="math inline">\(0\)</span> haciendo referencia al punto inicial de la recurrencia. Nótese que el valor de <span class="math inline">\(p_0\)</span> está implícito en <span class="math inline">\((a,b)\)</span> ya que las probabilidades tienen que sumar uno. Por supuesto, <a href="C-Frequency-Modeling.html#eq:ab0">(2.1)</a> como relación de recurrencia para <span class="math inline">\(p_k\)</span>, hace que el cálculo de la <em>pmf</em> sea eficiente al eliminar las redundancias. Más adelante veremos que lo hace incluso en el caso de distribuciones compuestas con la distribución de frecuencias perteneciente a la clase <span class="math inline">\((a,b,0)\)</span> - esta característica es la razón más importante para justificar el estudio de estas tres distribuciones desde este punto de vista.</p>
<p><strong>Ejemplo 2.3.1.</strong>
Una distribución de probabilidad discreta tiene las siguientes propiedades
<span class="math display">\[
\begin{aligned}
p_k&amp;=c\left( 1+\frac{2}{k}\right) p_{k-1} \:\:\: k=1,2,3,\ldots\\
p_1&amp;= \frac{9}{256}
\end{aligned}
\]</span>
Determinar el valor esperado de esta variable aleatoria discreta.</p>
<h5 style="text-align: center;">
<a id="displayExample.2.3.1" href="javascript:toggleEX('toggleExample.2.3.1','displayExample.2.3.1');"><i><strong>Mostrar Solución de Ejemplo</strong></i></a>
</h5>
<div id="toggleExample.2.3.1" style="display: none">
<p><strong>Solución:</strong> Dado que la <em>pmf</em> satisface la relación de recurrencia <span class="math inline">\((a,b,0)\)</span> sabemos que la distribución subyacente es una entre las distribuciones binomial, Poisson y binomial negativa. Puesto que la relación de los parámetros (<em>i.e.</em> <span class="math inline">\(b/a\)</span>) es igual a <span class="math inline">\(2\)</span>, sabemos que es binomial negativa y que <span class="math inline">\(r=3\)</span>. Además, ya que para la binomial negativa <span class="math inline">\(p_1=r(1+\beta)^{-(r+1)}\beta\)</span>, tenemos</p>
<p><span class="math display">\[
\begin{aligned}
&amp;&amp;\frac{9}{256}=&amp;3\frac{\beta}{(1+\beta)^4}\\
\implies &amp;&amp;\frac{3}{(1+3)^4}=&amp;\frac{\beta}{(1+\beta)^4}\\
\implies &amp;&amp;\beta=&amp;3.
\end{aligned}
\]</span>
Por último, dado que la media de la binomial negativa es <span class="math inline">\(r\beta\)</span>, tenemos que la media de esta distribución es igual a <span class="math inline">\(9\)</span>.</p>
</div>
<hr />
</div>
<div id="S:estimating-frequency-distributions" class="section level2">
<h2><span class="header-section-number">2.4</span> Estimación de las distribuciones de frecuencias</h2>
<hr />
<p>En esta sección, se aprende a:</p>
<ul>
<li>Definir la verosimilitud para una muestra de observaciones de una distribución discreta</li>
<li>Definir el estimador de máxima verosimilitud para una muestra aleatoria de observaciones de una distribución discreta</li>
<li>Calcular el estimador de máxima verosimilitud para las distribuciones binomial, Poisson y binomial negativa</li>
</ul>
<hr />
<div id="S:parameter-estimation" class="section level3">
<h3><span class="header-section-number">2.4.1</span> Estimación de los parámetros</h3>
<p>En la sección <a href="C-Frequency-Modeling.html#S:basic-frequency-distributions">2.2</a> se introdujeron tres distribuciones muy importantes para la modelización de varios tipos de datos de recuento derivados de los seguros. Supongamos ahora que tenemos un conjunto de datos de recuento a los que queremos ajustar una distribución, y hemos determinado que una de las distribuciones <span class="math inline">\((a,b,0)\)</span> es más apropiada que las otras. Dado que cada una de ellas forma una clase de distribuciones si permitimos que su(s) parámetro(s) tome(n) cualquier valor permisible, queda la tarea de determinar el <strong>mejor</strong> valor del(los) parámetro(s) para los datos en cuestión. Éste es un problema estadístico de estimación puntual, y el paradigma de inferencia estadística de <em>máxima verosimilitud</em> suele producir estimadores eficientes en los problemas de inferencia paramétrica. En esta sección describiremos este paradigma y derivaremos los estimadores de máxima verosimilitud.</p>
<p>Supongamos que observamos las variables aleatorias independientes e idénticamente distribuidas, <em>iid</em>, <span class="math inline">\(X_1,X_2,\ldots,X_n\)</span> de una distribución con <em>pmf</em> <span class="math inline">\(p_\theta\)</span>, donde <span class="math inline">\(\theta\)</span> es un parámetro y un valor desconocido en el espacio del parámetro <span class="math inline">\(\Theta\subseteq \mathbb{R}^d\)</span>. Por ejemplo, en el caso de la distribución de Poisson</p>
<p><span class="math display">\[
p_\theta(x)=e^{-\theta}\frac{\theta^x}{x!}, \quad x=0,1,\ldots,
\]</span>
con <span class="math inline">\(\theta=(0,\infty)\)</span>. En el caso de la distribución binomial, tenemos
<span class="math display">\[
p_\theta(x)=\binom{m}{x} q^x(1-q)^{m-x}, \quad x=0,1,\ldots,m,
\]</span>
con <span class="math inline">\(\theta:=(m,q)\in \{0,1,2,\ldots\}\times[0,1]\)</span>. Supongamos que las observaciones son <span class="math inline">\(x_1,\ldots,x_n\)</span>, valores observados de la muestra aleatoria <span class="math inline">\(X_1,X_2,\ldots,X_n\)</span> presentada anteriormente. En este caso, la probabilidad de observar esta muestra a partir de <span class="math inline">\(p_\theta\)</span> es igual a</p>
<p><span class="math display">\[
\prod_{i=1}^n p_\theta(x_i).
\]</span>
Lo anterior, denotado por <span class="math inline">\(L(\theta)\)</span>, visto como una función de <span class="math inline">\(\theta\)</span> se denomina la <em>verosimilitud</em>. Nótese que eliminamos su dependencia de los datos, para enfatizar que lo estamos viendo como una función del parámetro. Por ejemplo, en el caso de la distribución de Poisson tenemos</p>
<p><span class="math display">\[
L(\lambda)=e^{-n\lambda} \lambda^{\sum_{i=1}^n x_i} \left(\prod_{i=1}^n x_i!\right)^{-1};
\]</span>
en el caso de la distribución binomial tenemos
<span class="math display">\[
L(m,q)=\left(\prod_{i=1}^n \binom{m}{x_i}\right) q^{\sum_{i=1}^n x_i} (1-q)^{nm-\sum_{i=1}^n x_i} .
\]</span>
El estimador máximo verosímil (MLE, según sus siglas en inglés) para <span class="math inline">\(\theta\)</span> es un maximizador de la verosimilitud; en cierto sentido, el <em>MLE</em> elige el conjunto de valores de los parámetros que mejor explican las observaciones observadas. Consideremos una muestra de tamaño <span class="math inline">\(3\)</span> de una distribución de Bernoulli (binomial con <span class="math inline">\(m=1\)</span>) con valores de <span class="math inline">\(0,1,0\)</span>. En este caso se comprueba fácilmente que la verosimilitud es igual a</p>
<p><span class="math display">\[
L(q)=q(1-q)^2,
\]</span>
y el gráfico de la verosimilitud se muestra en la Figura <a href="C-Frequency-Modeling.html#fig:berlik">2.1</a>. Como se muestra en el gráfico, el valor máximo de la verosimilitud es igual a <span class="math inline">\(4/27\)</span> y se alcanza en <span class="math inline">\(q=1/3\)</span>, y por lo tanto el estimador máximo verosímil para <span class="math inline">\(q\)</span> es <span class="math inline">\(1/3\)</span> para la muestra considerada. En este caso se puede recurrir al álgebra para mostrar que
<span class="math display">\[
q(1-q)^2=\left(q-\frac{1}{3}\right)^2\left(q-\frac{4}{3}\right)+\frac{4}{27},
\]</span></p>
<p>y concluir que el máximo es igual a <span class="math inline">\(4/27\)</span>, y se alcanza en <span class="math inline">\(q=1/3\)</span> (usando el hecho de que el primer término es no positivo en el intervalo <span class="math inline">\([0,1]\)</span>). Pero como es evidente, esta forma de derivar el <em>mle</em> utilizando el álgebra no es generalizada. Normalmente, se recurre al cálculo para derivar el <em>mle</em> - obsérvese que para algunas verosimilitudes uno puede tener que recurrir a otros métodos de optimización, especialmente cuando la verosimilitud tiene muchos extremos locales. Se acostumbra a maximizar de forma equivalente el logaritmo de la verosimilitud<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a> <span class="math inline">\(L(\cdot)\)</span>, denotado por <span class="math inline">\(l(\cdot)\)</span>, y mirar el conjunto de ceros de su primera derivada<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a> <span class="math inline">\(l&#39;(\cdot)\)</span>. En el caso de la verosimilitud anterior, <span class="math inline">\(l(q)=\log(q)+2\log(1-q)\)</span>, y</p>
<p><span class="math display">\[
l&#39;(q):=\frac{\rm d}{{\rm d}q}l(q)=\frac{1}{q}-\frac{2}{1-q}.
\]</span>
El único cero de <span class="math inline">\(l&#39;(\cdot)\)</span> iguala a <span class="math inline">\(1/3\)</span>, y dado que <span class="math inline">\(l&#39;&#39;(\cdot)\)</span> es negativa, tenemos que <span class="math inline">\(1/3\)</span> es el único maximizador de la verosimilitud y por tanto su estimador máximo verosímil.</p>
<div class="figure" style="text-align: center"><span id="fig:berlik"></span>
<img src="Figures/figure2.1.png" alt="Verosimilitud para $(0,1,0)$ de una muestra de $3$ de una Bernoulli" width="45%" />
<p class="caption">
Figure 2.1: Verosimilitud para <span class="math inline">\((0,1,0)\)</span> de una muestra de <span class="math inline">\(3\)</span> de una Bernoulli
</p>
</div>
</div>
<div id="S:frequency-distributions-mle" class="section level3">
<h3><span class="header-section-number">2.4.2</span> MLE de las distribuciones de frecuencias</h3>
<p>A continuación derivamos el estimador de máxima verosimilitud, <em>MLE</em>, para los tres miembros de la clase <span class="math inline">\((a,b,0)\)</span>. Empezamos resumiendo la discusión anterior. En el escenario de observar las variables aleatorias <em>iid</em>, independientes e idénticamente distribuidas, <span class="math inline">\(X_1,X_2,\ldots,X_n\)</span> de una distribución con <em>pmf</em> <span class="math inline">\(p_\theta\)</span>, donde <span class="math inline">\(\theta\)</span> toma un valor desconocido en <span class="math inline">\(\Theta\subseteq \mathbb{R}^d\)</span>, la verosimilitud <span class="math inline">\(L(\cdot)\)</span>, una función en <span class="math inline">\(\Theta\)</span> se define como</p>
<p><span class="math display">\[
L(\theta):=\prod_{i=1}^n p_\theta(x_i),
\]</span>
donde <span class="math inline">\(x_1,\ldots,x_n\)</span> son los valores observados. El <em>MLE</em> de <span class="math inline">\(\theta\)</span>, denotado como <span class="math inline">\(\hat{\theta}_{\rm MLE}\)</span>, es una función que asigna las observaciones a un elemento del conjunto de maximizadores de <span class="math inline">\(L(\cdot)\)</span>, concretamente
<span class="math display">\[
\{\theta \vert L(\theta)=\max_{\eta\in\Theta}L(\eta)\}.
\]</span>
Nótese que el conjunto anterior es una función de las observaciones, aunque esta dependencia no se muestra explícitamente.
En el caso de las tres distribuciones que estudiaremos, y de forma bastante general, el conjunto anterior es un conjunto unitario (singleton) con una probabilidad que tiende a uno (con un tamaño de muestra creciente). En otras palabras, para muchas distribuciones de uso común y cuando el tamaño de la muestra es grande, el estimador verosímil se define de forma única con una alta probabilidad.
A continuación, asumiremos que hemos observado <span class="math inline">\(n\)</span> variables aleatorias <em>iid</em> <span class="math inline">\(X_1,X_2,\ldots,X_n\)</span> de la distribución considerada, aunque el valor del parámetro es desconocido. Además, <span class="math inline">\(x_1,x_2,\ldots,x_n\)</span> denotará los valores observados. Cabe señalar en el caso de los datos de recuento, y de los datos de distribuciones discretas en general, que la verosimilitud puede representarse alternativamente como</p>
<p><span class="math display">\[
L(\theta):=\prod_{k\geq 0} \left(p_\theta(k)\right)^{m_k},
\]</span>
donde
<span class="math display">\[
m_k:= \left\vert \{i\vert x_i=k, 1\leq i \leq n\} \right\vert=\sum_{i= 1}^n I(x_i=k), \quad k\geq 0.
\]</span>
Obsérvese que esta transformación conserva todos los datos, compilándolos de manera racionalizada. Para un <span class="math inline">\(n\)</span> grande, lleva a la compresión de los datos en el sentido de <em>suficiencia</em>. A continuación, presentamos las expresiones para el <em>MLE</em> también en términos de <span class="math inline">\(\{m_k\}_{k\geq 1}\)</span>.</p>
<p><strong>MLE – Distribución de Poisson:</strong> En este caso, como se ha señalado anteriormente, la verosimilitud viene dada por
<span class="math display">\[
L(\lambda)=\left(\prod_{i=1}^n x_i!\right)^{-1}e^{-n\lambda}\lambda^{\sum_{i=1}^n x_i},
\]</span>
que implica
<span class="math display">\[
l(\lambda)= -\sum_{i=1}^n \log(x_i!) -n\lambda +\log(\lambda) \cdot \sum_{i=1}^n x_i,
\]</span>
y
<span class="math display">\[
l&#39;(\lambda)= -n +\frac{1}{\lambda}\sum_{i=1}^n x_i.
\]</span></p>
<p>En la evaluación de <span class="math inline">\(l&#39;&#39;(\lambda)\)</span>, cuando <span class="math inline">\(\sum_{i=1}^n x_i&gt;0\)</span>, <span class="math inline">\(l&#39;&#39;&lt; 0\)</span>. Por consiguiente, el máximo se alcanza en la media de la muestra, <span class="math inline">\(\overline{x}\)</span>, que se presenta a continuación. Cuando <span class="math inline">\(\sum_{i=1}^n x_i=0\)</span>, la verosimilitud es una función decreciente y, por lo tanto, el máximo se alcanza en el menor valor posible del parámetro; esto da como resultado que el estimador máximo verosímil sea cero. Por lo tanto, tenemos</p>
<p><span class="math display">\[
\overline{x} = \hat{\lambda}_{\rm MLE} = \frac{1}{n}\sum_{i=1}^n x_i. 
\]</span>
Nótese que la media muestral puede calcularse también como
<span class="math display">\[
\frac{1}{n} \sum_{k\geq 1} km_k.
\]</span></p>
<p>Cabe mencionar que en el caso de la Poisson, la distribución exacta de <span class="math inline">\(\hat{\lambda}_{\rm MLE}\)</span> está disponible en forma cerrada - es una Poisson a escala - cuando la distribución subyacente es una Poisson. Esto es así porque la suma de variables aleatorias independientes de Poisson es también una Poisson. Por supuesto, para muestras de gran tamaño se puede utilizar el Teorema Central del Límite (CLT, según sus siglas en inglés) ordinario para derivar una aproximación normal. Nótese que esta última aproximación es válida si la distribución subyacente es una distribución con un segundo momento finito.</p>
<p><strong>MLE – Distribución Binomial:</strong> A diferencia del caso de la distribución de Poisson, el espacio de parámetros en el caso de la binomial es bidimensional. Por lo tanto, el problema de optimización es un poco más difícil. Podemos comenzar observando que la verosimilitud viene dada por</p>
<p><span class="math display">\[
L(m,q)= \left(\prod_{i=1}^n \binom{m}{x_i}\right) q^{\sum_{i=1}^n x_i} (1-q)^{nm-\sum_{i=1}^n x_i}, 
\]</span>
y el logaritmo de la verosimilitud por
<span class="math display">\[
l(m,q)= \sum_{i=1}^n \log\left(\binom{m}{x_i}\right) + \left({\sum_{i=1}^n x_i}\right)\log(q)+ \left({nm-\sum_{i=1}^n x_i}\right)\log(1-q). 
\]</span>
Nótese que como <span class="math inline">\(m\)</span> sólo toma valores enteros no negativos, no podemos usar cálculo multivariante para encontrar los valores óptimos. Sin embargo, podemos usar el cálculo para una sola variable para mostrar que
<span class="math display" id="eq:binmle">\[\begin{equation}
\hat{q}_{\rm MLE}\times \hat{m}_{\rm MLE}= \frac{1}{n}\sum_{i=1}^n X_i.  
\tag{2.2}
\end{equation}\]</span>
En este sentido, observamos que para un valor fijado de <span class="math inline">\(m\)</span>,
<span class="math display">\[
\frac{\delta}{\delta q} l(m,q) = \left({\sum_{i=1}^n x_i}\right)\frac{1}{q}- \left({nm-\sum_{i=1}^n x_i}\right)\frac{1}{1-q},
\]</span>
y que
<span class="math display">\[
\frac{\delta^2}{\delta q^2} l(m,q) = -\left[\left({\sum_{i=1}^n x_i}\right)\frac{1}{q^2} + \left({nm-\sum_{i=1}^n x_i}\right)\frac{1}{(1-q)^2}\right]\leq 0.
\]</span>
Lo anterior implica que para un valor concreto de <span class="math inline">\(m\)</span>, el valor máximo de <span class="math inline">\(q\)</span> satisface
<span class="math display">\[
mq=\frac{1}{n}\sum_{i=1}^n X_i,
\]</span>
y por lo tanto establecemos la ecuación <a href="C-Frequency-Modeling.html#eq:binmle">(2.2)</a>. Lo anterior reduce la tarea a la búsqueda de <span class="math inline">\(\hat{m}_{\rm MLE}\)</span>, que es miembro del conjunto de los maximizadores de
<span class="math display" id="eq:binlikm">\[\begin{equation}
L\left(m,\frac{1}{nm}\sum_{i=1}^n x_i\right).
\tag{2.3}
\end{equation}\]</span>
Nótese que la verosimilitud sería cero para valores de <span class="math inline">\(m\)</span> menores a <span class="math inline">\(\max\limits_{1\leq i \leq n}x_i\)</span>, y por tanto
<span class="math display">\[
\hat{m}_{\rm MLE}\geq \max_{1\leq i \leq n}x_i.
\]</span>
Para especificar un algoritmo para calcular <span class="math inline">\(\hat{m}_{\rm MLE}\)</span>, primero señalamos que para algunos conjuntos de datos <span class="math inline">\(\hat{m}_{\rm MLE}\)</span> podría ser igual a <span class="math inline">\(\infty\)</span>, lo que indicaría que una distribución de Poisson se ajustaría mejor que una distribución binomial. Esto es así, ya que la distribución binomial con los parámetros <span class="math inline">\((m,\overline{x}/m)\)</span> se aproxima a la distribución de Poisson con el parámetro <span class="math inline">\(\overline{x}\)</span> con <span class="math inline">\(m\)</span> tendiendo a infinito. El hecho de que algunos conjuntos de datos <strong>prefieran</strong> una distribución de Poisson no debería ser sorprendente ya que desde este punto de vista el conjunto de la distribución de Poisson está en el límite del conjunto de las distribuciones binomiales.</p>
<p>Curiosamente, en <span class="citation">(Olkin, Petkau, and Zidek <a href="#ref-olkin1981" role="doc-biblioref">1981</a>)</span> muestran que si la media de la muestra es menor o igual a la varianza de la muestra entonces <span class="math inline">\(\hat{m}_{\rm MLE}=\infty\)</span>; de lo contrario, existe un <span class="math inline">\(m\)</span> finito que maximiza la ecuación <a href="C-Frequency-Modeling.html#eq:binlikm">(2.3)</a>. En la siguiente Figura <a href="C-Frequency-Modeling.html#fig:MLEm">2.2</a> se muestra el gráfico de <span class="math inline">\(L\left(m,\frac{1}{nm}\sum_{i=1}^n x_i\right)\)</span> para tres muestras diferentes de tamaño <span class="math inline">\(5\)</span>; sólo difieren en el valor del máximo de la muestra. La primera muestra de <span class="math inline">\((2,2,2,4,5)\)</span> tiene una relación entre la media de la muestra y la varianza de la muestra mayor a <span class="math inline">\(1\)</span> (<span class="math inline">\(1,875\)</span>), la segunda muestra de <span class="math inline">\((2,2,2,4,6)\)</span> tiene la relación igual a <span class="math inline">\(1,25\)</span> que está más cerca de <span class="math inline">\(1\)</span>, y la tercera muestra de <span class="math inline">\((2,2,2,4,7)\)</span> tiene una relación menor a <span class="math inline">\(1\)</span> (<span class="math inline">\(0,885\)</span>). Para las tres muestras, como se muestra en la Figura <a href="C-Frequency-Modeling.html#fig:MLEm">2.2</a>, <span class="math inline">\(\hat{m}_{\rm MLE}\)</span> es igual a <span class="math inline">\(7\)</span>, <span class="math inline">\(18\)</span> and <span class="math inline">\(\infty\)</span>, respectivamente. Nótese que el valor en el límite de <span class="math inline">\(L\left(m,\frac{1}{nm}\sum_{i=1}^n x_i\right)\)</span> cuando <span class="math inline">\(m\)</span> tiende a infinito es igual a</p>
<p><span class="math display" id="eq:Poilik">\[\begin{equation}
\left(\prod_{i=1}^n x_i! \right)^{-1} \exp\left\{-\sum_{i=1}^n x_i\right\} \overline{x}^{n\overline{x}}. 
\tag{2.4}
\end{equation}\]</span>
También, se debe señalar que la Figura <a href="C-Frequency-Modeling.html#fig:MLEm">2.2</a> muestra que el <em>MLE</em> de <span class="math inline">\(m\)</span> es no robusto, <em>i.e.</em>, pequeños cambios en el conjunto de datos pueden causar grandes cambios en el estimador.</p>
<p>La discusión anterior sugiere el siguiente algoritmo sencillo:</p>
<ul>
<li><p><em>Paso 1</em>. Si la media de la muestra es menor o igual a la varianza de la muestra, <span class="math inline">\(\hat{m}_{MLE}=\infty\)</span>. La distribución sugerida por <em>MLE</em> es una distribución de Poisson con <span class="math inline">\(\hat{\lambda}=\overline{x}\)</span>.</p></li>
<li><p><em>Paso 2</em>. Si la media de la muestra es mayor que la varianza de la muestra, entonces calcula <span class="math inline">\(L(m,\overline{x}/m)\)</span> para valores de <span class="math inline">\(m\)</span> mayores o iguales al máximo de la muestra hasta que <span class="math inline">\(L(m,\overline{x}/m)\)</span> se acerque al valor de la verosimilitud de Poisson dada en <a href="C-Frequency-Modeling.html#eq:Poilik">(2.4)</a>. El valor de <span class="math inline">\(m\)</span> que corresponde al valor máximo de <span class="math inline">\(L(m,\overline{x}/m)\)</span> entre los calculados es igual a <span class="math inline">\(\hat{m}_{MLE}\)</span>.</p></li>
</ul>
<p>Obsérvese que si la distribución subyacente es la distribución binomial con parámetros <span class="math inline">\((m,q)\)</span> (con <span class="math inline">\(q&gt;0\)</span>) entonces <span class="math inline">\(\hat{m}_{MLE}\)</span> será igual a <span class="math inline">\(m\)</span> para los tamaños de muestra grandes. Además, <span class="math inline">\(\hat{q}_{MLE}\)</span> tendrá una distribución asintóticamente normal y convergerá con probabilidad uno a <span class="math inline">\(q\)</span>.</p>
<!-- label{MLEm} -->
<!-- ```{r echo=FALSE, include=FALSE} -->
<!-- library(ggplot2) -->
<!-- likm<-function(m){ -->
<!--   prod((dbinom(x,m,mean(x)/m))) -->
<!-- } -->
<!-- x<-c(2,2,2,4,5); -->
<!-- n<-(5:100); -->
<!-- ll<-unlist(lapply(n,likm)); -->
<!-- n[ll==max(ll)] -->
<!-- y<-cbind(n,ll); -->
<!-- x<-c(2,2,2,4,6); -->
<!-- ll<-unlist(lapply(n,likm)); -->
<!-- n[ll==max(ll)] -->
<!-- y<-cbind(y,ll); -->
<!-- x<-c(2,2,2,4,7); -->
<!-- ll<-unlist(lapply(n,likm)); -->
<!-- n[ll==max(ll)] -->
<!-- y<-cbind(y,ll); -->
<!-- colnames(y)<-c("m","$\\tilde{x}=(2,2,2,4,5)$","$\\tilde{x}=(2,2,2,4,6)$","$\\tilde{x}=(2,2,2,4,7)$"); -->
<!-- dy<-data.frame(y); -->
<!-- ``` -->
<!-- ```{r, MLEm, echo=FALSE, fig.cap="Gráfico de $L(m,\\overline{x}/m)$ para la distribución binomial", out.width='80%', fig.align='center'} -->
<!-- ggplot(dy) +  -->
<!--   geom_point(aes(x=m, y=(X..tilde.x...2.2.2.4.5..),shape="$\\tilde{x}=(2,2,2,4,5):\\hat{m}=7$"),size=0.75) +  -->
<!--   geom_point(aes(x=m, y=(X..tilde.x...2.2.2.4.6..),shape="$\\tilde{x}=(2,2,2,4,6):\\hat{m}=18$"),size=0.75) + -->
<!--   geom_point(aes(x=m, y=(X..tilde.x...2.2.2.4.7..),shape="$\\tilde{x}=(2,2,2,4,7):\\hat{m}=\\infty$"),size=0.75) + -->
<!--   geom_point(aes(x=c(7),y=dy$X..tilde.x...2.2.2.4.5..[3],colour="$\\hat{m}$",shape="$\\tilde{x}=(2,2,2,4,5):\\hat{m}=7$"),size=0.75)+ -->
<!--   geom_point(aes(x=c(18),y=dy$X..tilde.x...2.2.2.4.6..[14],colour="$\\hat{m}$",shape="$\\tilde{x}=(2,2,2,4,6):\\hat{m}=18$"),size=0.75)+ -->
<!--   labs(x="m",y=expression(L(m,bar(x)/m)),title="MLE para m: No-Robustez de MLE ")  -->
<!-- ``` -->
<div class="figure" style="text-align: center"><span id="fig:MLEm"></span>
<img src="Figures/figure2.2.png" alt="Gráfico de $L(m,\overline{x}/m)$ de la distribución binomial" width="80%" />
<p class="caption">
Figure 2.2: Gráfico de <span class="math inline">\(L(m,\overline{x}/m)\)</span> de la distribución binomial
</p>
</div>
<p><strong>MLE – Distribución Binomial Negativa:</strong> El caso de la distribución binomial negativa es similar al de la distribución binomial en el sentido de que tenemos dos parámetros y los <em>MLE</em> no existen en una forma cerrada. Una diferencia entre ellas es que, a diferencia del parámetro de la binomial <span class="math inline">\(m\)</span> que toma valores enteros positivos, el parámetro <span class="math inline">\(r\)</span> de la binomial negativa puede tomar cualquier valor real positivo. Esto hace que el problema de optimización sea un poco más complejo. Comencemos señalando que la verosimilitud puede expresarse de la siguiente forma:</p>
<p><span class="math display">\[
L(r,\beta)=\left(\prod_{i=1}^n \binom{r+x_i-1}{x_i}\right) (1+\beta)^{-n(r+\overline{x})} \beta^{n\overline{x}}.  
\]</span>
Lo anterior implica que la log-verosimilitud viene dada por
<span class="math display">\[
l(r,\beta)=\sum_{i=1}^n \log\binom{r+x_i-1}{x_i} -n(r+\overline{x}) \log(1+\beta) +n\overline{x}\log\beta,
\]</span>
Y por tanto
<span class="math display">\[
\frac{\delta}{\delta\beta} l(r,\beta) = -\frac{n(r+\overline{x})}{1+\beta} + \frac{n\overline{x}}{\beta}.
\]</span>
Igualando la ecuación a cero, tenemos
<span class="math display">\[
\hat{r}_{MLE}\times \hat{\beta}_{MLE} = \overline{x}.
\]</span>
Lo anterior reduce el problema de optimización bidimensional a un problema unidimensional- se necesita maximizar
<span class="math display">\[
l(r,\overline{x}/r)=\sum_{i=1}^n \log\binom{r+x_i-1}{x_i} -n(r+\overline{x}) \log(1+\overline{x}/r) +n\overline{x}\log(\overline{x}/r),
\]</span>
con respecto a <span class="math inline">\(r\)</span>, siendo el maximizador de <span class="math inline">\(r\)</span> su <em>MLE</em> y <span class="math inline">\(\hat{\beta}_{MLE}=\overline{x}/\hat{r}_{MLE}\)</span>. En <span class="citation">(Levin, Reeds, and others <a href="#ref-levin1977" role="doc-biblioref">1977</a>)</span> se muestra que si la varianza muestral es mayor que la media muestral, entonces existe un único <span class="math inline">\(r&gt;0\)</span> que maximiza <span class="math inline">\(l(r,\overline{x}/r)\)</span> y por lo tanto un único <em>MLE</em> para <span class="math inline">\(r\)</span> y <span class="math inline">\(\beta\)</span>. Además, muestran que si <span class="math inline">\(\hat{\sigma}^2\leq \overline{x}\)</span>, entonces la verosimilitud de la binomial negativa estará dominada por la verosimilitud de la Poisson con <span class="math inline">\(\hat{\lambda}=\overline{x}\)</span>. En otras palabras, una distribución de Poisson ofrece un mejor ajuste a los datos. La garantía en el caso de <span class="math inline">\(\hat{\sigma}^2&gt;\hat{\mu}\)</span> nos permite usar un algoritmo para maximizar <span class="math inline">\(l(r,\overline{x}/r)\)</span>. Mediante un método alternativo de calcular la verosimilitud, observamos que
<span class="math display">\[
l(r,\overline{x}/r)=\sum_{i=1}^n \sum_{j=1}^{x_i}\log(r-1+j) - \sum_{i=1}^n\log(x_i!) - n(r+\overline{x}) \log(r+\overline{x}) + nr\log(r) + n\overline{x}\log(\overline{x}),
\]</span>
lo que genera
<span class="math display">\[
\left(\frac{1}{n}\right)\frac{\delta}{\delta r}l(r,\overline{x}/r)=\left(\frac{1}{n}\right)\sum_{i=1}^n \sum_{j=1}^{x_i}\frac{1}{r-1+j} - \log(r+\overline{x}) + \log(r).
\]</span>
Observamos que, en las expresiones anteriores para los términos que implican un doble sumatorio, el sumatorio interno es igual a cero si <span class="math inline">\(x_i=0\)</span>. El <em>estimador máximo verosímil</em> de <span class="math inline">\(r\)</span> es una raíz de la última expresión y podemos usar un algoritmo de búsqueda de raíces para calcularla. Además, tenemos</p>
<p><span class="math display">\[
\left(\frac{1}{n}\right)\frac{\delta^2}{\delta r^2}l(r,\overline{x}/r)=\frac{\overline{x}}{r(r+\overline{x})}-\left(\frac{1}{n}\right)\sum_{i=1}^n \sum_{j=1}^{x_i}\frac{1}{(r-1+j)^2}.
\]</span>
Un algoritmo iterativo de búsqueda de raíces simple y de rápida convergencia es el método de Newton, que se cree que los Babilonios ya utilizaban para calcular raíces cuadradas. Con este método, se selecciona una aproximación inicial para la raíz y se generan sucesivamente nuevas aproximaciones para la raíz hasta la convergencia. Aplicando el método de Newton a nuestro problema se obtiene el siguiente algoritmo:<br />
<!-- \begin{enumerate}[leftmargin=0.5in,label=Step \roman*] -->
<em>Paso i</em>. Elegir una solución aproximada, denominada <span class="math inline">\(r_0\)</span>. Fijar <span class="math inline">\(k\)</span> igual a <span class="math inline">\(0\)</span>.<br />
<em>Paso ii</em>. Definir <span class="math inline">\(r_{k+1}\)</span> como
<span class="math display">\[
r_{k+1}:= r_k - \frac{\left(\frac{1}{n}\right)\sum_{i=1}^n \sum_{j=1}^{x_i}\frac{1}{r_k-1+j} - \log(r_k+\overline{x}) + \log(r_k)}{\frac{\overline{x}}{r_k(r_k+\overline{x})}-\left(\frac{1}{n}\right)\sum_{i=1}^n \sum_{j=1}^{x_i}\frac{1}{(r_k-1+j)^2}}
\]</span><br />
<em>Paso iii</em>. Si <span class="math inline">\(r_{k+1}\sim r_k\)</span>, entonces establece <span class="math inline">\(r_{k+1}\)</span> como <em>estimador máximo verosímil</em>; en otro caso, incrementa <span class="math inline">\(k\)</span> por <span class="math inline">\(1\)</span> y repite <em>Paso ii</em>.</p>
<p>Por ejemplo, simulamos una muestra de <span class="math inline">\(5\)</span> observaciones de <span class="math inline">\(41, 49, 40, 27, 23\)</span> de la binomial negativa con los parámetros <span class="math inline">\(r=10\)</span> y <span class="math inline">\(\beta=5\)</span>. Escogiendo el valor inicial de <span class="math inline">\(r\)</span> de tal manera que
<span class="math display">\[
r\beta=\hat{\mu} \quad \hbox{and} \quad r\beta(1+\beta)=\hat{\sigma}^2
\]</span>
donde <span class="math inline">\(\hat{\mu}\)</span> representa la media estimada y <span class="math inline">\(\hat{\sigma}^2\)</span> es la varianza estimada. Esto nos conduce a un valor inicial de <span class="math inline">\(r\)</span> de <span class="math inline">\(23,14286\)</span>. Las iteraciones de <span class="math inline">\(r\)</span> del método de Newton son
<span class="math display">\[
21,39627, 21,60287, 21,60647, 21,60647;
\]</span>
la rápida convergencia anterior es frecuente con el método de Newton. Por lo tanto, en este ejemplo, <span class="math inline">\(\hat{r}_{MLE}\sim21,60647\)</span> y <span class="math inline">\(\hat{\beta}_{MLE}=8,3308\)</span>.</p>
<p><em>Implementación R del Método de Newton - MLE binomial negativa para <span class="math inline">\(r\)</span></em></p>
<h5 style="text-align: center;">
<a id="displayCode.Freq.1" href="javascript:togglecode('toggleCode.Freq.1','displayCode.Freq.1');"><i><strong>Mostrar Código R</strong></i></a>
</h5>
<div id="toggleCode.Freq.1" style="display: none">
<pre><code>Newton&lt;-function(x,abserr){
mu&lt;-mean(x);
sigma2&lt;-mean(x^2)-mu^2;
r&lt;-mu^2/(sigma2-mu);
b&lt;-TRUE;
iter&lt;-0;
while (b) {
tr&lt;-r;
m1&lt;-mean(c(x[x==0],sapply(x[x&gt;0],function(z){sum(1/(tr:(tr-1+z)))})));
m2&lt;-mean(c(x[x==0],sapply(x[x&gt;0],function(z){sum(1/(tr:(tr-1+z))^2)})));
r&lt;-tr-(m1-log(1+mu/tr))/(mu/(tr*(tr+mu))-m2);
b&lt;-!(abs(tr-r)&lt;abserr);
iter&lt;-iter+1;
}
c(r,iter)
}</code></pre>
</div>
<hr />
<p>Para concluir nuestra discusión sobre <em>MLE</em> para la clase de distribuciones <span class="math inline">\((a,b,0)\)</span>, en la Figura <a href="C-Frequency-Modeling.html#fig:MLEab0">2.3</a> siguiente representamos gráficamente el valor máximo de la verosimilitud de Poisson, <span class="math inline">\(L(m,\overline{x}/m)\)</span> para la binomial, y <span class="math inline">\(L(r,\overline{x}/r)\)</span> para la binomial negativa, para las tres muestras de tamaño <span class="math inline">\(5\)</span> dadas en <a href="#tab:2.1">Tabla 2.1</a>. Los datos se construyeron de tal forma que cubrieran las tres ordenaciones entre la media y varianza muestrales.
Como se muestra en la Figura <a href="C-Frequency-Modeling.html#fig:MLEab0">2.3</a>, y demostrado por la teoría, si <span class="math inline">\(\hat{\mu}&lt;\hat{\sigma}^2\)</span>
entonces la binomial negativa dará un mayor valor del máximo de verosimilitud; si <span class="math inline">\(\hat{\mu}=\hat{\sigma}^2\)</span> la Poisson dará el mayor valor de la verosimilitud; y finalmente en el caso que <span class="math inline">\(\hat{\mu}&gt;\hat{\sigma}^2\)</span> la binomial dará un mejor ajuste que las otras. Así que, antes de ajustar los datos con una distribución de frecuencias <span class="math inline">\((a,b,0)\)</span>, es mejor empezar por examinar el orden entre <span class="math inline">\(\hat{\mu}\)</span> y <span class="math inline">\(\hat{\sigma}^2\)</span>. Cabe volver a enfatizar que la Poisson está en el <strong>límite</strong> de las distribuciones binomial negativa y binomial. Por lo tanto en el caso que <span class="math inline">\(\hat{\mu}\geq\hat{\sigma}^2\)</span> (<span class="math inline">\(\hat{\mu}\leq\hat{\sigma}^2\)</span>, resp.) la Poisson dará un mejor ajuste que la binomial negativa (binomial, resp.), que también se indicará con <span class="math inline">\(\hat{r}=\infty\)</span> (<span class="math inline">\(\hat{m}=\infty\)</span>, resp.).</p>
<p><a id=tab:2.1></a></p>
<p><span class="math display">\[\begin{matrix}
\begin{array}{c|c|c}
\hline
\text{Datos} &amp; \text{Media }(\hat{\mu}) &amp; \text{Varianza }(\hat{\sigma}^2) \\
\hline
(2,3,6,8,9) &amp; 5,60 &amp; 7,44 \\ 
(2,5,6,8,9) &amp; 6 &amp; 6\\
(4,7,8,10,11) &amp; 8 &amp; 6\\\hline
\end{array}
\end{matrix}\]</span></p>
<p><a href="#tab:2.1">Tabla 2.1</a> : Tres Muestras de Tamaño <span class="math inline">\(5\)</span></p>
<div class="figure" style="text-align: center"><span id="fig:MLEab0"></span>
<img src="Figures/figure2.3.png" alt="Gráfico de las Verosimilitudes Parcialmente Maximizadas $(a,b,0)$" width="80%" />
<p class="caption">
Figure 2.3: Gráfico de las Verosimilitudes Parcialmente Maximizadas <span class="math inline">\((a,b,0)\)</span>
</p>
</div>
</div>
</div>
<div id="S:other-frequency-distributions" class="section level2">
<h2><span class="header-section-number">2.5</span> Otras Distribuciones de Frecuencias</h2>
<hr />
<p>En esta sección, se aprende a:</p>
<ul>
<li>Definir la clase de distribuciones de frecuencia (a,b,1) y discutir la importancia de la relación recursiva que sustenta esta clase de distribuciones</li>
<li>Interpretar las versiones truncadas y modificadas en cero de las distribuciones binomiales, Poisson y binomial negativa</li>
<li>Calcular las probabilidades usando la relación recursiva</li>
</ul>
<hr />
<p>En las secciones anteriores hemos examinado tres distribuciones con soporte definido en el conjunto de números enteros no negativos, que se adaptan bien a muchas aplicaciones de seguros. Además, al permitir frecuentemente que los parámetros sean una función de variables explicativas conocidas (por el asegurador) como la edad, el sexo, la ubicación geográfica (territorio), etc., estas distribuciones nos permiten explicar las probabilidades de siniestro en términos de estas variables. El ámbito de la estadística que analiza estos modelos se conoce como análisis de regresión - es un tópico importante de interés actuarial que no se tratará en este libro; ver <span class="citation">(Edward W Frees <a href="#ref-freesregression" role="doc-biblioref">2009</a>)</span>.</p>
<p>Es evidente que existen infinitas otras distribuciones de recuento, y lo que es más importante, las distribuciones anteriores por sí mismas no satisfacen todas las necesidades prácticas. En concreto, una característica de algunos datos en seguros es que la proporción de ceros puede ser muy diferente en la relación a la proporción de otros valores para que puedan ser explicados por las distribuciones anteriores. A continuación, se modifican las distribuciones previas para permitir una probabilidad arbitraria para el recuento de ceros, independientemente de la asignación relativa de probabilidades para los otros valores. Otra característica de un conjunto de datos que está compuesto por subconjuntos homogéneos es que, aunque las distribuciones anteriores pueden proporcionar buenos ajustes a cada subconjunto, pueden no hacerlo para la totalidad del conjunto de datos. Posteriormente, se amplían de forma natural las distribuciones <span class="math inline">\((a,b,0)\)</span> para poder cubrir, en particular, estos conjuntos de datos.</p>
<div id="S:zero-truncation-or-modification" class="section level3">
<h3><span class="header-section-number">2.5.1</span> Modificación o Truncamiento en Cero</h3>
<p>Supongamos que analizamos pólizas del seguro de automóvil que aparecen en una base de datos de siniestros de automóvil ocurridos en un determinado período. Si se estudia el número de siniestros que estas pólizas han tenido durante este período, entonces obviamente la distribución tiene que asignar una probabilidad de cero a la variable de recuento que asume el valor cero. En otras palabras, al restringir la atención a los datos de recuento en las pólizas de la base de datos de siniestros, en cierto modo hemos truncado en cero los datos de recuento de todas las pólizas. En productos de seguros a particulares (como en el caso de los automóviles), los asegurados pueden no querer informar del primer siniestro por temor a que aumente el precio del seguro en el futuro - este comportamiento inflará la proporción de recuentos de cero. Ejemplos como estos últimos modifican la proporción de ceros. Es interesante mencionar que las modificaciones naturales de las tres distribuciones anteriores son capaces de proporcionar buenos ajustes a los conjuntos de datos cero modificados/truncados que se generan en el seguro.</p>
<p>Como se presenta a continuación, se modifica la probabilidad que se le asigna al valor cero en la clase <span class="math inline">\((a,b,0)\)</span> manteniendo las probabilidades relativas asignadas a los valores no nulos - modificación en cero. Nótese que como la clase de distribuciones <span class="math inline">\((a,b,0)\)</span> satisface la recurrencia <a href="C-Frequency-Modeling.html#eq:ab0">(2.1)</a>, el mantenimiento de las probabilidades relativas de los valores no nulos implica que la recurrencia <a href="C-Frequency-Modeling.html#eq:ab0">(2.1)</a> se satisface para <span class="math inline">\(k\geq 2\)</span>. Esto nos lleva a la definición de la siguiente clase de distribuciones.</p>
<p><strong>Definición</strong>. Una distribución de recuento es un miembro de la clase <span class="math inline">\((a, b, 1)\)</span> si para las constantes <span class="math inline">\(a\)</span> y <span class="math inline">\(b\)</span> las probabilidades <span class="math inline">\(p_k\)</span> satisfacen
<span class="math display" id="eq:ab1">\[\begin{equation}
\frac{p_k}{p_{k-1}}=a+\frac{b}{k},\quad k\geq 2.
\tag{2.5}
\end{equation}\]</span></p>
<p>Nótese que como la recursión empieza en <span class="math inline">\(p_1\)</span>, y no en <span class="math inline">\(p_0\)</span>, nos referimos a esta superclase de distribuciones <span class="math inline">\((a,b,0)\)</span> como (a,b,1). Para entender esta clase, recordemos que cada par de valores válidos para <span class="math inline">\(a\)</span> y <span class="math inline">\(b\)</span> de la clase <span class="math inline">\((a,b,0)\)</span> corresponde a un único vector de probabilidades <span class="math inline">\(\{p_k\}_{k\geq 0}\)</span>. Si ahora observamos el vector de probabilidades <span class="math inline">\(\{\tilde{p}_k\}_{k\geq 0}\)</span> dado por</p>
<p><span class="math display">\[
\tilde{p}_k= \frac{1-\tilde{p}_0}{1-p_0}\cdot p_k, \quad k\geq 1,
\]</span></p>
<p>donde <span class="math inline">\(\tilde{p}_0\in[0,1)\)</span> se elige arbitrariamente, entonces como las probabilidades relativas de valores positivos de acuerdo con <span class="math inline">\(\{p_k\}_{k\geq 0}\)</span> y <span class="math inline">\(\{\tilde{p}_k\}_{k\geq 0}\)</span> son las mismas, tenemos que <span class="math inline">\(\{\tilde{p}_k\}_{k\geq 0}\)</span> satisface la recurrencia <a href="C-Frequency-Modeling.html#eq:ab1">(2.5)</a>. Esto, en particular, muestra que la clase de distribuciones <span class="math inline">\((a,b,1)\)</span> es estrictamente más amplia que la <span class="math inline">\((a,b,0)\)</span>.</p>
<p>Previamente, hemos establecido un par de valores para <span class="math inline">\(a\)</span> y <span class="math inline">\(b\)</span> que llevaron a una distribución válida de <span class="math inline">\((a,b,0)\)</span>, y luego miramos las distribuciones <span class="math inline">\((a,b,1)\)</span> que correspondían a esta distribución <span class="math inline">\((a,b,0)\)</span>. Ahora argumentaremos que la clase <span class="math inline">\((a,b,1)\)</span> admite un conjunto mayor de distribuciones permitidas para <span class="math inline">\(a\)</span> y <span class="math inline">\(b\)</span> que la clase <span class="math inline">\((a,b,0)\)</span>. Recordemos de la Sección <a href="C-Frequency-Modeling.html#S:the-a-b-0-class">2.3</a> que en el caso de <span class="math inline">\(a&lt;0\)</span> no se utiliza el hecho de que la recurrencia <a href="C-Frequency-Modeling.html#eq:ab0">(2.1)</a> empieza en <span class="math inline">\(k=1\)</span>, y por lo tanto el conjunto de pares <span class="math inline">\((a,b)\)</span> con <span class="math inline">\(a&lt;0\)</span> que son admisibles para la clase <span class="math inline">\((a,b,0)\)</span> es idéntico al que es admisible para la clase <span class="math inline">\((a,b,1)\)</span>. La misma conclusión se puede extraer fácilmente para los pares con <span class="math inline">\(a=0\)</span>. En el caso que <span class="math inline">\(a&gt;0\)</span>, en lugar de la restricción <span class="math inline">\(a+b&gt;0\)</span> para la clase <span class="math inline">\((a,b,0)\)</span>, tenemos ahora la restricción más débil de <span class="math inline">\(a+b/2&gt;0\)</span> para la clase <span class="math inline">\((a,b,1)\)</span>. Con la parametrización <span class="math inline">\(b=(r-1)a\)</span> utilizada en la Sección <a href="C-Frequency-Modeling.html#S:the-a-b-0-class">2.3</a>, en lugar de <span class="math inline">\(r&gt;0\)</span> tenemos ahora la restricción más débil de <span class="math inline">\(r&gt;-1\)</span>. En particular, vemos que mientras que al modificar el cero en una distribución <span class="math inline">\((a,b,0)\)</span> conduce a una distribución de la clase <span class="math inline">\((a,b,1)\)</span>, está conclusión no se cumple en la dirección contraria.</p>
<!-- %\textcolor{blue}{Añadir un ejemplo con a>0 y b=-3/2*a?}    -->
<p>La modificación en cero de una distribución de recuento <span class="math inline">\(F\)</span> tal que asigna una probabilidad cero al valor cero se llama un truncamiento en cero de <span class="math inline">\(F\)</span>. De este modo, la versión truncada en cero de las probabilidades <span class="math inline">\(\{p_k\}_{k\geq 0}\)</span> viene dada por</p>
<p><span class="math display">\[
\tilde{p}_k=\begin{cases}
0, &amp; k=0;\\
\frac{p_k}{1-p_0}, &amp; k\geq 1.
\end{cases}
\]</span></p>
<p>En concreto, tenemos que una modificación en cero de una distribución de recuento <span class="math inline">\(\{p_k^T\}_{k\geq 0}\)</span>, denotada por <span class="math inline">\(\{p^M_k\}_{k\geq 0}\)</span>, puede escribirse como una combinación convexa de la distribución degenerada en <span class="math inline">\(0\)</span> y el truncamiento en cero de <span class="math inline">\(\{p_k\}_{k\geq 0}\)</span>, denotado por <span class="math inline">\(\{p^T_k\}_{k\geq 0}\)</span>. De este modo tenemos</p>
<p><span class="math display">\[
p^M_k= p^M_0 \cdot \delta_{0}(k) + (1-p^M_0) \cdot p^T_k, \quad k\geq 0.  
\]</span></p>
<p><strong>Ejemplo 2.5.1. Poisson Cero Modificada/Truncada</strong>.
Considerar una distribución de Poisson con parámetro <span class="math inline">\(\lambda=2\)</span>. Calcular <span class="math inline">\(p_k, k=0,1,2,3\)</span>, para la usual (sin modificar), truncada y una versión modificada con <span class="math inline">\((p_0^M=0,6)\)</span>.</p>
<h5 style="text-align: center;">
<a id="displayExample.2.5.1" href="javascript:toggleEX('toggleExample.2.5.1','displayExample.2.5.1');"><i><strong>Mostrar Solución de Ejemplo</strong></i></a>
</h5>
<div id="toggleExample.2.5.1" style="display: none">
<p><strong>Solución.</strong> Para la distribución de Poisson como miembro de la clase ((<span class="math inline">\(a,b\)</span>,0), tenemos <span class="math inline">\(a=0\)</span> y <span class="math inline">\(b=\lambda=2\)</span>. Por lo tanto, podemos usar para cada tipo la recursión <span class="math inline">\(p_k = \lambda p_{k-1}/k= 2 p_{k-1}/k\)</span>, después de determinar las probabilidades iniciales. El cálculo de probabilidades para <span class="math inline">\(k\leq 3\)</span> se muestra en <a href="#tab:2.2">Tabla 2.2</a>.</p>
<p><a id=tab:2.2></a></p>
<p><span class="math display">\[\begin{matrix}
\begin{array}{c|c|c|c}
\hline
k &amp; p_k &amp; p_k^T &amp; p_k^M\\\hline
0 &amp; p_0=e^{-\lambda}=0,135335 &amp; 0 &amp; 0,6\\\hline
1 &amp; p_1=p_0(0+\frac{\lambda}{1})=0,27067 &amp;
\frac{p_1}{1-p_0}=0,313035 &amp;
\frac{1-p_0^M}{1-p_0}~p_1=0,125214\\\hline
2 &amp; p_2=p_1\left( \frac{\lambda}{2}\right)=0,27067 &amp;
p_2^T=p_1^T\left(\frac{\lambda}{2}\right)=0,313035 &amp;
p_2^M=p_1^M\left(\frac{\lambda}{2}\right)=0,125214\\\hline
3 &amp; p_3=p_2\left(\frac{\lambda}{3}\right)=0,180447 &amp;
p_3^T=p_2^T\left(\frac{\lambda}{3}\right)=0,208690 &amp;
p_3^M=p_2^M\left(\frac{\lambda}{3}\right)=0,083476\\\hline
\end{array}
\end{matrix}\]</span></p>
<p><a href="#tab:2.2">Tabla 2.2</a> : Cálculo de probabilidades para <span class="math inline">\(k\leq 3\)</span></p>
</div>
<hr />
</div>
</div>
<div id="S:mixture-distributions" class="section level2">
<h2><span class="header-section-number">2.6</span> Distribuciones Mixtas</h2>
<hr />
<p>En esta sección, se aprende a:</p>
<ul>
<li>Definir una distribución mixta cuando el componente de mixtura se basa en un número finito de subgrupos</li>
<li>Calcular las probabilidades de la distribución mixta a partir de las proporciones de la mixtura y el conocimiento de la distribución de cada subgrupo</li>
<li>Definir una distribución mixta cuando el componente de mixtura es continuo</li>
</ul>
<hr />
<p>En muchas aplicaciones la población subyacente consiste en subgrupos definidos de forma natural con cierta homogeneidad dentro de cada subgrupo. En estos casos es conveniente modelizar los subgrupos individuales y, de manera fundamentada, modelizar el conjunto de la población. Como veremos más adelante, más allá del atractivo del enfoque, también se amplía el abanico de aplicaciones que pueden cubrirse mediante las distribuciones paramétricas estándar.</p>
<p>Supongamos que <span class="math inline">\(k\)</span> denota el número de subgrupos definidos en una población, y <span class="math inline">\(F_i\)</span> denota la distribución de una observación extraída del subgrupo <span class="math inline">\(i\)</span>-ésimo. Si dejamos que <span class="math inline">\(\alpha_i\)</span> denote la proporción de la población en el subgrupo <span class="math inline">\(i\)</span>-ésimo, con <span class="math inline">\(\sum_{i=1}^k \alpha_i=1\)</span>, entonces la distribución de una observación elegida al azar de la población, denotada por <span class="math inline">\(F\)</span>, viene dada por</p>
<p><span class="math display" id="eq:mixdefn">\[\begin{equation}
F(x)=\sum_{i=1}^k \alpha_i \cdot F_i(x).
\tag{2.6}
\end{equation}\]</span></p>
<p>La expresión anterior puede considerarse una aplicación directa de la Ley de Probabilidad Total. Como ejemplo, consideremos una población de conductores dividida en dos subgrupos, los que tienen como máximo <span class="math inline">\(5\)</span> años de experiencia de conducción y los que tienen más de <span class="math inline">\(5\)</span> años de experiencia. Supongamos que <span class="math inline">\(\alpha\)</span> denota la proporción de conductores con menos de <span class="math inline">\(5\)</span> años de experiencia, y <span class="math inline">\(F_{\leq 5}\)</span> y <span class="math inline">\(F_{&gt; 5}\)</span> denotan la distribución del número de siniestros en un año para un conductor de cada grupo, respectivamente. Entonces la distribución del número de siniestros de un conductor seleccionado al azar viene dada por</p>
<p><span class="math display">\[
\alpha\cdot F_{\leq 5}(x) + (1-\alpha)F_{&gt; 5}(x).
\]</span></p>
<p>Una definición alternativa de una distribución mixta es la siguiente. Supongamos que <span class="math inline">\(N_i\)</span> es una variable aleatoria con distribución <span class="math inline">\(F_i\)</span>, <span class="math inline">\(i=1,\ldots, k\)</span>. Sea <span class="math inline">\(I\)</span> una variable aleatoria que toma valores <span class="math inline">\(1,2,\ldots,k\)</span> con probabilidades <span class="math inline">\(\alpha_1,\ldots,\alpha_k\)</span>, respectivamente. Entonces, la variable aleatoria <span class="math inline">\(N_I\)</span> tiene una distribución dada por la ecuación <a href="C-Frequency-Modeling.html#eq:mixdefn">(2.6)</a><a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a>.</p>
<p>En <a href="C-Frequency-Modeling.html#eq:mixdefn">(2.6)</a> vemos que la función de distribución es una combinación convexa de las funciones de distribución que la componen. Este resultado se extiende fácilmente a la función de densidad, la función de supervivencia, los momentos ordinarios y el valor esperado, ya que todos ellos son aplicaciones lineales de la función de distribución. Observamos que esto no es cierto en el caso de los momentos centrales como la varianza, y de las medidas condicionales como la función de tasa de riesgo (hazard rate). En el caso de la varianza se ve fácilmente como</p>
<p><span class="math display" id="eq:LawTotalVariation">\[\begin{equation}
\mathrm{Var}{[N_I]}=\mathrm{E}[{\mathrm{Var}[{N_I\vert I}]]} + \mathrm{Var}[{\mathrm{E}[{N_I|I}}]]=\sum_{i=1}^k \alpha_i \mathrm{Var}[{N_i}] + \mathrm{Var}[{\mathrm{E}[{N_I|I}}]] .
\tag{2.7}
\end{equation}\]</span></p>
<p>El Apéndice <a href="C-AppB.html#C:AppB">16</a> proporciona información adicional sobre esta importante expresión.</p>
<!-- \phantom{Ejercicio o ejemplo para densidad/supervivencia esperanza de tasa de riesgo (hazard rate) etc..} -->
<p><strong>Ejemplo 2.6.1. Pregunta de Examen Actuarial</strong>.
En una determinada ciudad el número de resfriados comunes que un individuo tendrá en un año sigue una distribución de Poisson que depende de la edad del individuo y su condición de fumador. La distribución de la población y el número medio de resfriados son los siguientes:</p>
<p><a id=tab:2.3></a></p>
<p><span class="math display">\[\begin{matrix}
\begin{array}{l|c|c}
\hline
 &amp; \text{Proporción de población} &amp;
\text{Número medio de resfriados}\\\hline
\text{Niños} &amp; 0,3 &amp; 3\\
\text{Adultos No-fumadores} &amp; 0,6 &amp; 1\\
\text{Adultos Fumadores} &amp; 0,1 &amp; 4\\\hline
\end{array}
\end{matrix}\]</span></p>
<p><a href="#tab:2.3">Tabla 2.3</a> : La distribución de la población y el número medio de resfriados</p>
<!-- \def\labelenumi{\arabic{enumi}.} -->
<ol style="list-style-type: decimal">
<li>Calcular la probabilidad de que una persona seleccionada al azar tenga 3 resfriados comunes en un año.</li>
<li>Calcular la probabilidad condicionada de que una persona con exactamente 3 resfriados comunes en un año sea un fumador adulto.</li>
</ol>
<h5 style="text-align: center;">
<a id="displayExample.2.6.1" href="javascript:toggleEX('toggleExample.2.6.1','displayExample.2.6.1');"><i><strong>Mostrar Solución de Ejemplo</strong></i></a>
</h5>
<div id="toggleExample.2.6.1" style="display: none">
<p><strong>Solución.</strong></p>
<p><!-- \def\labelenumi{\arabic{enumi}.} --></p>
<ol style="list-style-type: decimal">
<li><p>Utilizando la Ley de Probabilidad Total, podemos escribir la probabilidad requerida como <span class="math inline">\(\Pr(N_I=3)\)</span>, con <span class="math inline">\(I\)</span> que denota el grupo del individuo seleccionado al azar con <span class="math inline">\(1,2\)</span> y <span class="math inline">\(3\)</span> que significan los grupos <em>Niños</em>, <em>Adulto No Fumador</em>, y <em>Adulto Fumador</em>, respectivamente. Ahora, por el condicionamiento, tenemos
<span class="math display">\[
\Pr(N_I=3)=0,3\cdot\Pr(N_1=3)+0,6\cdot\Pr(N_2=3)+0,1\cdot\Pr(N_3=3),
\]</span>
con <span class="math inline">\(N_1,N_2\)</span> y <span class="math inline">\(N_3\)</span> que siguen una distribución de Poisson de media <span class="math inline">\(3,1\)</span>, y <span class="math inline">\(4\)</span>, respectivamente. Utilizando lo anterior, obtenemos <span class="math inline">\(\Pr(N_I=3)\sim0,1235\)</span></p></li>
<li><p>La probabilidad condicionada del evento A dado el evento B, <span class="math inline">\(\Pr(A\vert B) = \frac{\Pr(A,B)}{\Pr(B)}\)</span>. La probabilidad condicionada requerida en este problema puede escribirse como <span class="math inline">\(\Pr(I=3\vert N_I=3)\)</span>, que es igual a</p></li>
</ol>
<p><span class="math display">\[
\Pr(I=3\vert N_I=3)=\frac{\Pr(I=3,N_3=3)}{\Pr(N_I=3)}\sim\frac{0,1 \times 0,1954}{0,1235}\sim 0,1581.
\]</span></p>
</div>
<hr />
<p>En el ejemplo previo, el número de subgrupos <span class="math inline">\(k\)</span> era igual a tres. En general, <span class="math inline">\(k\)</span> puede ser cualquier número natural, pero cuando <span class="math inline">\(k\)</span> es grande es parsimonioso desde el punto de vista de la modelización tomar el siguiente enfoque de <em>infinitamente muchos subgrupos</em>. Para justificar este enfoque, supongamos que el subgrupo <span class="math inline">\(i\)</span>-ésimo sea tal que su distribución componente <span class="math inline">\(F_i\)</span> viene dada por <span class="math inline">\(G_{\tilde{\theta_i}}\)</span>, donde <span class="math inline">\(G_\cdot\)</span> es una familia paramétrica de distribuciones con espacio paramétrico <span class="math inline">\(\Theta\subseteq \mathbb{R}^d\)</span>. Con este supuesto, la función de distribución <span class="math inline">\(F\)</span> de una observación extraída aleatoriamente de la población viene dada por</p>
<p><span class="math display">\[
F(x)=\sum_{i=1}^k \alpha_i G_{\tilde{\theta_i}}(x),\quad \forall x\in\mathbb{R}.
\]</span>
que puede escribirse alternativamente como
<span class="math display">\[
F(x)=\mathrm{E}[{G_{\tilde{\vartheta}}(x)}],\quad \forall x\in\mathbb{R},
\]</span>
donde <span class="math inline">\(\tilde{\vartheta}\)</span> toma valores <span class="math inline">\(\tilde{\theta_i}\)</span> con probabilidad <span class="math inline">\(\alpha_i\)</span>, para <span class="math inline">\(i=1,\ldots,k\)</span>. Esto muestra que cuando <span class="math inline">\(k\)</span> es grande, se puede modelizar lo anterior tratando <span class="math inline">\(\tilde{\vartheta}\)</span> como una variable aleatoria continua.</p>
<p>Para ilustrar este enfoque, supongamos que tenemos una población de conductores con la distribución de siniestros de un conductor individual que se distribuye como una Poisson. Cada persona tiene su propio (personal) número esperado de siniestros <span class="math inline">\(\lambda\)</span> - valores más pequeños para los buenos conductores, y valores más grandes para el resto. Hay una distribución de <span class="math inline">\(\lambda\)</span> en la población; una opción común y conveniente para la modelización de esta distribución es una distribución gamma con parámetros <span class="math inline">\((\alpha, \theta)\)</span>. Con estas características resulta que la distribución resultante de <span class="math inline">\(N\)</span>, los siniestros de un conductor elegido aleatoriamente, es una binomial negativa con parámetros <span class="math inline">\((r=\alpha,\beta=\theta)\)</span>. Esto puede mostrarse de muchas maneras, pero una forma sencilla es la siguiente:</p>
<p><span class="math display">\[\begin{align*}
\Pr(N=k)&amp;= \int_0^\infty \frac{e^{-\lambda}\lambda^k}{k!} \frac{\lambda^{\alpha-1}e^{-\lambda/\theta}}{\Gamma{(\alpha)}\theta^{\alpha}} {\rm d}\lambda = 
\frac{1}{k!\Gamma(\alpha)\theta^\alpha}\int_0^\infty \lambda^{\alpha+k-1}e^{-\lambda(1+1/\theta)}{\rm d}\lambda=\frac{\Gamma{(\alpha+k)}}{k!\Gamma(\alpha)\theta^\alpha(1+1/\theta)^{\alpha+k}} \\
&amp;=\binom{\alpha+k-1}{k}\left(\frac{1}{1+\theta}\right)^\alpha\left(\frac{\theta}{1+\theta}\right)^k, \quad k=0,1,\ldots
\end{align*}\]</span>
Obsérvese que la derivación previa utiliza implícitamente lo siguiente:
<span class="math display">\[
f_{N\vert\Lambda=\lambda}(N=k)=\frac{e^{-\lambda}\lambda^k}{k!}, \quad k\geq 0; \quad \hbox{y} \quad f_{\Lambda}(\lambda)= \frac{\lambda^{\alpha-1}e^{-\lambda/\theta}}{\Gamma{(\alpha)}\theta^{\alpha}}, \quad \lambda&gt;0.
\]</span></p>
<p>Cabe mencionar que al considerar las mixturas de una clase paramétrica de distribuciones se incrementa la riqueza de la clase. Esta expansión de las distribuciones da como resultado que la clase de mixtura pueda adaptarse bien a más aplicaciones que la clase paramétrica inicial. La modelización de mixturas es una técnica de modelización muy importante en las aplicaciones de seguros, y en los capítulos posteriores se tratarán más aspectos de esta técnica de modelización.</p>
<p><strong>Ejemplo 2.6.2.</strong>
Supongamos que <span class="math inline">\(N|\Lambda \sim\)</span> Poisson<span class="math inline">\((\Lambda)\)</span> y que <span class="math inline">\(\Lambda \sim\)</span> gamma con media de 1 y varianza de 2. Determinar la probabilidad que <span class="math inline">\(N=1\)</span>.</p>
<h5 style="text-align: center;">
<a id="displayExample.2.6.1" href="javascript:toggleEX('toggleExample.2.6.1','displayExample.2.6.1');"><i><strong>Mostrar Solución de Ejemplo</strong></i></a>
</h5>
<div id="toggleExample.2.6.1" style="display: none">
<p><strong>Solución.</strong> Para una distribución gamma con parámetros <span class="math inline">\((\alpha, \theta)\)</span>, tenemos que la media es <span class="math inline">\(\alpha \theta\)</span> y la varianza es <span class="math inline">\(\alpha \theta^2\)</span>. En base a estas expresiones tenemos que
<span class="math display">\[
\begin{aligned}
\alpha &amp;= \frac{1}{2} \text{   y   } \theta =2.
\end{aligned}
\]</span>
Ahora, se puede utilizar directamente el resultado anterior para concluir que <span class="math inline">\(N\)</span> se distribuye como una binomial negativa con <span class="math inline">\(r = \alpha = \frac{1}{2}\)</span> y <span class="math inline">\(\beta= \theta =2\)</span>. Por lo tanto,
<span class="math display">\[
\begin{aligned}
\Pr(N=1)  &amp;= \binom{1+r-1}{1}(\frac{1}{(1+\beta)^r})\left(\frac{\beta}{1+\beta}\right)^1 \\
&amp;=                 \binom{1+\frac{1}{2}-1}{1}{\frac{1}{(1+2)^{1/2}}}\left(\frac{2}{1+2}\right)^1\\
&amp;=  \frac{1}{3^{3/2}} = 0,19245 .
\end{aligned}
\]</span></p>
</div>
<hr />
</div>
<div id="S:goodness-of-fit" class="section level2">
<h2><span class="header-section-number">2.7</span> Bondad del Ajuste</h2>
<hr />
<p>En esta sección, se aprende a:</p>
<ul>
<li>Calcular un estadístico de bondad del ajuste para comparar una distribución discreta hipotética con una muestra de observaciones discretas</li>
<li>Comparar el estadístico con una distribución de referencia para evaluar la adecuación del ajuste</li>
</ul>
<hr />
<p>Previamente se han analizado tres distribuciones de frecuencias elementales, junto con sus extensiones mediante la modificación/truncamiento en cero y mostrando las mixturas de estas distribuciones. Ahora bien, estas clases siguen siendo paramétricas y, por tanto, por su propia naturaleza, un pequeño subconjunto de la clase de todas las distribuciones de frecuencia posibles (<em>i.e.</em> el conjunto de distribuciones para números enteros no negativos.) Por lo tanto, aunque hemos mostrado métodos para estimar los parámetros desconocidos, la distribución <em>ajustada</em> no será una buena representación de la distribución subyacente si ésta está <strong>lejos</strong> de la clase de distribución utilizada en la modelización. De hecho, se puede demostrar que el <em>estimador máximo verosímil</em> convergerá a un valor de tal forma que la distribución correspondiente será una <em>proyección</em> Kullback-Leibler de la distribución subyacente en la clase de distribuciones utilizada para la modelización. A continuación presentamos un método de contraste - el estadístico chi-cuadrado de Pearson - para comprobar la bondad del ajuste de la distribución ajustada. Para más detalles sobre el contraste chi-cuadrado de Pearson, a un nivel introductorio de estadística matemática, remitimos al lector a la Sección 9.1 de <span class="citation">(Hogg, Tanis, and Zimmerman <a href="#ref-zimmerman2015" role="doc-biblioref">2015</a>)</span>.</p>
<p>En <span class="math inline">\(1993\)</span>, una cartera de <span class="math inline">\(n=7.483\)</span> pólizas de seguro de automóvil de una importante compañía de seguros de Singapur tenía la distribución de accidentes de automóvil por asegurado como se indica en <a href="#tab:2.4">Tabla 2.4</a>.</p>
<p><a id=tab:2.4></a></p>
<p><span class="math display">\[\begin{matrix}
\begin{array}{c|c|c|c|c|c|c}
\hline
\text{Número }(k) &amp; 0 &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; \text{Total}\\
\hline
\text{No. de Pólizas con }k\text{ accidentes }(m_k) &amp; 6.996 &amp; 455 &amp; 28 &amp; 4 &amp; 0 &amp; 7483\\
\hline
\end{array}
\end{matrix}\]</span></p>
<p><a href="#tab:2.4">Tabla 2.4</a> : Datos de Accidentes de Automóvil en Singapur</p>
<p>Si se ajusta una distribución de Poisson, entonces el <em>MLE</em> para <span class="math inline">\(\lambda\)</span>, la media de la Poisson, es la media muestral que es igual a
<span class="math display">\[
\overline{N} = \frac{0\cdot 6996 + 1 \cdot 455 + 2 \cdot 28 + 3 \cdot 4 + 4 \cdot 0}{7483} = 0,06989.
\]</span>
Ahora si se usa la Poisson (<span class="math inline">\(\hat{\lambda}_{MLE}\)</span>) como la distribución ajustada, entonces una comparación tabular de los valores ajustados y los valores observados se muestra en la <a href="#tab:2.5">Tabla 2.5</a> siguiente, donde <span class="math inline">\(\hat{p}_k\)</span> representa las probabilidades estimadas mediante la distribución de Poisson ajustada.</p>
<p><a id=tab:2.5></a></p>
<p><span class="math display">\[\begin{matrix}
\begin{array}{c|c|c}
\hline
\text{Número}  &amp; \text{Observado}  &amp; \text{Ajustado}\\
(k) &amp; (m_k) &amp; \text{Usando Poisson }(n\hat{p}_k)\\
\hline
0 &amp; 6.996 &amp; 6.977,86 \\
1 &amp; 455 &amp; 487,70 \\
2 &amp; 28 &amp; 17,04 \\
3 &amp; 4 &amp; 0,40 \\
\geq4 &amp; 0 &amp; 0,01\\
\hline
\text{Total} &amp; 7.483 &amp; 7.483,00\\
\hline
\end{array}
\end{matrix}\]</span></p>
<p><a href="#tab:2.5">Tabla 2.5</a> : Comparación entre valores observados y ajustados: Datos de automóviles de Singapur</p>
<p>Mientras que el ajuste parece <em>razonable</em>, una comparación tabular no es suficiente como contraste estadístico de la hipótesis de que la distribución subyacente es efectivamente la Poisson. El estadístico de chi-cuadrado de Pearson es una medida de bondad del ajuste que puede utilizarse para este propósito. Para explicar este estadístico, supongamos que un conjunto de datos de tamaño <span class="math inline">\(n\)</span> se agrupa en <span class="math inline">\(k\)</span> celdas, siendo <span class="math inline">\(m_k/n\)</span> y <span class="math inline">\(\hat{p}_k\)</span>, para <span class="math inline">\(k=1\ldots,K\)</span>, las probabilidades observadas y estimadas de que una observación pertenezca a la celda <span class="math inline">\(k\)</span>-ésima, respectivamente. El estadístico del contraste chi-cuadrado de Pearson viene dado por</p>
<p><span class="math display">\[
\sum_{k=1}^K\frac{\left( m_k-n\widehat{p}_k \right) ^{2}}{n\widehat{p}_k}.
\]</span>
La justificación del estadístico anterior se deriva del hecho que</p>
<p><span class="math display">\[
\sum_{k=1}^K\frac{\left( m_k-n{p}_k \right) ^{2}}{n{p}_k}
\]</span></p>
<p>tiene como límite una distribución chi-cuadrado con <span class="math inline">\(K-1\)</span> grados de libertad si <span class="math inline">\(p_k\)</span>, <span class="math inline">\(k=1,\ldots,K\)</span> son las probabilidades verdaderas de cada celda. Ahora supongamos que sólo los datos resumidos representados por <span class="math inline">\(m_k\)</span>, <span class="math inline">\(k=1,\ldots,K\)</span> están disponibles. Además, si las <span class="math inline">\(p_k\)</span> son funciones de <span class="math inline">\(s\)</span> parámetros, sustituyendo las <span class="math inline">\(p_k\)</span> por cualquier probabilidad estimada <em>eficientemente</em> <span class="math inline">\(\widehat{p}_k\)</span>, el estadístico sigue teniendo una distribución chi-cuadrado como límite pero con <span class="math inline">\(K-1-s\)</span> grados de libertad. Estas estimaciones eficientes pueden obtenerse, por ejemplo, usando el método <em>MLE</em> (con una verosimilitud multinomial) o estimando los <span class="math inline">\(s\)</span> parámetros que minimizan el estadístico chi-cuadrado de Pearson previo. Por ejemplo, el código <code>R</code> que se muestra a continuación realiza una estimación de <span class="math inline">\(\lambda\)</span> en base a esto último y se obtiene un estimador de <span class="math inline">\(0,06623153\)</span>, cercano pero diferente del <em>MLE</em> de <span class="math inline">\(\lambda\)</span> usando la totalidad de los datos:</p>
<pre><code>m&lt;-c(6996,455,28,4,0);
op&lt;-m/sum(m);
g&lt;-function(lam){sum((op-c(dpois(0:3,lam),1-ppois(3,lam)))^2)};
optim(sum(op*(0:4)),g,method=&quot;Brent&quot;,lower=0,upper=10)$par</code></pre>
<p>Cuando se usa la totalidad de los datos para estimar las probabilidades, la distribución asintótica está <em>entre</em> las distribuciones chi-cuadrado con parámetros <span class="math inline">\(K-1\)</span> y <span class="math inline">\(K-1-s\)</span>. En la práctica, normalmente no se considera este matiz y se asume que la chi-cuadrado límite tiene <span class="math inline">\(K-1-s\)</span> grados de libertad. Curiosamente, esta forma de actuar funciona bastante bien en el caso de la distribución de Poisson.</p>
<p>Para los datos de autos de Singapur el estadístico chi-cuadrado de Pearson es igual a <span class="math inline">\(41,98\)</span> utilizando el conjunto de datos <em>MLE</em> para <span class="math inline">\({\lambda}\)</span>. Usando la distribución límite de chi-cuadrado con <span class="math inline">\(5-1-1=3\)</span> grados de libertad, vemos que el valor de <span class="math inline">\(41,98\)</span> está muy lejos en la cola (el percentil <span class="math inline">\(99\)</span> está por debajo de <span class="math inline">\(12\)</span>). Por lo tanto, podemos concluir que la distribución de Poisson proporciona un ajuste inadecuado para los datos.</p>
<p>Anteriormente, en la tabla resumen previa hemos considerado que las celdas vienen dadas. En la práctica, una pregunta relevante es cómo definir las celdas para que la distribución chi-cuadrado sea una buena aproximación a la distribución de muestra finita del estadístico. Una regla empírica es definir las celdas de tal manera que el <span class="math inline">\(80%\)</span> de las celdas, si no todas, tengan al menos valores esperados mayores a <span class="math inline">\(5\)</span>. Además, puesto que un mayor número de celdas proporciona una mayor potencia del contraste, una simple regla empírica es por tanto maximizar el número de celdas de tal forma que cada celda tenga al menos 5 observaciones.</p>
</div>
<div id="S:exercises" class="section level2">
<h2><span class="header-section-number">2.8</span> Ejercicios</h2>
<div id="ejercicios-teóricos" class="section level4 unnumbered">
<h4>Ejercicios Teóricos</h4>
<p><strong>Ejercicio 2.1.</strong> Derivar una expresión para <span class="math inline">\(p_N(\cdot)\)</span> en términos de <span class="math inline">\(F_N(\cdot)\)</span> y <span class="math inline">\(S_N(\cdot)\)</span>.</p>
<p><strong>Ejercicio 2.2.</strong> Una medida del centro de localización debe ser <strong>equivariable</strong> con respecto a los desplazamientos, o transformaciones de localización. En otras palabras, si <span class="math inline">\(N_1\)</span> y <span class="math inline">\(N_2\)</span> son dos variables aleatorias tales que <span class="math inline">\(N_1+c\)</span> tiene la misma distribución que <span class="math inline">\(N_2\)</span>, para una constante <span class="math inline">\(c\)</span>, entonces la diferencia entre las medidas del centro de localización de <span class="math inline">\(N_2\)</span> y <span class="math inline">\(N_1\)</span> debe ser igual a <span class="math inline">\(c\)</span>. Mostrar que la media satisface esta propiedad.</p>
<p><strong>Ejercicio 2.3.</strong> Las medidas de dispersión deben ser invariables con respecto a desplazamientos y escala equi-variables. Demuestre que la desviación estándar satisface estas propiedades haciendo lo siguiente:</p>
<ul>
<li>Mostrar que para una variable aleatoria <span class="math inline">\(N\)</span>, su desviación estándar es igual a la de <span class="math inline">\(N+c\)</span>, para cualquier constante <span class="math inline">\(c\)</span>.</li>
<li>Mostrar que para una variable aleatoria <span class="math inline">\(N\)</span>, su desviación estándar es igual a <span class="math inline">\(1/c\)</span> por <span class="math inline">\(cN\)</span>, para cualquier constante positiva <span class="math inline">\(c\)</span>.</li>
</ul>
<p><strong>Ejercicio 2.4.</strong> Supongamos que <span class="math inline">\(N\)</span> es una variable aleatoria con función masa de probabilidad dada por
<span class="math display">\[
p_N(k):= \begin{cases}
\left(\frac{6}{\pi^2}\right)\left(\frac{1}{k^{2}}\right), &amp; k\geq 1;\\
0, &amp;\hbox{en otro caso}.
\end{cases}
\]</span>
Demostrar que la media de <span class="math inline">\(N\)</span> es <span class="math inline">\(\infty\)</span>.</p>
<p><strong>Ejercicio 2.5.</strong> Supongamos que <span class="math inline">\(N\)</span> es una variable aleatoria con segundo momento finito. Demostrar que la función <span class="math inline">\(\psi(\cdot)\)</span> definida por <span class="math display">\[
\psi(x):=\mathrm{E}{(N-x)^2}. \quad x\in\mathbb{R}
\]</span>
se minimiza en <span class="math inline">\(\mu_N\)</span> sin realizar cálculos. También, proporcionar una demostración de este resultado mediante derivadas. Concluir que el valor mínimo es igual a la varianza de <span class="math inline">\(N\)</span>.</p>
<p><strong>Ejercicio 2.6.</strong> Derivar los dos primeros momentos centrales de las distribuciones <span class="math inline">\((a,b,0)\)</span> utilizando los métodos mencionados a continuación:</p>
<ul>
<li>Para la distribución binomial, derivar los momentos usando sólo su <em>pmf</em>, luego su <em>mgf</em>, y luego su <em>pgf</em>.</li>
<li>Para la distribución Poisson, derivar los momentos usando sólo su <em>mgf</em>.</li>
<li>Para la distribución binomial negativa, derivar los momentos usando sólo su <em>pmf</em>, y luego su <em>pgf</em>.</li>
</ul>
<p><strong>Ejercicio 2.7.</strong> Supongamos que <span class="math inline">\(N_1\)</span> y <span class="math inline">\(N_2\)</span> son dos variables aleatorias independientes de Poisson con medias <span class="math inline">\(\lambda_1\)</span> y <span class="math inline">\(\lambda_2\)</span>, respectivamente. Identificar la distribución condicionada de <span class="math inline">\(N_1\)</span> dado <span class="math inline">\(N_1+N_2\)</span>.</p>
<p><strong>Ejercicio 2.8.</strong> (<strong>No unicidad de MLE</strong>)</p>
<p>Considerar la siguiente familia paramétrica de densidades indexadas por el parámetro <span class="math inline">\(p\)</span> que toma valores en <span class="math inline">\([0,1]\)</span>:
<span class="math display">\[
f_p(x)=p\cdot\phi(x+2)+(1-p)\cdot\phi(x-2), \quad x\in\mathbb{R},
\]</span>
donde <span class="math inline">\(\phi(\cdot)\)</span> representa la densidad normal estándar.</p>
<ul>
<li>Mostrar que para todos los <span class="math inline">\(p\in[0,1]\)</span>, la <span class="math inline">\(f_p(\cdot)\)</span> previa es una función de densidad válida.</li>
<li>Encontrar una expresión en <span class="math inline">\(p\)</span> para la media y la varianza de <span class="math inline">\(f_p(\cdot)\)</span>.</li>
<li>Supongamos una muestra de tamaño uno que consiste en <span class="math inline">\(x\)</span>. Mostrar que cuando <span class="math inline">\(x\)</span> es igual a <span class="math inline">\(0\)</span>, el conjunto de <em>estimadores máximo verosímiles</em> para <span class="math inline">\(p\)</span> es igual a <span class="math inline">\([0,1]\)</span>; también mostrar que el <em>MLE</em> es único en otro caso.</li>
</ul>
<p><strong>Ejercicio 2.9.</strong> Representar gráficamente la región del plano que corresponde a los valores de <span class="math inline">\((a,b)\)</span> que dan lugar a distribuciones <span class="math inline">\((a,b,0)\)</span> válidas. Hacer lo mismo para las distribuciones <span class="math inline">\((a,b,1)\)</span>.</p>
<p><strong>Ejercicio 2.10.</strong> (<strong>Complejidad computacional</strong>) Para la clase de distribuciones <span class="math inline">\((a,b,0)\)</span>, contabilizar el número de operaciones matemáticas básicas (suma, resta, multiplicación, división) necesarias para calcular las <span class="math inline">\(n\)</span> probabilidades <span class="math inline">\(p_0\ldots p_{n-1}\)</span> utilizando la relación de recurrencia. Para la distribución binomial negativa con <span class="math inline">\(r\)</span> no entero, contabilizar el número de estas operaciones. ¿Qué es lo que observas?</p>
<p><strong>Ejercicio 2.11.</strong> (** **) Utilizando el desarrollo de la Sección 2.3 mostrar de forma rigurosa que no sólo la recurrencia <a href="C-Frequency-Modeling.html#eq:ab0">(2.1)</a> une las distribuciones binomiales, Poisson y binomial negativa, sino que también las caracteriza.</p>
</div>
<div id="ejercicios-con-enfoque-práctico" class="section level4 unnumbered">
<h4>Ejercicios con Enfoque Práctico</h4>
<p><strong>Ejercicio 2.12. Pregunta de Examen Actuarial.</strong> Se conoce:</p>
<!-- \def\labelenumi{\arabic{enumi}.} -->
<ol style="list-style-type: decimal">
<li><span class="math inline">\(p_k\)</span> denota la probabilidad que el número de siniestros sea igual a <span class="math inline">\(k\)</span>
para <span class="math inline">\(k=0,1,2,\ldots\)</span></li>
<li><span class="math inline">\(\frac{p_n}{p_m}=\frac{m!}{n!}, m\ge 0, n\ge 0\)</span></li>
</ol>
<p>Usando la distribución correspondiente del número de siniestros cero-modificada con <span class="math inline">\(p_0^M=0,1\)</span>, calcular <span class="math inline">\(p_1^M\)</span>.</p>
<p><strong>Ejercicio 2.13. Pregunta de Examen Actuarial.</strong>
Durante un período de un año, el número de accidentes por día se distribuyó de la siguiente manera:</p>
<!-- \label{tabsing3} -->
<p><span class="math display">\[
\begin{matrix}
\begin{array}{c|c|c|c|c|c|c}
\hline
\text{No. de Accidentes} &amp; 0 &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5\\
\hline
\text{No. de Días} &amp; 209 &amp; 111 &amp; 33 &amp; 7 &amp; 5 &amp; 2\\
\hline
\end{array}
\end{matrix}
\]</span></p>
<p>Se utiliza un contraste chi-cuadrado para medir el ajuste de una distribución de Poisson con una media de 0,60. El número mínimo esperado de observaciones en cualquier grupo debe ser 5. El número máximo de grupos debe utilizarse. Determinar el valor del estadístico chi-cuadrado.</p>
<p><strong>Ejercicio 2.14.</strong> Una distribución de probabilidad discreta tiene las siguientes propiedades
<span class="math display">\[
\begin{aligned}
\Pr(N=k) = \left( \frac{3k+9}{8k}\right) \Pr(N=k-1), \quad k=1,2,3,\ldots
\end{aligned}
\]</span>
Determinar el valor de <span class="math inline">\(\Pr(N=3)\)</span>. (Resp: 0,1609)</p>
</div>
<div id="ejercicios-adicionales" class="section level4 unnumbered">
<h4>Ejercicios Adicionales</h4>
<p>Aquí se encuentra un conjunto de ejercicios que guían al lector a través de algunos de los fundamentos teóricos de <strong>Loss Data Analytics</strong>. Cada tutorial se basa en una o más preguntas de los exámenes actuariales profesionales - normalmente el examen C de la Society of Actuaries.</p>
<p style="text-align: center;">
<a href="https://www.ssc.wisc.edu/~jfrees/loss-data-analytics/loss-data-analytics-problems/">Tutoriales Guiados de Distribución de Frecuencias</a>
</p>
</div>
</div>
<div id="Freq-further-reading-and-resources" class="section level2">
<h2><span class="header-section-number">2.9</span> Recursos adicionales y autores</h2>
<p>El Capítulo del Apéndice <a href="C-AppA.html#C:AppA">15</a> ofrece una introducción general a la teoría de máxima verosimilitud en relación con la estimación de los parámetros de una familia paramétrica. El Capítulo del Apéndice <a href="C-AppC.html#C:AppC">17</a> proporciona ejemplos más específicos y expande algunos de los conceptos.</p>
<div id="autoría" class="section level4 unnumbered">
<h4>Autoría</h4>
<ul>
<li><p><strong>N.D. Shyamalkumar</strong>, The University of Iowa, y <strong>Krupa Viswanathan</strong>, Temple University, son los autores principales de la versión inicial de este capítulo. Email: <a href="mailto:shyamal-kumar@uiowa.edu" class="email">shyamal-kumar@uiowa.edu</a> para comentarios del capítulo y sugerencias de mejora.</p></li>
<li><p>Revisores del Capítulo incluyen: Paul Johnson, Hirokazu (Iwahiro) Iwasawa, Rajesh Sahasrabuddhe, Michelle Xia.</p></li>
<li><p>Traducción al español: Ramon Alemany y Miguel Santolino (Universitat de Barcelona).</p></li>
</ul>
</div>
<div id="S:rcode" class="section level3">
<h3><span class="header-section-number">2.9.1</span> TS 2.A. Código R para Gráficos</h3>
<p><strong>Código para Figura <a href="C-Frequency-Modeling.html#fig:MLEab0">2.3</a>:</strong></p>
<h5 style="text-align: center;">
<a id="displayCode.Freq.2" href="javascript:togglecode('toggleCode.Freq.2','displayCode.Freq.2');"><i><strong>Mostrar Código R</strong></i></a>
</h5>
<div id="toggleCode.Freq.2" style="display: none">
<pre><code>likbinm&lt;-function(m){ 
  # verosimilitud binomial maximizada con respecto a p
  prod((dbinom(x,m,mean(x)/m)))
}

liknbinm&lt;-function(r){
  # verosimilitud binomial negativa maximizada con respecto a beta
  prod(dnbinom(x,r,1-mean(x)/(mean(x)+r)))
}

# Datos Matriciales; Tres muestras, una en cada Columna; 
# Primera Muestra tiene Var&lt;Mean
# Segunda Muestra tiene Var=Mean
# Tercera Muestra tiene Var&gt;Mean

  X&lt;-cbind(c(2,5,6,8,9)+2,c(2,5,6,8,9),c(2,3,6,8,9)); 

# Se utiliza para crear las etiquetas en la matriz z 
 ord_char&lt;-c(&quot;&lt;&quot;,&quot;=&quot;,&quot;&gt;&quot;); 
  
# Matrices vacías; 
  Y&lt;-matrix(1,ncol=2,nrow=0); 
  Z&lt;-matrix(1,ncol=2,nrow=0); 

for (i in (1:3)) {
  # Trabaja con los datos de la i-ésima muestra 
   x&lt;-X[,i]; 
  
  # Verosimilitud Binomial
      # Intervalo de n valores cubriendo la MLE
        n&lt;-(9:100);
      # Evaluación de la verosimilitud en varios valores de n
        ll&lt;-sapply(n,likbinm); 
      # Encontrando la MLE de n
        n[ll==max(ll[!is.na(ll)])] 
      # Almacenamiento de los datos y las etiquetas
        Y&lt;-rbind(Y,cbind(n,ll));
        Z&lt;-rbind(Z,cbind(rep(paste(&quot;$\\hat{\\sigma}^2&quot;,ord_char[i],&quot;\\hat{\\mu}$&quot;),length(n)),rep(&quot;Binomial - $L(m,\\overline{x}/m)$&quot;,length(n))));

  # Verosimilitud Binomial Negativa
    # Intervalo de valores de r 
      r&lt;-(1:100);
    # Evaluación de la verosimilitud en varios valores de r
      ll&lt;-sapply(r,liknbinm); 
    # Encontrando la MLE de r
      ll[is.na(ll)]=0;
      r[ll==max(ll[!is.na(ll)])]; 
    # Almacenamiento de los datos y las etiquetas
      Y&lt;-rbind(Y,cbind(r,ll));
      Z&lt;-rbind(Z,cbind(rep(paste(&quot;$\\hat{\\sigma}^2&quot;,ord_char[i],&quot;\\hat{\\mu}$&quot;),length(r)),rep(&quot;Neg.Binomial - $L(r,\\overline{x}/r)$&quot;,length(r))));
      
  # Verosimilitud de Poisson
    # Almacenamiento de los datos y las etiquetas
    # En el caso de la Poisson la MLE es la media muestral
      Y&lt;-rbind(Y,cbind(r,rep(prod(dpois(x,mean(x))),length(r))));
      Z&lt;-rbind(Z,cbind(rep(paste(&quot;$\\hat{\\sigma}^2&quot;,ord_char[i],&quot;\\hat{\\mu}$&quot;),length(r)),rep(&quot;Poisson - $L(\\overline{x})$&quot;,length(r))));
}

  # Asignación de Nombres a las Columnas
    colnames(Y)&lt;-c(&quot;x&quot;,&quot;lik&quot;);
    colnames(Z)&lt;-c(&quot;dataset&quot;,&quot;Distribution&quot;);
  # Creación de un Dataframe para usar ggplot
    dy&lt;-cbind(data.frame(Y),data.frame(Z));


library(tikzDevice);
library(ggplot2);
options(tikzMetricPackages = c(&quot;\\usepackage[utf8]{inputenc}&quot;,&quot;\\usepackage[T1]{fontenc}&quot;, &quot;\\usetikzlibrary{calc}&quot;, 
                               &quot;\\usepackage{amssymb}&quot;,&quot;\\usepackage{amsmath}&quot;,&quot;\\usepackage[active]{preview}&quot;))
tikz(file = &quot;plot_test_2.tex&quot;, width = 6.25, height = 6.25);
ggplot(data=dy,aes(x=x,y=lik,col=Distribution)) + geom_point(size=0.25) + facet_grid(dataset~.)+
  labs(x=&quot;m/r&quot;,y=&quot;Verosimilitud&quot;,title=&quot;&quot;); 
dev.off();</code></pre>
</div>
<hr />
<p><strong>Código para Figura <a href="C-Frequency-Modeling.html#fig:MLEm">2.2</a>:</strong></p>
<h5 style="text-align: center;">
<a id="displayCode.Freq.3" href="javascript:togglecode('toggleCode.Freq.3','displayCode.Freq.3');"><i><strong>Mostrar Código de R</strong></i></a>
</h5>
<div id="toggleCode.Freq.3" style="display: none">
<pre><code>likm&lt;-function(m){
  prod((dbinom(x,m,mean(x)/m)))
}
x&lt;-c(2,2,2,4,5);
n&lt;-(5:100);
# Cálculo de la verosimilitud 
ll&lt;-sapply(n,likm); 
# Calculando MLE
n[ll==max(ll)]
# Almacenamiento de la curva de verosimilitud
y&lt;-cbind(n,ll);

# Segundo conjunto de datos
x&lt;-c(2,2,2,4,6);
ll&lt;-sapply(n,likm);
n[ll==max(ll)]
y&lt;-cbind(y,ll);

# Tercer conjunto de datos
x&lt;-c(2,2,2,4,7);
ll&lt;-sapply(n,likm);
n[ll==max(ll)]
y&lt;-cbind(y,ll);

colnames(y)&lt;-c(&quot;m&quot;,&quot;$\\tilde{x}=(2,2,2,4,5)$&quot;,&quot;$\\tilde{x}=(2,2,2,4,6)$&quot;,&quot;$\\tilde{x}=(2,2,2,4,7)$&quot;);
dy&lt;-data.frame(y);
library(tikzDevice);
library(ggplot2);
options(tikzMetricPackages = c(&quot;\\usepackage[utf8]{inputenc}&quot;,&quot;\\usepackage[T1]{fontenc}&quot;, &quot;\\usetikzlibrary{calc}&quot;, 
                               &quot;\\usepackage{amssymb}&quot;,&quot;\\usepackage{amsmath}&quot;,&quot;\\usepackage[active]{preview}&quot;))
tikz(file = &quot;plot_test.tex&quot;, width = 6.25, height = 3.125);
ggplot(dy) + 
  geom_point(aes(x=m, y=(X..tilde.x...2.2.2.4.5..),shape=&quot;$\\tilde{x}=(2,2,2,4,5):\\hat{m}=7$&quot;),size=0.75) + 
  geom_point(aes(x=m, y=(X..tilde.x...2.2.2.4.6..),shape=&quot;$\\tilde{x}=(2,2,2,4,6):\\hat{m}=18$&quot;),size=0.75) +
  geom_point(aes(x=m, y=(X..tilde.x...2.2.2.4.7..),shape=&quot;$\\tilde{x}=(2,2,2,4,7):\\hat{m}=\\infty$&quot;),size=0.75) +
  geom_point(aes(x=c(7),y=dy$X..tilde.x...2.2.2.4.5..[3],colour=&quot;$\\hat{m}$&quot;,shape=&quot;$\\tilde{x}=(2,2,2,4,5):\\hat{m}=7$&quot;),size=0.75)+
  geom_point(aes(x=c(18),y=dy$X..tilde.x...2.2.2.4.6..[14],colour=&quot;$\\hat{m}$&quot;,shape=&quot;$\\tilde{x}=(2,2,2,4,6):\\hat{m}=18$&quot;),size=0.75)+
  labs(x=&quot;m&quot;,y=&quot;$L(m,\\overline{x}/m)$&quot;,title=&quot;MLE para $m$: No-Robustez de MLE &quot;); 
dev.off();</code></pre>
</div>
<hr />

</div>
</div>
</div>
<h3>Bibliography</h3>
<div id="refs" class="references">
<div id="ref-billingsley">
<p>Billingsley, Patrick. 2008. <em>Probability and Measure</em>. John Wiley &amp; Sons.</p>
</div>
<div id="ref-freesregression">
<p>Frees, Edward W. 2009. <em>Regression Modeling with Actuarial and Financial Applications</em>. Cambridge University Press.</p>
</div>
<div id="ref-zimmerman2015">
<p>Hogg, Robert V, Elliot A Tanis, and Dale L Zimmerman. 2015. <em>Probability and Statistical Inference, 9th Edition</em>. Pearson, New York.</p>
</div>
<div id="ref-levin1977">
<p>Levin, Bruce, James Reeds, and others. 1977. “Compound Multinomial Likelihood Functions Are Unimodal: Proof of a Conjecture of Ij Good.” <em>The Annals of Statistics</em> 5 (1): 79–87.</p>
</div>
<div id="ref-olkin1981">
<p>Olkin, Ingram, A John Petkau, and James V Zidek. 1981. “A Comparison of N Estimators for the Binomial Distribution.” <em>Journal of the American Statistical Association</em> 76 (375): 637–42.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="2">
<li id="fn2"><p>Por comodidad, indexamos <span class="math inline">\(\mu_N\)</span> con la variable aleatoria <span class="math inline">\(N\)</span> en lugar de <span class="math inline">\(F_N\)</span> o <span class="math inline">\(p_N\)</span>, aunque es una función de la distribución de la variable aleatoria.<a href="C-Frequency-Modeling.html#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p><span class="math inline">\(0^0 = 1\)</span><a href="C-Frequency-Modeling.html#fnref3" class="footnote-back">↩</a></p></li>
<li id="fn4"><p>A partir de aquí suprimiremos la referencia a <span class="math inline">\(N\)</span> y denotaremos la <em>pmf</em> por la secuencia <span class="math inline">\(\{p_k\}_{k\geq 0}\)</span>, en vez de la función <span class="math inline">\(p_N(\cdot)\)</span>.<a href="C-Frequency-Modeling.html#fnref4" class="footnote-back">↩</a></p></li>
<li id="fn5"><p>Para la base teórica que subyace de la expresión anterior, ver <span class="citation">(Billingsley <a href="#ref-billingsley" role="doc-biblioref">2008</a>)</span>.<a href="C-Frequency-Modeling.html#fnref5" class="footnote-back">↩</a></p></li>
<li id="fn6"><p>El conjunto de maximizadores de <span class="math inline">\(L(\cdot)\)</span> son los mismos que el conjunto de maximizadores de cualquier función estrictamente creciente de <span class="math inline">\(L(\cdot)\)</span>, y por lo tanto son los mismos que los de <span class="math inline">\(l(\cdot)\)</span>.<a href="C-Frequency-Modeling.html#fnref6" class="footnote-back">↩</a></p></li>
<li id="fn7"><p>Una pequeña ventaja de trabajar con <span class="math inline">\(l(\cdot)\)</span> es que los términos constantes en <span class="math inline">\(L(\cdot)\)</span> no aparecen en <span class="math inline">\(l&#39;(\cdot)\)</span> mientras que si lo hacen en <span class="math inline">\(L&#39;(\cdot)\)</span>.<a href="C-Frequency-Modeling.html#fnref7" class="footnote-back">↩</a></p></li>
<li id="fn8"><p>Esto, en concreto, establece una forma de simular a partir de una distribución mixta que hace uso de los eficientes esquemas de simulación que pueden existir para las distribuciones que la componen.<a href="C-Frequency-Modeling.html#fnref8" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="C-Intro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="C-Severity.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["LossDataAnalytics.pdf", "LossDataAnalytics.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
